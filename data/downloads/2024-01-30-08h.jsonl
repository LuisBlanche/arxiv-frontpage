{"created":"2024-01-29 18:59:56","title":"Computer Vision for Primate Behavior Analysis in the Wild","abstract":"Advances in computer vision as well as increasingly widespread video-based behavioral monitoring have great potential for transforming how we study animal cognition and behavior. However, there is still a fairly large gap between the exciting prospects and what can actually be achieved in practice today, especially in videos from the wild. With this perspective paper, we want to contribute towards closing this gap, by guiding behavioral scientists in what can be expected from current methods and steering computer vision researchers towards problems that are relevant to advance research in animal behavior. We start with a survey of the state-of-the-art methods for computer vision problems that are directly relevant to the video-based study of animal behavior, including object detection, multi-individual tracking, (inter)action recognition and individual identification. We then review methods for effort-efficient learning, which is one of the biggest challenges from a practical perspective. Finally, we close with an outlook into the future of the emerging field of computer vision for animal behavior, where we argue that the field should move fast beyond the common frame-by-frame processing and treat video as a first-class citizen.","sentences":["Advances in computer vision as well as increasingly widespread video-based behavioral monitoring have great potential for transforming how we study animal cognition and behavior.","However, there is still a fairly large gap between the exciting prospects and what can actually be achieved in practice today, especially in videos from the wild.","With this perspective paper, we want to contribute towards closing this gap, by guiding behavioral scientists in what can be expected from current methods and steering computer vision researchers towards problems that are relevant to advance research in animal behavior.","We start with a survey of the state-of-the-art methods for computer vision problems that are directly relevant to the video-based study of animal behavior, including object detection, multi-individual tracking, (inter)action recognition and individual identification.","We then review methods for effort-efficient learning, which is one of the biggest challenges from a practical perspective.","Finally, we close with an outlook into the future of the emerging field of computer vision for animal behavior, where we argue that the field should move fast beyond the common frame-by-frame processing and treat video as a first-class citizen."],"url":"http://arxiv.org/abs/2401.16424v1"}
{"created":"2024-01-29 18:59:55","title":"Synchformer: Efficient Synchronization from Sparse Cues","abstract":"Our objective is audio-visual synchronization with a focus on 'in-the-wild' videos, such as those on YouTube, where synchronization cues can be sparse. Our contributions include a novel audio-visual synchronization model, and training that decouples feature extraction from synchronization modelling through multi-modal segment-level contrastive pre-training. This approach achieves state-of-the-art performance in both dense and sparse settings. We also extend synchronization model training to AudioSet a million-scale 'in-the-wild' dataset, investigate evidence attribution techniques for interpretability, and explore a new capability for synchronization models: audio-visual synchronizability.","sentences":["Our objective is audio-visual synchronization with a focus on 'in-the-wild' videos, such as those on YouTube, where synchronization cues can be sparse.","Our contributions include a novel audio-visual synchronization model, and training that decouples feature extraction from synchronization modelling through multi-modal segment-level contrastive pre-training.","This approach achieves state-of-the-art performance in both dense and sparse settings.","We also extend synchronization model training to AudioSet a million-scale 'in-the-wild' dataset, investigate evidence attribution techniques for interpretability, and explore a new capability for synchronization models: audio-visual synchronizability."],"url":"http://arxiv.org/abs/2401.16423v1"}
{"created":"2024-01-29 18:59:22","title":"Strategic Usage in a Multi-Learner Setting","abstract":"Real-world systems often involve some pool of users choosing between a set of services. With the increase in popularity of online learning algorithms, these services can now self-optimize, leveraging data collected on users to maximize some reward such as service quality. On the flipside, users may strategically choose which services to use in order to pursue their own reward functions, in the process wielding power over which services can see and use their data. Extensive prior research has been conducted on the effects of strategic users in single-service settings, with strategic behavior manifesting in the manipulation of observable features to achieve a desired classification; however, this can often be costly or unattainable for users and fails to capture the full behavior of multi-service dynamic systems. As such, we analyze a setting in which strategic users choose among several available services in order to pursue positive classifications, while services seek to minimize loss functions on their observations. We focus our analysis on realizable settings, and show that naive retraining can still lead to oscillation even if all users are observed at different times; however, if this retraining uses memory of past observations, convergent behavior can be guaranteed for certain loss function classes. We provide results obtained from synthetic and real-world data to empirically validate our theoretical findings.","sentences":["Real-world systems often involve some pool of users choosing between a set of services.","With the increase in popularity of online learning algorithms, these services can now self-optimize, leveraging data collected on users to maximize some reward such as service quality.","On the flipside, users may strategically choose which services to use in order to pursue their own reward functions, in the process wielding power over which services can see and use their data.","Extensive prior research has been conducted on the effects of strategic users in single-service settings, with strategic behavior manifesting in the manipulation of observable features to achieve a desired classification; however, this can often be costly or unattainable for users and fails to capture the full behavior of multi-service dynamic systems.","As such, we analyze a setting in which strategic users choose among several available services in order to pursue positive classifications, while services seek to minimize loss functions on their observations.","We focus our analysis on realizable settings, and show that naive retraining can still lead to oscillation even if all users are observed at different times; however, if this retraining uses memory of past observations, convergent behavior can be guaranteed for certain loss function classes.","We provide results obtained from synthetic and real-world data to empirically validate our theoretical findings."],"url":"http://arxiv.org/abs/2401.16422v1"}
{"created":"2024-01-29 18:59:07","title":"Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation","abstract":"In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE). For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding. The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding. The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding. Theoretical analysis shows this disentanglement of positional information makes learning more effective. The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities.","sentences":["In this work, we leverage the intrinsic segmentation of language sequences and design a new positional encoding method called Bilevel Positional Encoding (BiPE).","For each position, our BiPE blends an intra-segment encoding and an inter-segment encoding.","The intra-segment encoding identifies the locations within a segment and helps the model capture the semantic information therein via absolute positional encoding.","The inter-segment encoding specifies the segment index, models the relationships between segments, and aims to improve extrapolation capabilities via relative positional encoding.","Theoretical analysis shows this disentanglement of positional information makes learning more effective.","The empirical results also show that our BiPE has superior length extrapolation capabilities across a wide range of tasks in diverse text modalities."],"url":"http://arxiv.org/abs/2401.16421v1"}
{"created":"2024-01-29 18:59:02","title":"InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model","abstract":"We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension. This model goes beyond conventional vision-language understanding, adeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual specifications, and reference images, enabling highly customizable content creation. InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach that applies additional LoRA parameters exclusively to image tokens to preserve the integrity of pre-trained language knowledge, striking a balance between precise vision understanding and text composition with literary talent. Experimental results demonstrate the superiority of InternLM-XComposer2 based on InternLM2-7B in producing high-quality long-text multi-modal content and its exceptional vision-language understanding performance across various benchmarks, where it not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments. This highlights its remarkable proficiency in the realm of multimodal understanding. The InternLM-XComposer2 model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer.","sentences":["We introduce InternLM-XComposer2, a cutting-edge vision-language model excelling in free-form text-image composition and comprehension.","This model goes beyond conventional vision-language understanding, adeptly crafting interleaved text-image content from diverse inputs like outlines, detailed textual specifications, and reference images, enabling highly customizable content creation.","InternLM-XComposer2 proposes a Partial LoRA (PLoRA) approach that applies additional LoRA parameters exclusively to image tokens to preserve the integrity of pre-trained language knowledge, striking a balance between precise vision understanding and text composition with literary talent.","Experimental results demonstrate the superiority of InternLM-XComposer2 based on InternLM2-7B in producing high-quality long-text multi-modal content and its exceptional vision-language understanding performance across various benchmarks, where it not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V and Gemini Pro in certain assessments.","This highlights its remarkable proficiency in the realm of multimodal understanding.","The InternLM-XComposer2 model series with 7B parameters are publicly available at https://github.com/InternLM/InternLM-XComposer."],"url":"http://arxiv.org/abs/2401.16420v1"}
{"created":"2024-01-29 18:57:45","title":"Semi-parametric Expert Bayesian Network Learning with Gaussian Processes and Horseshoe Priors","abstract":"This paper proposes a model learning Semi-parametric rela- tionships in an Expert Bayesian Network (SEBN) with linear parameter and structure constraints. We use Gaussian Pro- cesses and a Horseshoe prior to introduce minimal nonlin- ear components. To prioritize modifying the expert graph over adding new edges, we optimize differential Horseshoe scales. In real-world datasets with unknown truth, we gen- erate diverse graphs to accommodate user input, addressing identifiability issues and enhancing interpretability. Evalua- tion on synthetic and UCI Liver Disorders datasets, using metrics like structural Hamming Distance and test likelihood, demonstrates our models outperform state-of-the-art semi- parametric Bayesian Network model.","sentences":["This paper proposes a model learning Semi-parametric rela- tionships in an Expert Bayesian Network (SEBN) with linear parameter and structure constraints.","We use Gaussian Pro- cesses and a Horseshoe prior to introduce minimal nonlin- ear components.","To prioritize modifying the expert graph over adding new edges, we optimize differential Horseshoe scales.","In real-world datasets with unknown truth, we gen- erate diverse graphs to accommodate user input, addressing identifiability issues and enhancing interpretability.","Evalua- tion on synthetic and UCI Liver Disorders datasets, using metrics like structural Hamming Distance and test likelihood, demonstrates our models outperform state-of-the-art semi- parametric Bayesian Network model."],"url":"http://arxiv.org/abs/2401.16419v1"}
{"created":"2024-01-29 18:56:10","title":"Channel Coding with Mean and Variance Cost Constraints","abstract":"We consider channel coding for discrete memoryless channels (DMCs) with a novel cost constraint that constrains both the mean and the variance of the cost of the codewords. We show that the maximum (asymptotically) achievable rate under the new cost formulation is equal to the capacity-cost function; in particular, the strong converse holds. We further characterize the optimal second-order coding rate of these cost-constrained codes; in particular, the optimal second-order coding rate is finite. We then show that the second-order coding performance is strictly improved with feedback using a new variation of timid/bold coding, significantly broadening the applicability of timid/bold coding schemes from unconstrained compound-dispersion channels to all cost-constrained channels. Equivalent results on the minimum average probability of error are also given.","sentences":["We consider channel coding for discrete memoryless channels (DMCs) with a novel cost constraint that constrains both the mean and the variance of the cost of the codewords.","We show that the maximum (asymptotically) achievable rate under the new cost formulation is equal to the capacity-cost function; in particular, the strong converse holds.","We further characterize the optimal second-order coding rate of these cost-constrained codes; in particular, the optimal second-order coding rate is finite.","We then show that the second-order coding performance is strictly improved with feedback using a new variation of timid/bold coding, significantly broadening the applicability of timid/bold coding schemes from unconstrained compound-dispersion channels to all cost-constrained channels.","Equivalent results on the minimum average probability of error are also given."],"url":"http://arxiv.org/abs/2401.16417v1"}
{"created":"2024-01-29 18:55:29","title":"Endo-4DGS: Distilling Depth Ranking for Endoscopic Monocular Scene Reconstruction with 4D Gaussian Splatting","abstract":"In the realm of robot-assisted minimally invasive surgery, dynamic scene reconstruction can significantly enhance downstream tasks and improve surgical outcomes. Neural Radiance Fields (NeRF)-based methods have recently risen to prominence for their exceptional ability to reconstruct scenes. Nonetheless, these methods are hampered by slow inference, prolonged training, and substantial computational demands. Additionally, some rely on stereo depth estimation, which is often infeasible due to the high costs and logistical challenges associated with stereo cameras. Moreover, the monocular reconstruction quality for deformable scenes is currently inadequate. To overcome these obstacles, we present Endo-4DGS, an innovative, real-time endoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting (GS) and requires no ground truth depth data. This method extends 3D GS by incorporating a temporal component and leverages a lightweight MLP to capture temporal Gaussian deformations. This effectively facilitates the reconstruction of dynamic surgical scenes with variable conditions. We also integrate Depth-Anything to generate pseudo-depth maps from monocular views, enhancing the depth-guided reconstruction process. Our approach has been validated on two surgical datasets, where it has proven to render in real-time, compute efficiently, and reconstruct with remarkable accuracy. These results underline the vast potential of Endo-4DGS to improve surgical assistance.","sentences":["In the realm of robot-assisted minimally invasive surgery, dynamic scene reconstruction can significantly enhance downstream tasks and improve surgical outcomes.","Neural Radiance Fields (NeRF)-based methods have recently risen to prominence for their exceptional ability to reconstruct scenes.","Nonetheless, these methods are hampered by slow inference, prolonged training, and substantial computational demands.","Additionally, some rely on stereo depth estimation, which is often infeasible due to the high costs and logistical challenges associated with stereo cameras.","Moreover, the monocular reconstruction quality for deformable scenes is currently inadequate.","To overcome these obstacles, we present Endo-4DGS, an innovative, real-time endoscopic dynamic reconstruction approach that utilizes 4D Gaussian Splatting (GS) and requires no ground truth depth data.","This method extends 3D GS by incorporating a temporal component and leverages a lightweight MLP to capture temporal Gaussian deformations.","This effectively facilitates the reconstruction of dynamic surgical scenes with variable conditions.","We also integrate Depth-Anything to generate pseudo-depth maps from monocular views, enhancing the depth-guided reconstruction process.","Our approach has been validated on two surgical datasets, where it has proven to render in real-time, compute efficiently, and reconstruct with remarkable accuracy.","These results underline the vast potential of Endo-4DGS to improve surgical assistance."],"url":"http://arxiv.org/abs/2401.16416v1"}
{"created":"2024-01-29 18:51:02","title":"A Causal Model for Quantifying Multipartite Classical and Quantum Correlations","abstract":"We give an operational definition of information-theoretic resources within a given multipartite classical or quantum correlation. We present our causal model that serves as the source coding side of this correlation and introduce a novel concept of resource rate. We argue that, beyond classical secrecy, additional resources exist that are useful for the security of distributed computing problems, which can be captured by the resource rate. Furthermore, we establish a relationship between resource rate and an extension of Shannon's logarithmic information measure, namely, total correlation. Subsequently, we present a novel quantum secrecy monotone and investigate a quantum hybrid key distribution system as an extension of our causal model. Finally, we discuss some connections to optimal transport (OT) problem.","sentences":["We give an operational definition of information-theoretic resources within a given multipartite classical or quantum correlation.","We present our causal model that serves as the source coding side of this correlation and introduce a novel concept of resource rate.","We argue that, beyond classical secrecy, additional resources exist that are useful for the security of distributed computing problems, which can be captured by the resource rate.","Furthermore, we establish a relationship between resource rate and an extension of Shannon's logarithmic information measure, namely, total correlation.","Subsequently, we present a novel quantum secrecy monotone and investigate a quantum hybrid key distribution system as an extension of our causal model.","Finally, we discuss some connections to optimal transport (OT) problem."],"url":"http://arxiv.org/abs/2401.16414v1"}
{"created":"2024-01-29 18:49:50","title":"Learning to Manipulate under Limited Information","abstract":"By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with full information.","sentences":["By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference.","The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods.","Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote.","We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates.","We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with full information."],"url":"http://arxiv.org/abs/2401.16412v1"}
{"created":"2024-01-29 18:43:49","title":"Scaling Sparse Fine-Tuning to Large Language Models","abstract":"Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters. A family of parameter-efficient sparse fine-tuning (SFT) methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs. In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. At any given time, for a desired density level, we maintain an array of parameter indices and the deltas of these parameters relative to their pretrained values. We iterate among: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices. For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer. We experiment with instruction-tuning of LLMs on standard dataset mixtures, finding that SFT is often superior to popular parameter-efficient fine-tuning methods like LoRA (low-rank adaptation) in terms of performance and comparable in terms of run time. We additionally show that SFT is compatible with both quantization and efficient optimizers, to facilitate scaling to ever-larger model sizes. We release the code for SFT at https://github.com/AlanAnsell/peft and for the instruction-tuning experiments at https://github.com/ducdauge/sft-llm.","sentences":["Large Language Models (LLMs) are difficult to fully fine-tune (e.g., with instructions or human feedback) due to their sheer number of parameters.","A family of parameter-efficient sparse fine-tuning (SFT) methods have proven promising in terms of performance but their memory requirements increase proportionally to the size of the LLMs.","In this work, we scale sparse fine-tuning to state-of-the-art LLMs like LLaMA 2 7B and 13B. At any given time, for a desired density level, we maintain an array of parameter indices and the deltas of these parameters relative to their pretrained values.","We iterate among: (a) updating the active deltas, (b) pruning indices (based on the change of magnitude of their deltas) and (c) regrowth of indices.","For regrowth, we explore two criteria based on either the accumulated gradients of a few candidate parameters or their approximate momenta estimated using the efficient SM3 optimizer.","We experiment with instruction-tuning of LLMs on standard dataset mixtures, finding that SFT is often superior to popular parameter-efficient fine-tuning methods like LoRA (low-rank adaptation) in terms of performance and comparable in terms of run time.","We additionally show that SFT is compatible with both quantization and efficient optimizers, to facilitate scaling to ever-larger model sizes.","We release the code for SFT at https://github.com/AlanAnsell/peft and for the instruction-tuning experiments at https://github.com/ducdauge/sft-llm."],"url":"http://arxiv.org/abs/2401.16405v1"}
{"created":"2024-01-29 18:41:39","title":"ViLexNorm: A Lexical Normalization Corpus for Vietnamese Social Media Text","abstract":"Lexical normalization, a fundamental task in Natural Language Processing (NLP), involves the transformation of words into their canonical forms. This process has been proven to benefit various downstream NLP tasks greatly. In this work, we introduce Vietnamese Lexical Normalization (ViLexNorm), the first-ever corpus developed for the Vietnamese lexical normalization task. The corpus comprises over 10,000 pairs of sentences meticulously annotated by human annotators, sourced from public comments on Vietnam's most popular social media platforms. Various methods were used to evaluate our corpus, and the best-performing system achieved a result of 57.74% using the Error Reduction Rate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline. For extrinsic evaluation, employing the model trained on ViLexNorm demonstrates the positive impact of the Vietnamese lexical normalization task on other NLP tasks. Our corpus is publicly available exclusively for research purposes.","sentences":["Lexical normalization, a fundamental task in Natural Language Processing (NLP), involves the transformation of words into their canonical forms.","This process has been proven to benefit various downstream NLP tasks greatly.","In this work, we introduce Vietnamese Lexical Normalization (ViLexNorm), the first-ever corpus developed for the Vietnamese lexical normalization task.","The corpus comprises over 10,000 pairs of sentences meticulously annotated by human annotators, sourced from public comments on Vietnam's most popular social media platforms.","Various methods were used to evaluate our corpus, and the best-performing system achieved a result of 57.74% using the Error Reduction Rate (ERR) metric (van der Goot, 2019a) with the Leave-As-Is (LAI) baseline.","For extrinsic evaluation, employing the model trained on ViLexNorm demonstrates the positive impact of the Vietnamese lexical normalization task on other NLP tasks.","Our corpus is publicly available exclusively for research purposes."],"url":"http://arxiv.org/abs/2401.16403v1"}
{"created":"2024-01-29 18:41:21","title":"A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect","abstract":"Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the concept of normality in visual data, widely applied across diverse domains, e.g., industrial defect inspection, and medical lesion detection. This survey comprehensively examines recent advancements in VAD by identifying three primary challenges: 1) scarcity of training data, 2) diversity of visual modalities, and 3) complexity of hierarchical anomalies. Starting with a brief overview of the VAD background and its generic concept definitions, we progressively categorize, emphasize, and discuss the latest VAD progress from the perspective of sample number, data modality, and anomaly hierarchy. Through an in-depth analysis of the VAD field, we finally summarize future developments for VAD and conclude the key findings and contributions of this survey.","sentences":["Visual Anomaly Detection (VAD) endeavors to pinpoint deviations from the concept of normality in visual data, widely applied across diverse domains, e.g., industrial defect inspection, and medical lesion detection.","This survey comprehensively examines recent advancements in VAD by identifying three primary challenges: 1) scarcity of training data, 2) diversity of visual modalities, and 3) complexity of hierarchical anomalies.","Starting with a brief overview of the VAD background and its generic concept definitions, we progressively categorize, emphasize, and discuss the latest VAD progress from the perspective of sample number, data modality, and anomaly hierarchy.","Through an in-depth analysis of the VAD field, we finally summarize future developments for VAD and conclude the key findings and contributions of this survey."],"url":"http://arxiv.org/abs/2401.16402v1"}
{"created":"2024-01-29 18:39:08","title":"Single-Winner Voting with Alliances: Avoiding the Spoiler Effect","abstract":"We study the setting of single-winner elections with ordinal preferences where candidates might be members of \\emph{alliances} (which may correspond to e.g., political parties, factions, or coalitions). However, we do not assume that candidates from the same alliance are necessarily adjacent in voters' rankings. In such case, every classical voting rule is vulnerable to the spoiler effect, i.e., the presence of a candidate may harm his or her alliance. We therefore introduce a new idea of \\emph{alliance-aware} voting rules which extend the classical ones. We show that our approach is superior both to using classical cloneproof voting rules and to running primaries within alliances before the election.   We introduce several alliance-aware voting rules and show that they satisfy the most desirable standard properties of their classical counterparts as well as newly introduced axioms for the model with alliances which, e.g., exclude the possibility of the spoiler effect. Our rules have natural definitions and are simple enough to explain to be used in practice.","sentences":["We study the setting of single-winner elections with ordinal preferences where candidates might be members of \\emph{alliances} (which may correspond to e.g., political parties, factions, or coalitions).","However, we do not assume that candidates from the same alliance are necessarily adjacent in voters' rankings.","In such case, every classical voting rule is vulnerable to the spoiler effect, i.e., the presence of a candidate may harm his or her alliance.","We therefore introduce a new idea of \\emph{alliance-aware} voting rules which extend the classical ones.","We show that our approach is superior both to using classical cloneproof voting rules and to running primaries within alliances before the election.   ","We introduce several alliance-aware voting rules and show that they satisfy the most desirable standard properties of their classical counterparts as well as newly introduced axioms for the model with alliances which, e.g., exclude the possibility of the spoiler effect.","Our rules have natural definitions and are simple enough to explain to be used in practice."],"url":"http://arxiv.org/abs/2401.16399v1"}
{"created":"2024-01-29 18:38:29","title":"Zero-shot Imitation Policy via Search in Demonstration Dataset","abstract":"Behavioral cloning uses a dataset of demonstrations to learn a policy. To overcome computationally expensive training procedures and address the policy adaptation problem, we propose to use latent spaces of pre-trained foundation models to index a demonstration dataset, instantly access similar relevant experiences, and copy behavior from these situations. Actions from a selected similar situation can be performed by the agent until representations of the agent's current situation and the selected experience diverge in the latent space. Thus, we formulate our control problem as a dynamic search problem over a dataset of experts' demonstrations. We test our approach on BASALT MineRL-dataset in the latent representation of a Video Pre-Training model. We compare our model to state-of-the-art, Imitation Learning-based Minecraft agents. Our approach can effectively recover meaningful demonstrations and show human-like behavior of an agent in the Minecraft environment in a wide variety of scenarios. Experimental results reveal that performance of our search-based approach clearly wins in terms of accuracy and perceptual evaluation over learning-based models.","sentences":["Behavioral cloning uses a dataset of demonstrations to learn a policy.","To overcome computationally expensive training procedures and address the policy adaptation problem, we propose to use latent spaces of pre-trained foundation models to index a demonstration dataset, instantly access similar relevant experiences, and copy behavior from these situations.","Actions from a selected similar situation can be performed by the agent until representations of the agent's current situation and the selected experience diverge in the latent space.","Thus, we formulate our control problem as a dynamic search problem over a dataset of experts' demonstrations.","We test our approach on BASALT MineRL-dataset in the latent representation of a Video Pre-Training model.","We compare our model to state-of-the-art, Imitation Learning-based Minecraft agents.","Our approach can effectively recover meaningful demonstrations and show human-like behavior of an agent in the Minecraft environment in a wide variety of scenarios.","Experimental results reveal that performance of our search-based approach clearly wins in terms of accuracy and perceptual evaluation over learning-based models."],"url":"http://arxiv.org/abs/2401.16398v1"}
{"created":"2024-01-29 18:36:28","title":"Deciding Subtyping for Asynchronous Multiparty Sessions","abstract":"Multiparty session types (MSTs) are a type-based approach to verifying communication protocols, represented as global types in the framework. We present a precise subtyping relation for asynchronous MSTs with communicating state machines (CSMs) as implementation model. We address two problems: when can a local implementation safely substitute another, and when does an arbitrary CSM implement a global type? We define safety with respect to a given global type, in terms of subprotocol fidelity and deadlock freedom. Our implementation model subsumes existing work which considers local types with restricted choice. We exploit the connection between MST subtyping and refinement to formulate concise conditions that are directly checkable on the candidate implementations, and use them to show that both problems are decidable in polynomial time.","sentences":["Multiparty session types (MSTs) are a type-based approach to verifying communication protocols, represented as global types in the framework.","We present a precise subtyping relation for asynchronous MSTs with communicating state machines (CSMs) as implementation model.","We address two problems: when can a local implementation safely substitute another, and when does an arbitrary CSM implement a global type?","We define safety with respect to a given global type, in terms of subprotocol fidelity and deadlock freedom.","Our implementation model subsumes existing work which considers local types with restricted choice.","We exploit the connection between MST subtyping and refinement to formulate concise conditions that are directly checkable on the candidate implementations, and use them to show that both problems are decidable in polynomial time."],"url":"http://arxiv.org/abs/2401.16395v1"}
{"created":"2024-01-29 18:34:36","title":"Amazon's 2023 Drought: Sentinel-1 Reveals Extreme Rio Negro River Contraction","abstract":"The Amazon, the world's largest rainforest, faces a severe historic drought. The Rio Negro River, one of the major Amazon River tributaries, reaches its lowest level in a century in October 2023. Here, we used a U-net deep learning model to map water surfaces in the Rio Negro River basin every 12 days in 2022 and 2023 using 10 m spatial resolution Sentinel-1 satellite radar images. The accuracy of the water surface model was high with an F1-score of 0.93. The 12 days mosaic time series of water surface was generated from the Sentinel-1 prediction. The water surface mask demonstrated relatively consistent agreement with the Global Surface Water (GSW) product from Joint Research Centre (F1-score: 0.708) and with the Brazilian Mapbiomas Water initiative (F1-score: 0.686). The main errors of the map were omission errors in flooded woodland, in flooded shrub and because of clouds. Rio Negro water surfaces reached their lowest level around the 25th of November 2023 and were reduced to 68.1\\% (9,559.9 km$^2$) of the maximum water surfaces observed in the period 2022-2023 (14,036.3 km$^2$). Synthetic Aperture Radar (SAR) data, in conjunction with deep learning techniques, can significantly improve near real-time mapping of water surface in tropical regions.","sentences":["The Amazon, the world's largest rainforest, faces a severe historic drought.","The Rio Negro River, one of the major Amazon River tributaries, reaches its lowest level in a century in October 2023.","Here, we used a U-net deep learning model to map water surfaces in the Rio Negro River basin every 12 days in 2022 and 2023 using 10 m spatial resolution Sentinel-1 satellite radar images.","The accuracy of the water surface model was high with an F1-score of 0.93.","The 12 days mosaic time series of water surface was generated from the Sentinel-1 prediction.","The water surface mask demonstrated relatively consistent agreement with the Global Surface Water (GSW) product from Joint Research Centre (F1-score: 0.708) and with the Brazilian Mapbiomas Water initiative (F1-score: 0.686).","The main errors of the map were omission errors in flooded woodland, in flooded shrub and because of clouds.","Rio Negro water surfaces reached their lowest level around the 25th of November 2023 and were reduced to 68.1\\% (9,559.9 km$^2$) of the maximum water surfaces observed in the period 2022-2023 (14,036.3 km$^2$).","Synthetic Aperture Radar (SAR) data, in conjunction with deep learning techniques, can significantly improve near real-time mapping of water surface in tropical regions."],"url":"http://arxiv.org/abs/2401.16393v1"}
{"created":"2024-01-29 18:32:19","title":"Quantum Private Membership Aggregation","abstract":"We consider the problem of private set membership aggregation of $N$ parties by using an entangled quantum state. In this setting, the $N$ parties, which share an entangled state, aim to \\emph{privately} know the number of times each element (message) is repeated among the $N$ parties, with respect to a universal set $\\mathcal{K}$. This problem has applications in private comparison, ranking, voting, etc. We propose an encoding algorithm that maps the classical information into distinguishable quantum states, along with a decoding algorithm that exploits the distinguishability of the mapped states. The proposed scheme can also be used to calculate the $N$ party private summation modulo $P$.","sentences":["We consider the problem of private set membership aggregation of $N$ parties by using an entangled quantum state.","In this setting, the $N$ parties, which share an entangled state, aim to \\emph{privately} know the number of times each element (message) is repeated among the $N$ parties, with respect to a universal set $\\mathcal{K}$. This problem has applications in private comparison, ranking, voting, etc.","We propose an encoding algorithm that maps the classical information into distinguishable quantum states, along with a decoding algorithm that exploits the distinguishability of the mapped states.","The proposed scheme can also be used to calculate the $N$ party private summation modulo $P$."],"url":"http://arxiv.org/abs/2401.16390v1"}
{"created":"2024-01-29 18:28:28","title":"Green Adaptation of Real-Time Web Services for Industrial CPS within a Cloud Environment","abstract":"Managing energy efficiency under timing constraints is an interesting and big challenge. This work proposes an accurate power model in data centers for time-constrained servers in Cloud computing. This model, as opposed to previous approaches, does not only consider the workload assigned to the processing element, but also incorporates the need of considering the static power consumption and, even more interestingly, its dependency with temperature. The proposed model has been used in a multi-objective optimization environment in which the Dynamic Voltage and Frequency Scaling (DVFS) and workload assignment have been efficiently optimized.","sentences":["Managing energy efficiency under timing constraints is an interesting and big challenge.","This work proposes an accurate power model in data centers for time-constrained servers in Cloud computing.","This model, as opposed to previous approaches, does not only consider the workload assigned to the processing element, but also incorporates the need of considering the static power consumption and, even more interestingly, its dependency with temperature.","The proposed model has been used in a multi-objective optimization environment in which the Dynamic Voltage and Frequency Scaling (DVFS) and workload assignment have been efficiently optimized."],"url":"http://arxiv.org/abs/2401.16387v1"}
{"created":"2024-01-29 18:27:52","title":"Continual Learning with Pre-Trained Models: A Survey","abstract":"Nowadays, real-world applications often face streaming data, which requires the learning system to absorb new knowledge as data evolves. Continual Learning (CL) aims to achieve this goal and meanwhile overcome the catastrophic forgetting of former knowledge when learning new ones. Typical CL methods build the model from scratch to grow with incoming data. However, the advent of the pre-trained model (PTM) era has sparked immense research interest, particularly in leveraging PTMs' robust representational capabilities. This paper presents a comprehensive survey of the latest advancements in PTM-based CL. We categorize existing methodologies into three distinct groups, providing a comparative analysis of their similarities, differences, and respective advantages and disadvantages. Additionally, we offer an empirical study contrasting various state-of-the-art methods to highlight concerns regarding fairness in comparisons. The source code to reproduce these evaluations is available at: https://github.com/sun-hailong/LAMDA-PILOT","sentences":["Nowadays, real-world applications often face streaming data, which requires the learning system to absorb new knowledge as data evolves.","Continual Learning (CL) aims to achieve this goal and meanwhile overcome the catastrophic forgetting of former knowledge when learning new ones.","Typical CL methods build the model from scratch to grow with incoming data.","However, the advent of the pre-trained model (PTM) era has sparked immense research interest, particularly in leveraging PTMs' robust representational capabilities.","This paper presents a comprehensive survey of the latest advancements in PTM-based CL.","We categorize existing methodologies into three distinct groups, providing a comparative analysis of their similarities, differences, and respective advantages and disadvantages.","Additionally, we offer an empirical study contrasting various state-of-the-art methods to highlight concerns regarding fairness in comparisons.","The source code to reproduce these evaluations is available at: https://github.com/sun-hailong/LAMDA-PILOT"],"url":"http://arxiv.org/abs/2401.16386v1"}
{"created":"2024-01-29 18:24:16","title":"Learning logic programs by finding minimal unsatisfiable subprograms","abstract":"The goal of inductive logic programming (ILP) is to search for a logic program that generalises training examples and background knowledge. We introduce an ILP approach that identifies minimal unsatisfiable subprograms (MUSPs). We show that finding MUSPs allows us to efficiently and soundly prune the search space. Our experiments on multiple domains, including program synthesis and game playing, show that our approach can reduce learning times by 99%.","sentences":["The goal of inductive logic programming (ILP) is to search for a logic program that generalises training examples and background knowledge.","We introduce an ILP approach that identifies minimal unsatisfiable subprograms (MUSPs).","We show that finding MUSPs allows us to efficiently and soundly prune the search space.","Our experiments on multiple domains, including program synthesis and game playing, show that our approach can reduce learning times by 99%."],"url":"http://arxiv.org/abs/2401.16383v1"}
{"created":"2024-01-29 18:22:11","title":"A KDM-Based Approach for Architecture Conformance Checking in Adaptive Systems","abstract":"Adaptive Systems (ASs) are capable to monitor their behavior and make adjustments when quality goals are not achieved through the MAPE-K, a widely recognized reference model that offers abstractions for designing ASs. By making these abstractions evident in the system structure, numerous benefits emerge, particularly in terms of enhancing the architecture's maintenance and comprehensibility. However, it is observed that many existing ASs are not designed in accordance with MAPE-K, causing these abstractions to remain hidden in their architecture. To address this issue, Architectural Conformance Checking (ACC) emerges as a valuable technique for verifying whether the current architecture (CA) of a system adheres to the rules prescribed by the planned architecture (PA) or a reference model, such as MAPE-K. In this paper, we present REMEDY, a domain-specific approach that encompasses the specification of the planned adaptive architecture based on the MAPE-K reference model, the recovery of the current adaptive architecture, the conformance checking process, and architecture visualizations. Furthermore, our approach is specifically tailored for ASs, incorporating well-known rules from the MAPE-K model. The evaluation of the REMEDY DSL involves a comparison with a general-purpose DSL, and the results demonstrate improvements in productivity. REMEDY facilitates the identification and correction of architectural non-conformance issues, thereby enhancing the overall quality of adaptive systems.","sentences":["Adaptive Systems (ASs) are capable to monitor their behavior and make adjustments when quality goals are not achieved through the MAPE-K, a widely recognized reference model that offers abstractions for designing ASs.","By making these abstractions evident in the system structure, numerous benefits emerge, particularly in terms of enhancing the architecture's maintenance and comprehensibility.","However, it is observed that many existing ASs are not designed in accordance with MAPE-K, causing these abstractions to remain hidden in their architecture.","To address this issue, Architectural Conformance Checking (ACC) emerges as a valuable technique for verifying whether the current architecture (CA) of a system adheres to the rules prescribed by the planned architecture (PA) or a reference model, such as MAPE-K. In this paper, we present REMEDY, a domain-specific approach that encompasses the specification of the planned adaptive architecture based on the MAPE-K reference model, the recovery of the current adaptive architecture, the conformance checking process, and architecture visualizations.","Furthermore, our approach is specifically tailored for ASs, incorporating well-known rules from the MAPE-K model.","The evaluation of the REMEDY DSL involves a comparison with a general-purpose DSL, and the results demonstrate improvements in productivity.","REMEDY facilitates the identification and correction of architectural non-conformance issues, thereby enhancing the overall quality of adaptive systems."],"url":"http://arxiv.org/abs/2401.16382v1"}
{"created":"2024-01-29 18:19:08","title":"Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling","abstract":"Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased. Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained. This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web. In this work, we propose Web Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as \"like Wikipedia\" or in \"question-answer format\" to jointly pre-train LLMs on real and synthetic rephrases. First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\\sim3x$. At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%. Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings. Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data.","sentences":["Large language models are trained on massive scrapes of the web, which are often unstructured, noisy, and poorly phrased.","Current scaling laws show that learning from such data requires an abundance of both compute and data, which grows with the size of the model being trained.","This is infeasible both because of the large compute costs and duration associated with pre-training, and the impending scarcity of high-quality data on the web.","In this work, we propose Web Rephrase Augmented Pre-training ($\\textbf{WRAP}$) that uses an off-the-shelf instruction-tuned model prompted to paraphrase documents on the web in specific styles such as \"like Wikipedia\" or in \"question-answer format\" to jointly pre-train LLMs on real and synthetic rephrases.","First, we show that using WRAP on the C4 dataset, which is naturally noisy, speeds up pre-training by $\\sim3x$.","At the same pre-training compute budget, it improves perplexity by more than 10% on average across different subsets of the Pile, and improves zero-shot question answer accuracy across 13 tasks by more than 2%.","Second, we investigate the impact of the re-phrasing style on the performance of the model, offering insights into how the composition of the training data can impact the performance of LLMs in OOD settings.","Our gains are attributed to the fact that re-phrased synthetic data has higher utility than just real data because it (i) incorporates style diversity that closely reflects downstream evaluation style, and (ii) has higher 'quality' than web-scraped data."],"url":"http://arxiv.org/abs/2401.16380v1"}
{"created":"2024-01-29 18:13:54","title":"Spot the Error: Non-autoregressive Graphic Layout Generation with Wireframe Locator","abstract":"Layout generation is a critical step in graphic design to achieve meaningful compositions of elements. Most previous works view it as a sequence generation problem by concatenating element attribute tokens (i.e., category, size, position). So far the autoregressive approach (AR) has achieved promising results, but is still limited in global context modeling and suffers from error propagation since it can only attend to the previously generated tokens. Recent non-autoregressive attempts (NAR) have shown competitive results, which provides a wider context range and the flexibility to refine with iterative decoding. However, current works only use simple heuristics to recognize erroneous tokens for refinement which is inaccurate. This paper first conducts an in-depth analysis to better understand the difference between the AR and NAR framework. Furthermore, based on our observation that pixel space is more sensitive in capturing spatial patterns of graphic layouts (e.g., overlap, alignment), we propose a learning-based locator to detect erroneous tokens which takes the wireframe image rendered from the generated layout sequence as input. We show that it serves as a complementary modality to the element sequence in object space and contributes greatly to the overall performance. Experiments on two public datasets show that our approach outperforms both AR and NAR baselines. Extensive studies further prove the effectiveness of different modules with interesting findings. Our code will be available at https://github.com/ffffatgoose/SpotError.","sentences":["Layout generation is a critical step in graphic design to achieve meaningful compositions of elements.","Most previous works view it as a sequence generation problem by concatenating element attribute tokens (i.e., category, size, position).","So far the autoregressive approach (AR) has achieved promising results, but is still limited in global context modeling and suffers from error propagation since it can only attend to the previously generated tokens.","Recent non-autoregressive attempts (NAR) have shown competitive results, which provides a wider context range and the flexibility to refine with iterative decoding.","However, current works only use simple heuristics to recognize erroneous tokens for refinement which is inaccurate.","This paper first conducts an in-depth analysis to better understand the difference between the AR and NAR framework.","Furthermore, based on our observation that pixel space is more sensitive in capturing spatial patterns of graphic layouts (e.g., overlap, alignment), we propose a learning-based locator to detect erroneous tokens which takes the wireframe image rendered from the generated layout sequence as input.","We show that it serves as a complementary modality to the element sequence in object space and contributes greatly to the overall performance.","Experiments on two public datasets show that our approach outperforms both AR and NAR baselines.","Extensive studies further prove the effectiveness of different modules with interesting findings.","Our code will be available at https://github.com/ffffatgoose/SpotError."],"url":"http://arxiv.org/abs/2401.16375v1"}
{"created":"2024-01-29 18:12:32","title":"Bayesian optimization as a flexible and efficient design framework for sustainable process systems","abstract":"Bayesian optimization (BO) is a powerful technology for optimizing noisy expensive-to-evaluate black-box functions, with a broad range of real-world applications in science, engineering, economics, manufacturing, and beyond. In this paper, we provide an overview of recent developments, challenges, and opportunities in BO for design of next-generation process systems. After describing several motivating applications, we discuss how advanced BO methods have been developed to more efficiently tackle important problems in these applications. We conclude the paper with a summary of challenges and opportunities related to improving the quality of the probabilistic model, the choice of internal optimization procedure used to select the next sample point, and the exploitation of problem structure to improve sample efficiency.","sentences":["Bayesian optimization (BO) is a powerful technology for optimizing noisy expensive-to-evaluate black-box functions, with a broad range of real-world applications in science, engineering, economics, manufacturing, and beyond.","In this paper, we provide an overview of recent developments, challenges, and opportunities in BO for design of next-generation process systems.","After describing several motivating applications, we discuss how advanced BO methods have been developed to more efficiently tackle important problems in these applications.","We conclude the paper with a summary of challenges and opportunities related to improving the quality of the probabilistic model, the choice of internal optimization procedure used to select the next sample point, and the exploitation of problem structure to improve sample efficiency."],"url":"http://arxiv.org/abs/2401.16373v1"}
{"created":"2024-01-29 18:10:01","title":"Mixed-Order Meshes through rp-adaptivity for Surface Fitting to Implicit Geometries","abstract":"Computational analysis with the finite element method requires geometrically accurate meshes. It is well known that high-order meshes can accurately capture curved surfaces with fewer degrees of freedom in comparison to low-order meshes. Existing techniques for high-order mesh generation typically output meshes with same polynomial order for all elements. However, high order elements away from curvilinear boundaries or interfaces increase the computational cost of the simulation without increasing geometric accuracy. In prior work, we have presented one such approach for generating body-fitted uniform-order meshes that takes a given mesh and morphs it to align with the surface of interest prescribed as the zero isocontour of a level-set function. We extend this method to generate mixed-order meshes such that curved surfaces of the domain are discretized with high-order elements, while low-order elements are used elsewhere. Numerical experiments demonstrate the robustness of the approach and show that it can be used to generate mixed-order meshes that are much more efficient than high uniform-order meshes. The proposed approach is purely algebraic, and extends to different types of elements (quadrilaterals/triangles/tetrahedron/hexahedra) in two- and three-dimensions.","sentences":["Computational analysis with the finite element method requires geometrically accurate meshes.","It is well known that high-order meshes can accurately capture curved surfaces with fewer degrees of freedom in comparison to low-order meshes.","Existing techniques for high-order mesh generation typically output meshes with same polynomial order for all elements.","However, high order elements away from curvilinear boundaries or interfaces increase the computational cost of the simulation without increasing geometric accuracy.","In prior work, we have presented one such approach for generating body-fitted uniform-order meshes that takes a given mesh and morphs it to align with the surface of interest prescribed as the zero isocontour of a level-set function.","We extend this method to generate mixed-order meshes such that curved surfaces of the domain are discretized with high-order elements, while low-order elements are used elsewhere.","Numerical experiments demonstrate the robustness of the approach and show that it can be used to generate mixed-order meshes that are much more efficient than high uniform-order meshes.","The proposed approach is purely algebraic, and extends to different types of elements (quadrilaterals/triangles/tetrahedron/hexahedra) in two- and three-dimensions."],"url":"http://arxiv.org/abs/2401.16369v1"}
{"created":"2024-01-29 18:07:56","title":"TQCompressor: improving tensor decomposition methods in neural networks via permutations","abstract":"We introduce TQCompressor, a novel method for neural network model compression with improved tensor decompositions. We explore the challenges posed by the computational and storage demands of pre-trained language models in NLP tasks and propose a permutation-based enhancement to Kronecker decomposition. This enhancement makes it possible to reduce loss in model expressivity which is usually associated with factorization. We demonstrate this method applied to the GPT-2$_{small}$. The result of the compression is TQCompressedGPT-2 model, featuring 81 mln. parameters compared to 124 mln. in the GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available. We further enhance the performance of the TQCompressedGPT-2 through a training strategy involving multi-step knowledge distillation, using only a 3.1% of the OpenWebText. TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative evaluations, marking an advancement in the efficient and effective deployment of models in resource-constrained environments.","sentences":["We introduce TQCompressor, a novel method for neural network model compression with improved tensor decompositions.","We explore the challenges posed by the computational and storage demands of pre-trained language models in NLP tasks and propose a permutation-based enhancement to Kronecker decomposition.","This enhancement makes it possible to reduce loss in model expressivity which is usually associated with factorization.","We demonstrate this method applied to the GPT-2$_{small}$.","The result of the compression is TQCompressedGPT-2 model, featuring 81 mln.","parameters compared to 124 mln.","in the GPT-2$_{small}$. We make TQCompressedGPT-2 publicly available.","We further enhance the performance of the TQCompressedGPT-2 through a training strategy involving multi-step knowledge distillation, using only a 3.1% of the OpenWebText.","TQCompressedGPT-2 surpasses DistilGPT-2 and KnGPT-2 in comparative evaluations, marking an advancement in the efficient and effective deployment of models in resource-constrained environments."],"url":"http://arxiv.org/abs/2401.16367v1"}
{"created":"2024-01-29 18:06:24","title":"Choiceless Polynomial Space","abstract":"Abstract State Machines (ASMs) provide a model of computations on structures rather than strings. Blass, Gurevich and Shelah showed that deterministic PTIME-bounded ASMs define the choiceless fragment of PTIME, but cannot capture PTIME. In this article deterministic PSPACE-bounded ASMs are introduced, and it is proven that they cannot capture PSPACE. The key for the proof is a characterisation by partial fixed-point formulae over the St\\\"ark/Nanchen logic for deterministic ASMs and a construction of transitive structures, in which such formulae must hold. This construction exploits that the decisive support theorem for choiceless polynomial time holds under slightly weaker assumptions.","sentences":["Abstract State Machines (ASMs) provide a model of computations on structures rather than strings.","Blass, Gurevich and Shelah showed that deterministic PTIME-bounded ASMs define the choiceless fragment of PTIME, but cannot capture PTIME.","In this article deterministic PSPACE-bounded ASMs are introduced, and it is proven that they cannot capture PSPACE.","The key for the proof is a characterisation by partial fixed-point formulae over the St\\\"ark/Nanchen logic for deterministic ASMs and a construction of transitive structures, in which such formulae must hold.","This construction exploits that the decisive support theorem for choiceless polynomial time holds under slightly weaker assumptions."],"url":"http://arxiv.org/abs/2401.16366v1"}
{"created":"2024-01-29 18:00:50","title":"Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus","abstract":"OpenAlex is a promising open source of scholarly metadata, and competitor to the established proprietary sources, the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this empirical paper, we will study the reference and metadata coverage within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16,788,282 recent publications shared by all three databases, OpenAlex has average reference numbers comparable to both Web of Science and Scopus. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results, with OpenAlex capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access information per article when compared to both Web of Science and Scopus.","sentences":["OpenAlex is a promising open source of scholarly metadata, and competitor to the established proprietary sources, the Web of Science and Scopus.","As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers.","However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data.","In this empirical paper, we will study the reference and metadata coverage within each database and compare them with each other to help address this open question in bibliometrics.","In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16,788,282 recent publications shared by all three databases, OpenAlex has average reference numbers comparable to both Web of Science and Scopus.","We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results, with OpenAlex capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access information per article when compared to both Web of Science and Scopus."],"url":"http://arxiv.org/abs/2401.16359v1"}
{"created":"2024-01-29 17:59:19","title":"PathMMU: A Massive Multimodal Expert-Level Benchmark for Understanding and Reasoning in Pathology","abstract":"The emergence of large multimodal models has unlocked remarkable potential in AI, particularly in pathology. However, the lack of specialized, high-quality benchmark impeded their development and precise evaluation. To address this, we introduce PathMMU, the largest and highest-quality expert-validated pathology benchmark for LMMs. It comprises 33,573 multimodal multi-choice questions and 21,599 images from various sources, and an explanation for the correct answer accompanies each question. The construction of PathMMU capitalizes on the robust capabilities of GPT-4V, utilizing approximately 30,000 gathered image-caption pairs to generate Q\\&As. Significantly, to maximize PathMMU's authority, we invite six pathologists to scrutinize each question under strict standards in PathMMU's validation and test sets, while simultaneously setting an expert-level performance benchmark for PathMMU. We conduct extensive evaluations, including zero-shot assessments of 14 open-sourced and three closed-sourced LMMs and their robustness to image corruption. We also fine-tune representative LMMs to assess their adaptability to PathMMU. The empirical findings indicate that advanced LMMs struggle with the challenging PathMMU benchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7\\% zero-shot performance, significantly lower than the 71.4\\% demonstrated by human pathologists. After fine-tuning, even open-sourced LMMs can surpass GPT-4V with a performance of over 60\\%, but still fall short of the expertise shown by pathologists. We hope that the PathMMU will offer valuable insights and foster the development of more specialized, next-generation LLMs for pathology.","sentences":["The emergence of large multimodal models has unlocked remarkable potential in AI, particularly in pathology.","However, the lack of specialized, high-quality benchmark impeded their development and precise evaluation.","To address this, we introduce PathMMU, the largest and highest-quality expert-validated pathology benchmark for LMMs.","It comprises 33,573 multimodal multi-choice questions and 21,599 images from various sources, and an explanation for the correct answer accompanies each question.","The construction of PathMMU capitalizes on the robust capabilities of GPT-4V, utilizing approximately 30,000 gathered image-caption pairs to generate Q\\&As.","Significantly, to maximize PathMMU's authority, we invite six pathologists to scrutinize each question under strict standards in PathMMU's validation and test sets, while simultaneously setting an expert-level performance benchmark for PathMMU.","We conduct extensive evaluations, including zero-shot assessments of 14 open-sourced and three closed-sourced LMMs and their robustness to image corruption.","We also fine-tune representative LMMs to assess their adaptability to PathMMU.","The empirical findings indicate that advanced LMMs struggle with the challenging PathMMU benchmark, with the top-performing LMM, GPT-4V, achieving only a 51.7\\% zero-shot performance, significantly lower than the 71.4\\% demonstrated by human pathologists.","After fine-tuning, even open-sourced LMMs can surpass GPT-4V with a performance of over 60\\%, but still fall short of the expertise shown by pathologists.","We hope that the PathMMU will offer valuable insights and foster the development of more specialized, next-generation LLMs for pathology."],"url":"http://arxiv.org/abs/2401.16355v1"}
{"created":"2024-01-29 17:56:45","title":"Empirical and Theoretical Analysis of Liquid Staking Protocols","abstract":"Liquid staking has become the largest category of decentralized finance protocols in terms of total value locked. However, few studies exist on its implementation designs or underlying risks. The liquid staking protocols allow for earning staking rewards without the disadvantage of locking the capital at the validators. Yet, they are seen by some as a threat to the Proof-of-Stake blockchain security.   This paper is the first work that classifies liquid staking implementations. It analyzes the historical performance of major liquid staking tokens in comparison to the traditional staking for the largest Proof-of-Stake blockchains. Furthermore, the research investigates the impact of centralization, maximum extractable value and the migration of Ethereum from Proof-of-Work to Proof-of-Stake on the tokens' performance. Examining the tracking error of the liquid stacking providers to the staking rewards shows that they are persistent and cannot be explained by macro-variables of the currency, such as the variance or return.","sentences":["Liquid staking has become the largest category of decentralized finance protocols in terms of total value locked.","However, few studies exist on its implementation designs or underlying risks.","The liquid staking protocols allow for earning staking rewards without the disadvantage of locking the capital at the validators.","Yet, they are seen by some as a threat to the Proof-of-Stake blockchain security.   ","This paper is the first work that classifies liquid staking implementations.","It analyzes the historical performance of major liquid staking tokens in comparison to the traditional staking for the largest Proof-of-Stake blockchains.","Furthermore, the research investigates the impact of centralization, maximum extractable value and the migration of Ethereum from Proof-of-Work to Proof-of-Stake on the tokens' performance.","Examining the tracking error of the liquid stacking providers to the staking rewards shows that they are persistent and cannot be explained by macro-variables of the currency, such as the variance or return."],"url":"http://arxiv.org/abs/2401.16353v1"}
{"created":"2024-01-29 17:56:42","title":"Adversarial Training on Purification (AToP): Advancing Both Robustness and Generalization","abstract":"The deep neural networks are known to be vulnerable to well-designed adversarial attacks. The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks. Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness. Meanwhile, both methods share one common limitation on the degraded standard accuracy. To mitigate these issues, we propose a novel framework called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss. RT is essential to avoid overlearning to known attacks resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness. To evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves state-of-the-art results and exhibits generalization ability against unseen attacks.","sentences":["The deep neural networks are known to be vulnerable to well-designed adversarial attacks.","The most successful defense technique based on adversarial training (AT) can achieve optimal robustness against particular attacks but cannot generalize well to unseen attacks.","Another effective defense technique based on adversarial purification (AP) can enhance generalization but cannot achieve optimal robustness.","Meanwhile, both methods share one common limitation on the degraded standard accuracy.","To mitigate these issues, we propose a novel framework called Adversarial Training on Purification (AToP), which comprises two components: perturbation destruction by random transforms (RT) and purifier model fine-tuned (FT) by adversarial loss.","RT is essential to avoid overlearning to known attacks resulting in the robustness generalization to unseen attacks and FT is essential for the improvement of robustness.","To evaluate our method in an efficient and scalable way, we conduct extensive experiments on CIFAR-10, CIFAR-100, and ImageNette to demonstrate that our method achieves state-of-the-art results and exhibits generalization ability against unseen attacks."],"url":"http://arxiv.org/abs/2401.16352v1"}
{"created":"2024-01-29 17:56:15","title":"FedFair^3: Unlocking Threefold Fairness in Federated Learning","abstract":"Federated Learning (FL) is an emerging paradigm in machine learning without exposing clients' raw data. In practical scenarios with numerous clients, encouraging fair and efficient client participation in federated learning is of utmost importance, which is also challenging given the heterogeneity in data distribution and device properties. Existing works have proposed different client-selection methods that consider fairness; however, they fail to select clients with high utilities while simultaneously achieving fair accuracy levels. In this paper, we propose a fair client-selection approach that unlocks threefold fairness in federated learning. In addition to having a fair client-selection strategy, we enforce an equitable number of rounds for client participation and ensure a fair accuracy distribution over the clients. The experimental results demonstrate that FedFair^3, in comparison to the state-of-the-art baselines, achieves 18.15% less accuracy variance on the IID data and 54.78% on the non-IID data, without decreasing the global accuracy. Furthermore, it shows 24.36% less wall-clock training time on average.","sentences":["Federated Learning (FL) is an emerging paradigm in machine learning without exposing clients' raw data.","In practical scenarios with numerous clients, encouraging fair and efficient client participation in federated learning is of utmost importance, which is also challenging given the heterogeneity in data distribution and device properties.","Existing works have proposed different client-selection methods that consider fairness; however, they fail to select clients with high utilities while simultaneously achieving fair accuracy levels.","In this paper, we propose a fair client-selection approach that unlocks threefold fairness in federated learning.","In addition to having a fair client-selection strategy, we enforce an equitable number of rounds for client participation and ensure a fair accuracy distribution over the clients.","The experimental results demonstrate that FedFair^3, in comparison to the state-of-the-art baselines, achieves 18.15% less accuracy variance on the IID data and 54.78% on the non-IID data, without decreasing the global accuracy.","Furthermore, it shows 24.36% less wall-clock training time on average."],"url":"http://arxiv.org/abs/2401.16350v1"}
{"created":"2024-01-29 17:55:18","title":"ConFit: Improving Resume-Job Matching using Data Augmentation and Contrastive Learning","abstract":"A reliable resume-job matching system helps a company find suitable candidates from a pool of resumes, and helps a job seeker find relevant jobs from a list of job posts. However, since job seekers apply only to a few jobs, interaction records in resume-job datasets are sparse. Different from many prior work that use complex modeling techniques, we tackle this sparsity problem using data augmentations and a simple contrastive learning approach. ConFit first creates an augmented resume-job dataset by paraphrasing specific sections in a resume or a job post. Then, ConFit uses contrastive learning to further increase training samples from $B$ pairs per batch to $O(B^2)$ per batch. We evaluate ConFit on two real-world datasets and find it outperforms prior methods (including BM25 and OpenAI text-ada-002) by up to 19% and 31% absolute in nDCG@10 for ranking jobs and ranking resumes, respectively.","sentences":["A reliable resume-job matching system helps a company find suitable candidates from a pool of resumes, and helps a job seeker find relevant jobs from a list of job posts.","However, since job seekers apply only to a few jobs, interaction records in resume-job datasets are sparse.","Different from many prior work that use complex modeling techniques, we tackle this sparsity problem using data augmentations and a simple contrastive learning approach.","ConFit first creates an augmented resume-job dataset by paraphrasing specific sections in a resume or a job post.","Then, ConFit uses contrastive learning to further increase training samples from $B$ pairs per batch to $O(B^2)$ per batch.","We evaluate ConFit on two real-world datasets and find it outperforms prior methods (including BM25 and OpenAI text-ada-002) by up to 19% and 31% absolute in nDCG@10 for ranking jobs and ranking resumes, respectively."],"url":"http://arxiv.org/abs/2401.16349v1"}
{"created":"2024-01-29 17:54:04","title":"Beyond Automated Evaluation Metrics: Evaluating Topic Models On Practical Social Science Content Analysis Tasks","abstract":"Topic models are a popular tool for understanding text collections, but their evaluation has been a point of contention. Automated evaluation metrics such as coherence are often used, however, their validity has been questioned for neural topic models (NTMs) and can overlook the benefits of a model in real world applications. To this end, we conduct the first evaluation of neural, supervised and classical topic models in an interactive task based setting. We combine topic models with a classifier and test their ability to help humans conduct content analysis and document annotation. From simulated, real user and expert pilot studies, the Contextual Neural Topic Model does the best on cluster evaluation metrics and human evaluations; however, LDA is competitive with two other NTMs under our simulated experiment and user study results, contrary to what coherence scores suggest. We show that current automated metrics do not provide a complete picture of topic modeling capabilities, but the right choice of NTMs can be better than classical models on practical tasks.","sentences":["Topic models are a popular tool for understanding text collections, but their evaluation has been a point of contention.","Automated evaluation metrics such as coherence are often used, however, their validity has been questioned for neural topic models (NTMs) and can overlook the benefits of a model in real world applications.","To this end, we conduct the first evaluation of neural, supervised and classical topic models in an interactive task based setting.","We combine topic models with a classifier and test their ability to help humans conduct content analysis and document annotation.","From simulated, real user and expert pilot studies, the Contextual Neural Topic Model does the best on cluster evaluation metrics and human evaluations; however, LDA is competitive with two other NTMs under our simulated experiment and user study results, contrary to what coherence scores suggest.","We show that current automated metrics do not provide a complete picture of topic modeling capabilities, but the right choice of NTMs can be better than classical models on practical tasks."],"url":"http://arxiv.org/abs/2401.16348v1"}
{"created":"2024-01-29 17:53:25","title":"Cross-Modal Coordination Across a Diverse Set of Input Modalities","abstract":"Cross-modal retrieval is the task of retrieving samples of a given modality by using queries of a different one. Due to the wide range of practical applications, the problem has been mainly focused on the vision and language case, e.g. text to image retrieval, where models like CLIP have proven effective in solving such tasks. The dominant approach to learning such coordinated representations consists of projecting them onto a common space where matching views stay close and those from non-matching pairs are pushed away from each other. Although this cross-modal coordination has been applied also to other pairwise combinations, extending it to an arbitrary number of diverse modalities is a problem that has not been fully explored in the literature. In this paper, we propose two different approaches to the problem. The first is based on an extension of the CLIP contrastive objective to an arbitrary number of input modalities, while the second departs from the contrastive formulation and tackles the coordination problem by regressing the cross-modal similarities towards a target that reflects two simple and intuitive constraints of the cross-modal retrieval task. We run experiments on two different datasets, over different combinations of input modalities and show that the approach is not only simple and effective but also allows for tackling the retrieval problem in novel ways. Besides capturing a more diverse set of pair-wise interactions, we show that we can use the learned representations to improve retrieval performance by combining the embeddings from two or more such modalities.","sentences":["Cross-modal retrieval is the task of retrieving samples of a given modality by using queries of a different one.","Due to the wide range of practical applications, the problem has been mainly focused on the vision and language case, e.g. text to image retrieval, where models like CLIP have proven effective in solving such tasks.","The dominant approach to learning such coordinated representations consists of projecting them onto a common space where matching views stay close and those from non-matching pairs are pushed away from each other.","Although this cross-modal coordination has been applied also to other pairwise combinations, extending it to an arbitrary number of diverse modalities is a problem that has not been fully explored in the literature.","In this paper, we propose two different approaches to the problem.","The first is based on an extension of the CLIP contrastive objective to an arbitrary number of input modalities, while the second departs from the contrastive formulation and tackles the coordination problem by regressing the cross-modal similarities towards a target that reflects two simple and intuitive constraints of the cross-modal retrieval task.","We run experiments on two different datasets, over different combinations of input modalities and show that the approach is not only simple and effective but also allows for tackling the retrieval problem in novel ways.","Besides capturing a more diverse set of pair-wise interactions, we show that we can use the learned representations to improve retrieval performance by combining the embeddings from two or more such modalities."],"url":"http://arxiv.org/abs/2401.16347v1"}
{"created":"2024-01-29 17:48:12","title":"On Achievable Rates for the Shotgun Sequencing Channel with Erasures","abstract":"In the shotgun sequencing channel, the input sequence (possibly, a long DNA sequence composed of nucleotide bases) is read into multiple fragments (called `reads') of much shorter lengths. In the context of DNA data storage, the capacity of this channel was identified in a recent work, assuming that the reads themselves are noiseless substrings of the original sequence. Modern shotgun sequencers however also output quality scores for each base read, indicating the confidence in its identification. Bases with low quality scores can be considered to be erased. Motivated by this, we consider the shotgun sequencing channel with erasures, where each symbol in any read can be independently erased with some probability $\\delta$. We identify achievable rates for this channel, using a random code construction and a decoder that uses typicality-like arguments to merge the reads.","sentences":["In the shotgun sequencing channel, the input sequence (possibly, a long DNA sequence composed of nucleotide bases) is read into multiple fragments (called `reads') of much shorter lengths.","In the context of DNA data storage, the capacity of this channel was identified in a recent work, assuming that the reads themselves are noiseless substrings of the original sequence.","Modern shotgun sequencers however also output quality scores for each base read, indicating the confidence in its identification.","Bases with low quality scores can be considered to be erased.","Motivated by this, we consider the shotgun sequencing channel with erasures, where each symbol in any read can be independently erased with some probability $\\delta$.","We identify achievable rates for this channel, using a random code construction and a decoder that uses typicality-like arguments to merge the reads."],"url":"http://arxiv.org/abs/2401.16342v1"}
{"created":"2024-01-29 17:47:07","title":"S-HIDRA: A blockchain and SDN domain-based architecture to orchestrate fog computing environments","abstract":"Fog computing arises as a complement to cloud computing where computing and storage are provided in a decentralized way rather than the centralized approach of the cloud paradigm. In addition, blockchain provides a decentralized and immutable ledger which can provide support for running arbitrary logic thanks to smart contracts. These facts can lead to harness smart contracts on blockchain as the basis for a decentralized, autonomous, and resilient orchestrator for the resources in the fog. However, the potentially vast amount of geographically distributed fog nodes may threaten the feasibility of the orchestration. On the other hand, fog nodes can exhibit highly dynamic workloads which may result in the orchestrator redistributing the services among them. Thus, there is also a need to dynamically support the network connections to those services independently of their location. Software Defined Networking (SDN) can be integrated within the orchestrator to carry out a seamless service management. To tackle both aforementioned issues, the S-HIDRA architecture is proposed. It integrates SDN support within a blockchain-based orchestrator of container-based services for fog environments, in order to provide low network latency and high service availability. Also, a domain-based architecture is outlined \\marev{as potential scenario} to address the geographic distributed nature of fog environments. Results obtained from a proof-of-concept implementation assess the required functionality for S-HIDRA.","sentences":["Fog computing arises as a complement to cloud computing where computing and storage are provided in a decentralized way rather than the centralized approach of the cloud paradigm.","In addition, blockchain provides a decentralized and immutable ledger which can provide support for running arbitrary logic thanks to smart contracts.","These facts can lead to harness smart contracts on blockchain as the basis for a decentralized, autonomous, and resilient orchestrator for the resources in the fog.","However, the potentially vast amount of geographically distributed fog nodes may threaten the feasibility of the orchestration.","On the other hand, fog nodes can exhibit highly dynamic workloads which may result in the orchestrator redistributing the services among them.","Thus, there is also a need to dynamically support the network connections to those services independently of their location.","Software Defined Networking (SDN) can be integrated within the orchestrator to carry out a seamless service management.","To tackle both aforementioned issues, the S-HIDRA architecture is proposed.","It integrates SDN support within a blockchain-based orchestrator of container-based services for fog environments, in order to provide low network latency and high service availability.","Also, a domain-based architecture is outlined \\marev{as potential scenario} to address the geographic distributed nature of fog environments.","Results obtained from a proof-of-concept implementation assess the required functionality for S-HIDRA."],"url":"http://arxiv.org/abs/2401.16341v1"}
{"created":"2024-01-29 17:46:18","title":"The role of library versions in Developer-ChatGPT conversations","abstract":"The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks. Developers use ChatGPT to write code, review code changes, and even debug their programs. In these interactions, ChatGPT often recommends code snippets that depend on external libraries. However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.   In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations. We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries. Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations. In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses. Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research.","sentences":["The latest breakthroughs in large language models (LLM) have empowered software development tools, such as ChatGPT, to aid developers in complex tasks.","Developers use ChatGPT to write code, review code changes, and even debug their programs.","In these interactions, ChatGPT often recommends code snippets that depend on external libraries.","However, code from libraries changes over time, invalidating a once-correct code snippet and making it difficult to reuse recommended code.   ","In this study, we analyze DevGPT, a dataset of more than 4,000 Developer-ChatGPT interactions, to understand the role of library versions in code-related conversations.","We quantify how often library version constraints are mentioned in code-related conversations and when ChatGPT recommends the installation of specific libraries.","Our findings show that, albeit to constantly recommend and analyze code with external dependencies, library version constraints only appear in 9% of the conversations.","In the majority of conversations, the version constraints are prompted by users (as opposed to being specified by ChatGPT) as a method for receiving better quality responses.","Moreover, we study how library version constraints are used in the conversation through qualitative methods, identifying several potential problems that warrant further research."],"url":"http://arxiv.org/abs/2401.16340v1"}
{"created":"2024-01-29 17:45:23","title":"SAT-CEP-monitor: An air quality monitoring software architecture combining complex event processing with satellite remote sensing","abstract":"Air pollution is a major problem today that causes serious damage to human health. Urban areas are the most affected by the degradation of air quality caused by anthropogenic gas emissions. Although there are multiple proposals for air quality monitoring, in most cases, two limitations are imposed: the impossibility of processing data in Near Real-Time (NRT) for remote sensing approaches and the impossibility of reaching areas of limited accessibility or low network coverage for ground data approaches. We propose a software architecture that efficiently combines complex event processing with remote sensing data from various satellite sensors to monitor air quality in NRT, giving support to decision-makers. We illustrate the proposed solution by calculating the air quality levels for several areas of Morocco and Spain, extracting and processing satellite information in NRT. This study also validates the air quality measured by ground stations and satellite sensor data.","sentences":["Air pollution is a major problem today that causes serious damage to human health.","Urban areas are the most affected by the degradation of air quality caused by anthropogenic gas emissions.","Although there are multiple proposals for air quality monitoring, in most cases, two limitations are imposed: the impossibility of processing data in Near Real-Time (NRT) for remote sensing approaches and the impossibility of reaching areas of limited accessibility or low network coverage for ground data approaches.","We propose a software architecture that efficiently combines complex event processing with remote sensing data from various satellite sensors to monitor air quality in NRT, giving support to decision-makers.","We illustrate the proposed solution by calculating the air quality levels for several areas of Morocco and Spain, extracting and processing satellite information in NRT.","This study also validates the air quality measured by ground stations and satellite sensor data."],"url":"http://arxiv.org/abs/2401.16339v1"}
{"created":"2024-01-29 17:45:02","title":"Curriculum-Based Reinforcement Learning for Quadrupedal Jumping: A Reference-free Design","abstract":"Deep reinforcement learning (DRL) has emerged as a promising solution to mastering explosive and versatile quadrupedal jumping skills. However, current DRL-based frameworks usually rely on well-defined reference trajectories, which are obtained by capturing animal motions or transferring experience from existing controllers. This work explores the possibility of learning dynamic jumping without imitating a reference trajectory. To this end, we incorporate a curriculum design into DRL so as to accomplish challenging tasks progressively. Starting from a vertical in-place jump, we then generalize the learned policy to forward and diagonal jumps and, finally, learn to jump across obstacles. Conditioned on the desired landing location, orientation, and obstacle dimensions, the proposed approach contributes to a wide range of jumping motions, including omnidirectional jumping and robust jumping, alleviating the effort to extract references in advance. Particularly, without constraints from the reference motion, a 90cm forward jump is achieved, exceeding previous records for similar robots reported in the existing literature. Additionally, continuous jumping on the soft grassy floor is accomplished, even when it is not encountered in the training stage. A supplementary video showing our results can be found at https://youtu.be/nRaMCrwU5X8 .","sentences":["Deep reinforcement learning (DRL) has emerged as a promising solution to mastering explosive and versatile quadrupedal jumping skills.","However, current DRL-based frameworks usually rely on well-defined reference trajectories, which are obtained by capturing animal motions or transferring experience from existing controllers.","This work explores the possibility of learning dynamic jumping without imitating a reference trajectory.","To this end, we incorporate a curriculum design into DRL so as to accomplish challenging tasks progressively.","Starting from a vertical in-place jump, we then generalize the learned policy to forward and diagonal jumps and, finally, learn to jump across obstacles.","Conditioned on the desired landing location, orientation, and obstacle dimensions, the proposed approach contributes to a wide range of jumping motions, including omnidirectional jumping and robust jumping, alleviating the effort to extract references in advance.","Particularly, without constraints from the reference motion, a 90cm forward jump is achieved, exceeding previous records for similar robots reported in the existing literature.","Additionally, continuous jumping on the soft grassy floor is accomplished, even when it is not encountered in the training stage.","A supplementary video showing our results can be found at https://youtu.be/nRaMCrwU5X8 ."],"url":"http://arxiv.org/abs/2401.16337v1"}
{"created":"2024-01-29 17:43:42","title":"Iterative Data Smoothing: Mitigating Reward Overfitting and Overoptimization in RLHF","abstract":"Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values. The initial phase of RLHF involves learning human values using a reward model from ranking data. It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective. This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS). The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels. Our empirical findings highlight the superior performance of this approach over the traditional methods.","sentences":["Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique that aligns language models closely with human-centric values.","The initial phase of RLHF involves learning human values using a reward model from ranking data.","It is observed that the performance of the reward model degrades after one epoch of training, and optimizing too much against the learned reward model eventually hinders the true objective.","This paper delves into these issues, leveraging the theoretical insights to design improved reward learning algorithm termed 'Iterative Data Smoothing' (IDS).","The core idea is that during each training epoch, we not only update the model with the data, but also update the date using the model, replacing hard labels with soft labels.","Our empirical findings highlight the superior performance of this approach over the traditional methods."],"url":"http://arxiv.org/abs/2401.16335v1"}
{"created":"2024-01-29 17:38:14","title":"Tradeoffs Between Alignment and Helpfulness in Language Models","abstract":"Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones. It is often done by tuning the model or inserting preset aligning prompts. Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a). Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks. In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model. We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically. Interestingly, we find that while the helpfulness generally decreases, it does so quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering. We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment.","sentences":["Language model alignment has become an important component of AI safety, allowing safe interactions between humans and language models, by enhancing desired behaviors and inhibiting undesired ones.","It is often done by tuning the model or inserting preset aligning prompts.","Recently, representation engineering, a method which alters the model's behavior via changing its representations post-training, was shown to be effective in aligning LLMs (Zou et al., 2023a).","Representation engineering yields gains in alignment oriented tasks such as resistance to adversarial attacks and reduction of social biases, but was also shown to cause a decrease in the ability of the model to perform basic tasks.","In this paper we study the tradeoff between the increase in alignment and decrease in helpfulness of the model.","We propose a theoretical framework which provides bounds for these two quantities, and demonstrate their relevance empirically.","Interestingly, we find that while the helpfulness generally decreases, it does so quadratically with the norm of the representation engineering vector, while the alignment increases linearly with it, indicating a regime in which it is efficient to use representation engineering.","We validate our findings empirically, and chart the boundaries to the usefulness of representation engineering for alignment."],"url":"http://arxiv.org/abs/2401.16332v1"}
{"created":"2024-01-29 17:35:19","title":"Synthesis of 3D on-air signatures with the Sigma-Lognormal model","abstract":"Signature synthesis is a computation technique that generates artificial specimens which can support decision making in automatic signature verification. A lot of work has been dedicated to this subject, which centres on synthesizing dynamic and static two-dimensional handwriting on canvas. This paper proposes a framework to generate synthetic 3D on-air signatures exploiting the lognormality principle, which mimics the complex neuromotor control processes at play as the fingertip moves. Addressing the usual cases involving the development of artificial individuals and duplicated samples, this paper contributes to the synthesis of: (1) the trajectory and velocity of entirely 3D new signatures; (2) kinematic information when only the 3D trajectory of the signature is known, and (3) duplicate samples of 3D real signatures. Validation was conducted by generating synthetic 3D signature databases mimicking real ones and showing that automatic signature verifications of genuine and skilled forgeries report performances similar to those of real and synthetic databases. We also observed that training 3D automatic signature verifiers with duplicates can reduce errors. We further demonstrated that our proposal is also valid for synthesizing 3D air writing and gestures. Finally, a perception test confirmed the human likeness of the generated specimens. The databases generated are publicly available, only for research purposes, at .","sentences":["Signature synthesis is a computation technique that generates artificial specimens which can support decision making in automatic signature verification.","A lot of work has been dedicated to this subject, which centres on synthesizing dynamic and static two-dimensional handwriting on canvas.","This paper proposes a framework to generate synthetic 3D on-air signatures exploiting the lognormality principle, which mimics the complex neuromotor control processes at play as the fingertip moves.","Addressing the usual cases involving the development of artificial individuals and duplicated samples, this paper contributes to the synthesis of: (1) the trajectory and velocity of entirely 3D new signatures; (2) kinematic information when only the 3D trajectory of the signature is known, and (3) duplicate samples of 3D real signatures.","Validation was conducted by generating synthetic 3D signature databases mimicking real ones and showing that automatic signature verifications of genuine and skilled forgeries report performances similar to those of real and synthetic databases.","We also observed that training 3D automatic signature verifiers with duplicates can reduce errors.","We further demonstrated that our proposal is also valid for synthesizing 3D air writing and gestures.","Finally, a perception test confirmed the human likeness of the generated specimens.","The databases generated are publicly available, only for research purposes, at ."],"url":"http://arxiv.org/abs/2401.16329v1"}
{"created":"2024-01-29 17:32:22","title":"PICL: Physics Informed Contrastive Learning for Partial Differential Equations","abstract":"Neural operators have recently grown in popularity as Partial Differential Equation (PDEs) surrogate models. Learning solution functionals, rather than functions, has proven to be a powerful approach to calculate fast, accurate solutions to complex PDEs. While much work has been done evaluating neural operator performance on a wide variety of surrogate modeling tasks, these works normally evaluate performance on a single equation at a time. In this work, we develop a novel contrastive pretraining framework utilizing Generalized Contrastive Loss that improves neural operator generalization across multiple governing equations simultaneously. Governing equation coefficients are used to measure ground-truth similarity between systems. A combination of physics-informed system evolution and latent-space model output are anchored to input data and used in our distance function. We find that physics-informed contrastive pretraining improves both accuracy and generalization for the Fourier Neural Operator in fixed-future task, with comparable performance on the autoregressive rollout, and superresolution tasks for the 1D Heat, Burgers', and linear advection equations.","sentences":["Neural operators have recently grown in popularity as Partial Differential Equation (PDEs) surrogate models.","Learning solution functionals, rather than functions, has proven to be a powerful approach to calculate fast, accurate solutions to complex PDEs.","While much work has been done evaluating neural operator performance on a wide variety of surrogate modeling tasks, these works normally evaluate performance on a single equation at a time.","In this work, we develop a novel contrastive pretraining framework utilizing Generalized Contrastive Loss that improves neural operator generalization across multiple governing equations simultaneously.","Governing equation coefficients are used to measure ground-truth similarity between systems.","A combination of physics-informed system evolution and latent-space model output are anchored to input data and used in our distance function.","We find that physics-informed contrastive pretraining improves both accuracy and generalization for the Fourier Neural Operator in fixed-future task, with comparable performance on the autoregressive rollout, and superresolution tasks for the 1D Heat, Burgers', and linear advection equations."],"url":"http://arxiv.org/abs/2401.16327v1"}
{"created":"2024-01-29 17:21:41","title":"Defining and Extracting generalizable interaction primitives from DNNs","abstract":"Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI. To this end, Ren et al. (2023c) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables. However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN. Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs. Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs.","sentences":["Faithfully summarizing the knowledge encoded by a deep neural network (DNN) into a few symbolic primitive patterns without losing much information represents a core challenge in explainable AI.","To this end, Ren et al.","(2023c) have derived a series of theorems to prove that the inference score of a DNN can be explained as a small set of interactions between input variables.","However, the lack of generalization power makes it still hard to consider such interactions as faithful primitive patterns encoded by the DNN.","Therefore, given different DNNs trained for the same task, we develop a new method to extract interactions that are shared by these DNNs.","Experiments show that the extracted interactions can better reflect common knowledge shared by different DNNs."],"url":"http://arxiv.org/abs/2401.16318v1"}
{"created":"2024-01-29 17:18:47","title":"Creative Telescoping for Hypergeometric Double Sums","abstract":"We present efficient methods for calculating linear recurrences of hypergeometric double sums and, more generally, of multiple sums. In particular, we supplement this approach with the algorithmic theory of contiguous relations, which guarantees the applicability of our method for many input sums. In addition, we elaborate new techniques to optimize the underlying key task of our method to compute rational solutions of parameterized linear recurrences.","sentences":["We present efficient methods for calculating linear recurrences of hypergeometric double sums and, more generally, of multiple sums.","In particular, we supplement this approach with the algorithmic theory of contiguous relations, which guarantees the applicability of our method for many input sums.","In addition, we elaborate new techniques to optimize the underlying key task of our method to compute rational solutions of parameterized linear recurrences."],"url":"http://arxiv.org/abs/2401.16314v1"}
{"created":"2024-01-29 17:17:42","title":"Machine Translation Meta Evaluation through Translation Accuracy Challenge Sets","abstract":"Recent machine translation (MT) metrics calibrate their effectiveness by correlating with human judgement but without any insights about their behaviour across different error types. Challenge sets are used to probe specific dimensions of metric behaviour but there are very few such datasets and they either focus on a limited number of phenomena or a limited number of language pairs. We introduce ACES, a contrastive challenge set spanning 146 language pairs, aimed at discovering whether metrics can identify 68 translation accuracy errors. These phenomena range from simple alterations at the word/character level to more complex errors based on discourse and real-world knowledge. We conduct a large-scale study by benchmarking ACES on 50 metrics submitted to the WMT 2022 and 2023 metrics shared tasks. We benchmark metric performance, assess their incremental performance over successive campaigns, and measure their sensitivity to a range of linguistic phenomena. We also investigate claims that Large Language Models (LLMs) are effective as MT evaluators by evaluating on ACES. Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods fail to demonstrate reliable performance. Our analyses indicate that most metrics ignore the source sentence, tend to prefer surface-level overlap and end up incorporating properties of base models which are not always beneficial. We expand ACES to include error span annotations, denoted as SPAN-ACES and we use this dataset to evaluate span-based error metrics showing these metrics also need considerable improvement. Finally, we provide a set of recommendations for building better MT metrics, including focusing on error labels instead of scores, ensembling, designing strategies to explicitly focus on the source sentence, focusing on semantic content and choosing the right base model for representations.","sentences":["Recent machine translation (MT) metrics calibrate their effectiveness by correlating with human judgement but without any insights about their behaviour across different error types.","Challenge sets are used to probe specific dimensions of metric behaviour but there are very few such datasets and they either focus on a limited number of phenomena or a limited number of language pairs.","We introduce ACES, a contrastive challenge set spanning 146 language pairs, aimed at discovering whether metrics can identify 68 translation accuracy errors.","These phenomena range from simple alterations at the word/character level to more complex errors based on discourse and real-world knowledge.","We conduct a large-scale study by benchmarking ACES on 50 metrics submitted to the WMT 2022 and 2023 metrics shared tasks.","We benchmark metric performance, assess their incremental performance over successive campaigns, and measure their sensitivity to a range of linguistic phenomena.","We also investigate claims that Large Language Models (LLMs) are effective as MT evaluators by evaluating on ACES.","Our results demonstrate that different metric families struggle with different phenomena and that LLM-based methods fail to demonstrate reliable performance.","Our analyses indicate that most metrics ignore the source sentence, tend to prefer surface-level overlap and end up incorporating properties of base models which are not always beneficial.","We expand ACES to include error span annotations, denoted as SPAN-ACES and we use this dataset to evaluate span-based error metrics showing these metrics also need considerable improvement.","Finally, we provide a set of recommendations for building better MT metrics, including focusing on error labels instead of scores, ensembling, designing strategies to explicitly focus on the source sentence, focusing on semantic content and choosing the right base model for representations."],"url":"http://arxiv.org/abs/2401.16313v1"}
{"created":"2024-01-29 17:17:34","title":"Degradability of Modified Landau-Streater Type Low-Noise Quantum Channels in High Dimensions","abstract":"This paper delves into the degradability of quantum channels, with a specific focus on high-dimensional extensions of qubit depolarizing channels in low-noise regimes. We build upon the foundation of $\\eta$-approximate degradable channels, as established by Sutter et al. and Leditzky et al., to introduce and examine the Modified Landau-Streater (MLS) channels. These channels expand upon the qubit depolarizing and the recently proposed modified Werner-Holevo channels by Roofeh and Karimipour, extending them to higher-dimensional Hilbert spaces (with dimension $d=2j+1$, where $j$ are positive half-integers). Our investigation centers on their conformity to the $O(\\varepsilon^2)$ degradability pattern, aligning with and extending Leditzky et al.'s findings in the $d=2$ case. By replacing the SU($2$) generators with SU($d$) in our treatment, we may explore the potential inclusion of generalized Gell-Mann matrices in future research. Our results enhance the understanding of super-additivity in quantum channels within the low-noise regime and lay the groundwork for future explorations into conditions and structures that could lead to $O(\\varepsilon^2)$ degradability across a broader spectrum of quantum channels.","sentences":["This paper delves into the degradability of quantum channels, with a specific focus on high-dimensional extensions of qubit depolarizing channels in low-noise regimes.","We build upon the foundation of $\\eta$-approximate degradable channels, as established by Sutter et al. and Leditzky et al., to introduce and examine the Modified Landau-Streater (MLS) channels.","These channels expand upon the qubit depolarizing and the recently proposed modified Werner-Holevo channels by Roofeh and Karimipour, extending them to higher-dimensional Hilbert spaces (with dimension $d=2j+1$, where $j$ are positive half-integers).","Our investigation centers on their conformity to the $O(\\varepsilon^2)$ degradability pattern, aligning with and extending Leditzky et al.'s findings in the $d=2$ case.","By replacing the SU($2$) generators with SU($d$) in our treatment, we may explore the potential inclusion of generalized Gell-Mann matrices in future research.","Our results enhance the understanding of super-additivity in quantum channels within the low-noise regime and lay the groundwork for future explorations into conditions and structures that could lead to $O(\\varepsilon^2)$ degradability across a broader spectrum of quantum channels."],"url":"http://arxiv.org/abs/2401.16312v1"}
{"created":"2024-01-29 17:13:44","title":"Security Code Review by LLMs: A Deep Dive into Responses","abstract":"Security code review aims to combine automated tools and manual efforts to detect security defects during development. The rapid development of Large Language Models (LLMs) has shown promising potential in software development, as well as opening up new possibilities in automated security code review. To explore the challenges of applying LLMs in practical code review for security defect detection, this study compared the detection performance of three state-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on 549 code files that contain security defects from real-world code reviews. Through analyzing 82 responses generated by the best-performing LLM-prompt combination based on 100 randomly selected code files, we extracted and categorized quality problems present in these responses into 5 themes and 16 categories. Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection. This work reveals the deficiencies of LLM-generated responses in security code review and paves the way for future optimization of LLMs towards this task.","sentences":["Security code review aims to combine automated tools and manual efforts to detect security defects during development.","The rapid development of Large Language Models (LLMs) has shown promising potential in software development, as well as opening up new possibilities in automated security code review.","To explore the challenges of applying LLMs in practical code review for security defect detection, this study compared the detection performance of three state-of-the-art LLMs (Gemini Pro, GPT-4, and GPT-3.5) under five prompts on 549 code files that contain security defects from real-world code reviews.","Through analyzing 82 responses generated by the best-performing LLM-prompt combination based on 100 randomly selected code files, we extracted and categorized quality problems present in these responses into 5 themes and 16 categories.","Our results indicate that the responses produced by LLMs often suffer from verbosity, vagueness, and incompleteness, highlighting the necessity to enhance their conciseness, understandability, and compliance to security defect detection.","This work reveals the deficiencies of LLM-generated responses in security code review and paves the way for future optimization of LLMs towards this task."],"url":"http://arxiv.org/abs/2401.16310v1"}
{"created":"2024-01-29 17:08:57","title":"Momentary Stressor Logging and Reflective Visualizations: Implications for Stress Management with Wearables","abstract":"Commercial wearables from Fitbit, Garmin, and Whoop have recently introduced real-time notifications based on detecting changes in physiological responses indicating potential stress. In this paper, we investigate how these new capabilities can be leveraged to improve stress management. We developed a smartwatch app, a smartphone app, and a cloud service, and conducted a 100-day field study with 122 participants who received prompts triggered by physiological responses several times a day. They were asked whether they were stressed, and if so, to log the most likely stressor. Each week, participants received new visualizations of their data to self-reflect on patterns and trends. Participants reported better awareness of their stressors, and self-initiating fourteen kinds of behavioral changes to reduce stress in their daily lives. Repeated self-reports over 14 weeks showed reductions in both stress intensity (in 26,521 momentary ratings) and stress frequency (in 1,057 weekly surveys).","sentences":["Commercial wearables from Fitbit, Garmin, and Whoop have recently introduced real-time notifications based on detecting changes in physiological responses indicating potential stress.","In this paper, we investigate how these new capabilities can be leveraged to improve stress management.","We developed a smartwatch app, a smartphone app, and a cloud service, and conducted a 100-day field study with 122 participants who received prompts triggered by physiological responses several times a day.","They were asked whether they were stressed, and if so, to log the most likely stressor.","Each week, participants received new visualizations of their data to self-reflect on patterns and trends.","Participants reported better awareness of their stressors, and self-initiating fourteen kinds of behavioral changes to reduce stress in their daily lives.","Repeated self-reports over 14 weeks showed reductions in both stress intensity (in 26,521 momentary ratings) and stress frequency (in 1,057 weekly surveys)."],"url":"http://arxiv.org/abs/2401.16307v1"}
{"created":"2024-01-29 17:05:19","title":"MixSup: Mixed-grained Supervision for Label-efficient LiDAR-based 3D Object Detection","abstract":"Label-efficient LiDAR-based 3D object detection is currently dominated by weakly/semi-supervised methods. Instead of exclusively following one of them, we propose MixSup, a more practical paradigm simultaneously utilizing massive cheap coarse labels and a limited number of accurate labels for Mixed-grained Supervision. We start by observing that point clouds are usually textureless, making it hard to learn semantics. However, point clouds are geometrically rich and scale-invariant to the distances from sensors, making it relatively easy to learn the geometry of objects, such as poses and shapes. Thus, MixSup leverages massive coarse cluster-level labels to learn semantics and a few expensive box-level labels to learn accurate poses and shapes. We redesign the label assignment in mainstream detectors, which allows them seamlessly integrated into MixSup, enabling practicality and universality. We validate its effectiveness in nuScenes, Waymo Open Dataset, and KITTI, employing various detectors. MixSup achieves up to 97.31% of fully supervised performance, using cheap cluster annotations and only 10% box annotations. Furthermore, we propose PointSAM based on the Segment Anything Model for automated coarse labeling, further reducing the annotation burden. The code is available at https://github.com/BraveGroup/PointSAM-for-MixSup.","sentences":["Label-efficient LiDAR-based 3D object detection is currently dominated by weakly/semi-supervised methods.","Instead of exclusively following one of them, we propose MixSup, a more practical paradigm simultaneously utilizing massive cheap coarse labels and a limited number of accurate labels for Mixed-grained Supervision.","We start by observing that point clouds are usually textureless, making it hard to learn semantics.","However, point clouds are geometrically rich and scale-invariant to the distances from sensors, making it relatively easy to learn the geometry of objects, such as poses and shapes.","Thus, MixSup leverages massive coarse cluster-level labels to learn semantics and a few expensive box-level labels to learn accurate poses and shapes.","We redesign the label assignment in mainstream detectors, which allows them seamlessly integrated into MixSup, enabling practicality and universality.","We validate its effectiveness in nuScenes, Waymo Open Dataset, and KITTI, employing various detectors.","MixSup achieves up to 97.31% of fully supervised performance, using cheap cluster annotations and only 10% box annotations.","Furthermore, we propose PointSAM based on the Segment Anything Model for automated coarse labeling, further reducing the annotation burden.","The code is available at https://github.com/BraveGroup/PointSAM-for-MixSup."],"url":"http://arxiv.org/abs/2401.16305v1"}
{"created":"2024-01-29 17:04:32","title":"Regressing Transformers for Data-efficient Visual Place Recognition","abstract":"Visual place recognition is a critical task in computer vision, especially for localization and navigation systems. Existing methods often rely on contrastive learning: image descriptors are trained to have small distance for similar images and larger distance for dissimilar ones in a latent space. However, this approach struggles to ensure accurate distance-based image similarity representation, particularly when training with binary pairwise labels, and complex re-ranking strategies are required. This work introduces a fresh perspective by framing place recognition as a regression problem, using camera field-of-view overlap as similarity ground truth for learning. By optimizing image descriptors to align directly with graded similarity labels, this approach enhances ranking capabilities without expensive re-ranking, offering data-efficient training and strong generalization across several benchmark datasets.","sentences":["Visual place recognition is a critical task in computer vision, especially for localization and navigation systems.","Existing methods often rely on contrastive learning: image descriptors are trained to have small distance for similar images and larger distance for dissimilar ones in a latent space.","However, this approach struggles to ensure accurate distance-based image similarity representation, particularly when training with binary pairwise labels, and complex re-ranking strategies are required.","This work introduces a fresh perspective by framing place recognition as a regression problem, using camera field-of-view overlap as similarity ground truth for learning.","By optimizing image descriptors to align directly with graded similarity labels, this approach enhances ranking capabilities without expensive re-ranking, offering data-efficient training and strong generalization across several benchmark datasets."],"url":"http://arxiv.org/abs/2401.16304v1"}
{"created":"2024-01-29 17:03:28","title":"Quantum-safe Encryption: A New Method to Reduce Complexity and/or Improve Security Level","abstract":"This work presents some novel techniques to enhance an encryption scheme motivated by classical McEliece cryptosystem. Contributions include: (1) using masking matrices to hide sensitive data, (2) allowing both legitimate parties to incorporate randomness in the public key without sharing any additional public information, (3) using concatenation of a repetition code for error correction, permitting key recovery with a negligible decoding complexity, (4) making attacks more difficult by increasing the complexity in verifying a given key candidate has resulted in the actual key, (5) introducing memory in the error sequence such that: (i) error vector is composed of a random number of erroneous bits, (ii) errors can be all corrected when used in conjunction with concatenation of a repetition code of length 3. Proposed techniques allow generating significantly larger keys, at the same time, with a much lower complexity, as compared to known post-quantum key generation techniques relying on randomization.","sentences":["This work presents some novel techniques to enhance an encryption scheme motivated by classical McEliece cryptosystem.","Contributions include: (1) using masking matrices to hide sensitive data, (2) allowing both legitimate parties to incorporate randomness in the public key without sharing any additional public information, (3) using concatenation of a repetition code for error correction, permitting key recovery with a negligible decoding complexity, (4) making attacks more difficult by increasing the complexity in verifying a given key candidate has resulted in the actual key, (5) introducing memory in the error sequence such that: (i) error vector is composed of a random number of erroneous bits, (ii) errors can be all corrected when used in conjunction with concatenation of a repetition code of length 3.","Proposed techniques allow generating significantly larger keys, at the same time, with a much lower complexity, as compared to known post-quantum key generation techniques relying on randomization."],"url":"http://arxiv.org/abs/2401.16302v1"}
{"created":"2024-01-29 17:02:14","title":"Scalable Factor Graph-Based Heterogeneous Bayesian DDF for Dynamic Systems","abstract":"Heterogeneous Bayesian decentralized data fusion captures the set of problems in which two robots must combine two probability density functions over non-equal, but overlapping sets of random variables. In the context of multi-robot dynamic systems, this enables robots to take a \"divide and conquer\" approach to reason and share data over complementary tasks instead of over the full joint state space. For example, in a target tracking application, this allows robots to track different subsets of targets and share data on only common targets. This paper presents a framework by which robots can each use a local factor graph to represent relevant partitions of a complex global joint probability distribution, thus allowing them to avoid reasoning over the entirety of a more complex model and saving communication as well as computation costs. From a theoretical point of view, this paper makes contributions by casting the heterogeneous decentralized fusion problem in terms of a factor graph, analyzing the challenges that arise due to dynamic filtering, and then developing a new conservative filtering algorithm that ensures statistical correctness. From a practical point of view, we show how this framework can be used to represent different multi-robot applications and then test it with simulations and hardware experiments to validate and demonstrate its statistical conservativeness, applicability, and robustness to real-world challenges.","sentences":["Heterogeneous Bayesian decentralized data fusion captures the set of problems in which two robots must combine two probability density functions over non-equal, but overlapping sets of random variables.","In the context of multi-robot dynamic systems, this enables robots to take a \"divide and conquer\" approach to reason and share data over complementary tasks instead of over the full joint state space.","For example, in a target tracking application, this allows robots to track different subsets of targets and share data on only common targets.","This paper presents a framework by which robots can each use a local factor graph to represent relevant partitions of a complex global joint probability distribution, thus allowing them to avoid reasoning over the entirety of a more complex model and saving communication as well as computation costs.","From a theoretical point of view, this paper makes contributions by casting the heterogeneous decentralized fusion problem in terms of a factor graph, analyzing the challenges that arise due to dynamic filtering, and then developing a new conservative filtering algorithm that ensures statistical correctness.","From a practical point of view, we show how this framework can be used to represent different multi-robot applications and then test it with simulations and hardware experiments to validate and demonstrate its statistical conservativeness, applicability, and robustness to real-world challenges."],"url":"http://arxiv.org/abs/2401.16301v1"}
{"created":"2024-01-29 17:00:28","title":"Enhancing Molecular Property Prediction with Auxiliary Learning and Task-Specific Adaptation","abstract":"Pretrained Graph Neural Networks have been widely adopted for various molecular property prediction tasks. Despite their ability to encode structural and relational features of molecules, traditional fine-tuning of such pretrained GNNs on the target task can lead to poor generalization. To address this, we explore the adaptation of pretrained GNNs to the target task by jointly training them with multiple auxiliary tasks. This could enable the GNNs to learn both general and task-specific features, which may benefit the target task. However, a major challenge is to determine the relatedness of auxiliary tasks with the target task. To address this, we investigate multiple strategies to measure the relevance of auxiliary tasks and integrate such tasks by adaptively combining task gradients or by learning task weights via bi-level optimization. Additionally, we propose a novel gradient surgery-based approach, Rotation of Conflicting Gradients ($\\mathtt{RCGrad}$), that learns to align conflicting auxiliary task gradients through rotation. Our experiments with state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed methods, with improvements of up to 7.7% over fine-tuning. This suggests that incorporating auxiliary tasks along with target task fine-tuning can be an effective way to improve the generalizability of pretrained GNNs for molecular property prediction.","sentences":["Pretrained Graph Neural Networks have been widely adopted for various molecular property prediction tasks.","Despite their ability to encode structural and relational features of molecules, traditional fine-tuning of such pretrained GNNs on the target task can lead to poor generalization.","To address this, we explore the adaptation of pretrained GNNs to the target task by jointly training them with multiple auxiliary tasks.","This could enable the GNNs to learn both general and task-specific features, which may benefit the target task.","However, a major challenge is to determine the relatedness of auxiliary tasks with the target task.","To address this, we investigate multiple strategies to measure the relevance of auxiliary tasks and integrate such tasks by adaptively combining task gradients or by learning task weights via bi-level optimization.","Additionally, we propose a novel gradient surgery-based approach, Rotation of Conflicting Gradients ($\\mathtt{RCGrad}$), that learns to align conflicting auxiliary task gradients through rotation.","Our experiments with state-of-the-art pretrained GNNs demonstrate the efficacy of our proposed methods, with improvements of up to 7.7% over fine-tuning.","This suggests that incorporating auxiliary tasks along with target task fine-tuning can be an effective way to improve the generalizability of pretrained GNNs for molecular property prediction."],"url":"http://arxiv.org/abs/2401.16299v1"}
{"created":"2024-01-29 16:59:39","title":"Breaking the Barrier: Selective Uncertainty-based Active Learning for Medical Image Segmentation","abstract":"Active learning (AL) has found wide applications in medical image segmentation, aiming to alleviate the annotation workload and enhance performance. Conventional uncertainty-based AL methods, such as entropy and Bayesian, often rely on an aggregate of all pixel-level metrics. However, in imbalanced settings, these methods tend to neglect the significance of target regions, eg., lesions, and tumors. Moreover, uncertainty-based selection introduces redundancy. These factors lead to unsatisfactory performance, and in many cases, even underperform random sampling. To solve this problem, we introduce a novel approach called the Selective Uncertainty-based AL, avoiding the conventional practice of summing up the metrics of all pixels. Through a filtering process, our strategy prioritizes pixels within target areas and those near decision boundaries. This resolves the aforementioned disregard for target areas and redundancy. Our method showed substantial improvements across five different uncertainty-based methods and two distinct datasets, utilizing fewer labeled data to reach the supervised baseline and consistently achieving the highest overall performance. Our code is available at https://github.com/HelenMa9998/Selective\\_Uncertainty\\_AL.","sentences":["Active learning (AL) has found wide applications in medical image segmentation, aiming to alleviate the annotation workload and enhance performance.","Conventional uncertainty-based AL methods, such as entropy and Bayesian, often rely on an aggregate of all pixel-level metrics.","However, in imbalanced settings, these methods tend to neglect the significance of target regions, eg., lesions, and tumors.","Moreover, uncertainty-based selection introduces redundancy.","These factors lead to unsatisfactory performance, and in many cases, even underperform random sampling.","To solve this problem, we introduce a novel approach called the Selective Uncertainty-based AL, avoiding the conventional practice of summing up the metrics of all pixels.","Through a filtering process, our strategy prioritizes pixels within target areas and those near decision boundaries.","This resolves the aforementioned disregard for target areas and redundancy.","Our method showed substantial improvements across five different uncertainty-based methods and two distinct datasets, utilizing fewer labeled data to reach the supervised baseline and consistently achieving the highest overall performance.","Our code is available at https://github.com/HelenMa9998/Selective\\_Uncertainty\\_AL."],"url":"http://arxiv.org/abs/2401.16298v1"}
{"created":"2024-01-29 16:56:21","title":"On the Complexity of Establishing Hereditary Graph Properties via Vertex Splitting","abstract":"Vertex splitting is a graph operation that replaces a vertex $v$ with two nonadjacent new vertices and makes each neighbor of $v$ adjacent with one or both of the introduced vertices. Vertex splitting has been used in contexts from circuit design to statistical analysis. In this work, we explore the computational complexity of achieving a given graph property $\\Pi$ by a limited number of vertex splits, formalized as the problem $\\Pi$ Vertex Splitting ($\\Pi$-VS). We focus on hereditary graph properties and contribute four groups of results: First, we classify the classical complexity of $\\Pi$-VS for graph properties characterized by forbidden subgraphs of size at most 3. Second, we provide a framework that allows to show NP-completeness whenever one can construct a combination of a forbidden subgraph and prescribed vertex splits that satisfy certain conditions. Leveraging this framework we show NP-completeness when $\\Pi$ is characterized by forbidden subgraphs that are sufficiently well connected. In particular, we show that $F$-Free-VS is NP-complete for each biconnected graph $F$. Third, we study infinite families of forbidden subgraphs, obtaining NP-hardness for Bipartite-VS and Perfect-VS. Finally, we touch upon the parameterized complexity of $\\Pi$-VS with respect to the number of allowed splits, showing para-NP-hardness for $K_3$-Free-VS and deriving an XP-algorithm when each vertex is only allowed to be split at most once.","sentences":["Vertex splitting is a graph operation that replaces a vertex $v$ with two nonadjacent new vertices and makes each neighbor of $v$ adjacent with one or both of the introduced vertices.","Vertex splitting has been used in contexts from circuit design to statistical analysis.","In this work, we explore the computational complexity of achieving a given graph property $\\Pi$ by a limited number of vertex splits, formalized as the problem $\\Pi$ Vertex Splitting ($\\Pi$-VS).","We focus on hereditary graph properties and contribute four groups of results:","First, we classify the classical complexity of $\\Pi$-VS for graph properties characterized by forbidden subgraphs of size at most 3.","Second, we provide a framework that allows to show NP-completeness whenever one can construct a combination of a forbidden subgraph and prescribed vertex splits that satisfy certain conditions.","Leveraging this framework we show NP-completeness when $\\Pi$ is characterized by forbidden subgraphs that are sufficiently well connected.","In particular, we show that $F$-Free-VS is NP-complete for each biconnected graph $F$. Third, we study infinite families of forbidden subgraphs, obtaining NP-hardness for Bipartite-VS and Perfect-VS.","Finally, we touch upon the parameterized complexity of $\\Pi$-VS with respect to the number of allowed splits, showing para-NP-hardness for $K_3$-Free-VS and deriving an XP-algorithm when each vertex is only allowed to be split at most once."],"url":"http://arxiv.org/abs/2401.16296v1"}
{"created":"2024-01-29 16:53:04","title":"Dual feature-based and example-based explanation methods","abstract":"A new approach to the local and global explanation is proposed. It is based on selecting a convex hull constructed for the finite number of points around an explained instance. The convex hull allows us to consider a dual representation of instances in the form of convex combinations of extreme points of a produced polytope. Instead of perturbing new instances in the Euclidean feature space, vectors of convex combination coefficients are uniformly generated from the unit simplex, and they form a new dual dataset. A dual linear surrogate model is trained on the dual dataset. The explanation feature importance values are computed by means of simple matrix calculations. The approach can be regarded as a modification of the well-known model LIME. The dual representation inherently allows us to get the example-based explanation. The neural additive model is also considered as a tool for implementing the example-based explanation approach. Many numerical experiments with real datasets are performed for studying the approach. The code of proposed algorithms is available.","sentences":["A new approach to the local and global explanation is proposed.","It is based on selecting a convex hull constructed for the finite number of points around an explained instance.","The convex hull allows us to consider a dual representation of instances in the form of convex combinations of extreme points of a produced polytope.","Instead of perturbing new instances in the Euclidean feature space, vectors of convex combination coefficients are uniformly generated from the unit simplex, and they form a new dual dataset.","A dual linear surrogate model is trained on the dual dataset.","The explanation feature importance values are computed by means of simple matrix calculations.","The approach can be regarded as a modification of the well-known model LIME.","The dual representation inherently allows us to get the example-based explanation.","The neural additive model is also considered as a tool for implementing the example-based explanation approach.","Many numerical experiments with real datasets are performed for studying the approach.","The code of proposed algorithms is available."],"url":"http://arxiv.org/abs/2401.16294v1"}
{"created":"2024-01-29 16:50:56","title":"Textual Entailment for Effective Triple Validation in Object Prediction","abstract":"Knowledge base population seeks to expand knowledge graphs with facts that are typically extracted from a text corpus. Recently, language models pretrained on large corpora have been shown to contain factual knowledge that can be retrieved using cloze-style strategies. Such approach enables zero-shot recall of facts, showing competitive results in object prediction compared to supervised baselines. However, prompt-based fact retrieval can be brittle and heavily depend on the prompts and context used, which may produce results that are unintended or hallucinatory.We propose to use textual entailment to validate facts extracted from language models through cloze statements. Our results show that triple validation based on textual entailment improves language model predictions in different training regimes. Furthermore, we show that entailment-based triple validation is also effective to validate candidate facts extracted from other sources including existing knowledge graphs and text passages where named entities are recognized.","sentences":["Knowledge base population seeks to expand knowledge graphs with facts that are typically extracted from a text corpus.","Recently, language models pretrained on large corpora have been shown to contain factual knowledge that can be retrieved using cloze-style strategies.","Such approach enables zero-shot recall of facts, showing competitive results in object prediction compared to supervised baselines.","However, prompt-based fact retrieval can be brittle and heavily depend on the prompts and context used, which may produce results that are unintended or hallucinatory.","We propose to use textual entailment to validate facts extracted from language models through cloze statements.","Our results show that triple validation based on textual entailment improves language model predictions in different training regimes.","Furthermore, we show that entailment-based triple validation is also effective to validate candidate facts extracted from other sources including existing knowledge graphs and text passages where named entities are recognized."],"url":"http://arxiv.org/abs/2401.16293v1"}
{"created":"2024-01-29 16:50:43","title":"Pilotfish: Distributed Transaction Execution for Lazy Blockchains","abstract":"Pilotfish is the first scale-out blockchain execution engine able to harness any degree of parallelizability existing in its workload. Pilotfish allows each validator to employ multiple machines, named ExecutionWorkers, under its control to scale its execution layer. Given a sufficiently parallelizable and compute-intensive load, the number of transactions that the validator can execute increases linearly with the number of ExecutionWorkers at its disposal.   In addition, Pilotfish maintains the consistency of the state, even when many validators experience simultaneous machine failures. This is possible due to the meticulous co-design of our crash-recovery protocol which leverages the existing fault tolerance in the blockchain's consensus mechanism.   Finally, Pilotfish can also be seen as the first distributed deterministic execution engine that provides support for dynamic reads as transactions are not required to provide a fully accurate read and write set. This loosening of requirements would normally reduce the parallelizability available by blocking write-after-write conflicts, but our novel versioned-queues scheduling algorithm circumvents this by exploiting the lazy recovery property of Pilotfish, which only persists consistent state and re-executes any optimistic steps taken before the crash.   In order to prove our claims we implemented the common path of Pilotfish with support for the MoveVM and evaluated it against the parallel execution MoveVM of Sui. Our results show that our simpler scheduling algorithms outperforms Sui even with a single execution worker, but more importantly provides linear scalability up to 4 ExecutionWorkers even for simple asset-transfers and to any number of ExecutionWorkers for more computationally heavy workloads.","sentences":["Pilotfish is the first scale-out blockchain execution engine able to harness any degree of parallelizability existing in its workload.","Pilotfish allows each validator to employ multiple machines, named ExecutionWorkers, under its control to scale its execution layer.","Given a sufficiently parallelizable and compute-intensive load, the number of transactions that the validator can execute increases linearly with the number of ExecutionWorkers at its disposal.   ","In addition, Pilotfish maintains the consistency of the state, even when many validators experience simultaneous machine failures.","This is possible due to the meticulous co-design of our crash-recovery protocol which leverages the existing fault tolerance in the blockchain's consensus mechanism.   ","Finally, Pilotfish can also be seen as the first distributed deterministic execution engine that provides support for dynamic reads as transactions are not required to provide a fully accurate read and write set.","This loosening of requirements would normally reduce the parallelizability available by blocking write-after-write conflicts, but our novel versioned-queues scheduling algorithm circumvents this by exploiting the lazy recovery property of Pilotfish, which only persists consistent state and re-executes any optimistic steps taken before the crash.   ","In order to prove our claims we implemented the common path of Pilotfish with support for the MoveVM and evaluated it against the parallel execution MoveVM of Sui.","Our results show that our simpler scheduling algorithms outperforms Sui even with a single execution worker, but more importantly provides linear scalability up to 4 ExecutionWorkers even for simple asset-transfers and to any number of ExecutionWorkers for more computationally heavy workloads."],"url":"http://arxiv.org/abs/2401.16292v1"}
{"created":"2024-01-29 16:50:32","title":"MachineLearnAthon: An Action-Oriented Machine Learning Didactic Concept","abstract":"Machine Learning (ML) techniques are encountered nowadays across disciplines, from social sciences, through natural sciences to engineering. The broad application of ML and the accelerated pace of its evolution lead to an increasing need for dedicated teaching concepts aimed at making the application of this technology more reliable and responsible. However, teaching ML is a daunting task. Aside from the methodological complexity of ML algorithms, both with respect to theory and implementation, the interdisciplinary and empirical nature of the field need to be taken into consideration. This paper introduces the MachineLearnAthon format, an innovative didactic concept designed to be inclusive for students of different disciplines with heterogeneous levels of mathematics, programming and domain expertise. At the heart of the concept lie ML challenges, which make use of industrial data sets to solve real-world problems. These cover the entire ML pipeline, promoting data literacy and practical skills, from data preparation, through deployment, to evaluation.","sentences":["Machine Learning (ML) techniques are encountered nowadays across disciplines, from social sciences, through natural sciences to engineering.","The broad application of ML and the accelerated pace of its evolution lead to an increasing need for dedicated teaching concepts aimed at making the application of this technology more reliable and responsible.","However, teaching ML is a daunting task.","Aside from the methodological complexity of ML algorithms, both with respect to theory and implementation, the interdisciplinary and empirical nature of the field need to be taken into consideration.","This paper introduces the MachineLearnAthon format, an innovative didactic concept designed to be inclusive for students of different disciplines with heterogeneous levels of mathematics, programming and domain expertise.","At the heart of the concept lie ML challenges, which make use of industrial data sets to solve real-world problems.","These cover the entire ML pipeline, promoting data literacy and practical skills, from data preparation, through deployment, to evaluation."],"url":"http://arxiv.org/abs/2401.16291v1"}
{"created":"2024-01-29 16:48:34","title":"GAPS: Geometry-Aware Problem Solver","abstract":"Geometry problem solving presents a formidable challenge within the NLP community. Existing approaches often rely on models designed for solving math word problems, neglecting the unique characteristics of geometry math problems. Additionally, the current research predominantly focuses on geometry calculation problems, while overlooking other essential aspects like proving. In this study, we address these limitations by proposing the Geometry-Aware Problem Solver (GAPS) model. GAPS is specifically designed to generate solution programs for geometry math problems of various types with the help of its unique problem-type classifier. To achieve this, GAPS treats the solution program as a composition of operators and operands, segregating their generation processes. Furthermore, we introduce the geometry elements enhancement method, which enhances the ability of GAPS to recognize geometry elements accurately. By leveraging these improvements, GAPS showcases remarkable performance in resolving geometry math problems. Our experiments conducted on the UniGeo dataset demonstrate the superiority of GAPS over the state-of-the-art model, Geoformer. Specifically, GAPS achieves an accuracy improvement of more than 5.3% for calculation tasks and an impressive 41.1% for proving tasks. Notably, GAPS achieves an impressive accuracy of 97.5% on proving problems, representing a significant advancement in solving geometry proving tasks.","sentences":["Geometry problem solving presents a formidable challenge within the NLP community.","Existing approaches often rely on models designed for solving math word problems, neglecting the unique characteristics of geometry math problems.","Additionally, the current research predominantly focuses on geometry calculation problems, while overlooking other essential aspects like proving.","In this study, we address these limitations by proposing the Geometry-Aware Problem Solver (GAPS) model.","GAPS is specifically designed to generate solution programs for geometry math problems of various types with the help of its unique problem-type classifier.","To achieve this, GAPS treats the solution program as a composition of operators and operands, segregating their generation processes.","Furthermore, we introduce the geometry elements enhancement method, which enhances the ability of GAPS to recognize geometry elements accurately.","By leveraging these improvements, GAPS showcases remarkable performance in resolving geometry math problems.","Our experiments conducted on the UniGeo dataset demonstrate the superiority of GAPS over the state-of-the-art model, Geoformer.","Specifically, GAPS achieves an accuracy improvement of more than 5.3% for calculation tasks and an impressive 41.1% for proving tasks.","Notably, GAPS achieves an impressive accuracy of 97.5% on proving problems, representing a significant advancement in solving geometry proving tasks."],"url":"http://arxiv.org/abs/2401.16287v1"}
{"created":"2024-01-29 16:48:34","title":"Upper bounds on the rate of linear $q$-ary $k$-hash codes","abstract":"This paper presents new upper bounds on the rate of linear $k$-hash codes in $\\mathbb{F}_q^n$, $q\\geq k$, that is, codes with the property that any $k$ distinct codewords are all simultaneously distinct in at least one coordinate.","sentences":["This paper presents new upper bounds on the rate of linear $k$-hash codes in $\\mathbb{F}_q^n$, $q\\geq k$, that is, codes with the property that any $k$ distinct codewords are all simultaneously distinct in at least one coordinate."],"url":"http://arxiv.org/abs/2401.16288v1"}
{"created":"2024-01-29 16:42:34","title":"Capturing Pertinent Symbolic Features for Enhanced Content-Based Misinformation Detection","abstract":"Preventing the spread of misinformation is challenging. The detection of misleading content presents a significant hurdle due to its extreme linguistic and domain variability. Content-based models have managed to identify deceptive language by learning representations from textual data such as social media posts and web articles. However, aggregating representative samples of this heterogeneous phenomenon and implementing effective real-world applications is still elusive. Based on analytical work on the language of misinformation, this paper analyzes the linguistic attributes that characterize this phenomenon and how representative of such features some of the most popular misinformation datasets are. We demonstrate that the appropriate use of pertinent symbolic knowledge in combination with neural language models is helpful in detecting misleading content. Our results achieve state-of-the-art performance in misinformation datasets across the board, showing that our approach offers a valid and robust alternative to multi-task transfer learning without requiring any additional training data. Furthermore, our results show evidence that structured knowledge can provide the extra boost required to address a complex and unpredictable real-world problem like misinformation detection, not only in terms of accuracy but also time efficiency and resource utilization.","sentences":["Preventing the spread of misinformation is challenging.","The detection of misleading content presents a significant hurdle due to its extreme linguistic and domain variability.","Content-based models have managed to identify deceptive language by learning representations from textual data such as social media posts and web articles.","However, aggregating representative samples of this heterogeneous phenomenon and implementing effective real-world applications is still elusive.","Based on analytical work on the language of misinformation, this paper analyzes the linguistic attributes that characterize this phenomenon and how representative of such features some of the most popular misinformation datasets are.","We demonstrate that the appropriate use of pertinent symbolic knowledge in combination with neural language models is helpful in detecting misleading content.","Our results achieve state-of-the-art performance in misinformation datasets across the board, showing that our approach offers a valid and robust alternative to multi-task transfer learning without requiring any additional training data.","Furthermore, our results show evidence that structured knowledge can provide the extra boost required to address a complex and unpredictable real-world problem like misinformation detection, not only in terms of accuracy but also time efficiency and resource utilization."],"url":"http://arxiv.org/abs/2401.16285v1"}
{"created":"2024-01-29 16:42:15","title":"Leveraging Positional Encoding for Robust Multi-Reference-Based Object 6D Pose Estimation","abstract":"Accurately estimating the pose of an object is a crucial task in computer vision and robotics. There are two main deep learning approaches for this: geometric representation regression and iterative refinement. However, these methods have some limitations that reduce their effectiveness. In this paper, we analyze these limitations and propose new strategies to overcome them. To tackle the issue of blurry geometric representation, we use positional encoding with high-frequency components for the object's 3D coordinates. To address the local minimum problem in refinement methods, we introduce a normalized image plane-based multi-reference refinement strategy that's independent of intrinsic matrix constraints. Lastly, we utilize adaptive instance normalization and a simple occlusion augmentation method to help our model concentrate on the target object. Our experiments on Linemod, Linemod-Occlusion, and YCB-Video datasets demonstrate that our approach outperforms existing methods. We will soon release the code.","sentences":["Accurately estimating the pose of an object is a crucial task in computer vision and robotics.","There are two main deep learning approaches for this: geometric representation regression and iterative refinement.","However, these methods have some limitations that reduce their effectiveness.","In this paper, we analyze these limitations and propose new strategies to overcome them.","To tackle the issue of blurry geometric representation, we use positional encoding with high-frequency components for the object's 3D coordinates.","To address the local minimum problem in refinement methods, we introduce a normalized image plane-based multi-reference refinement strategy that's independent of intrinsic matrix constraints.","Lastly, we utilize adaptive instance normalization and a simple occlusion augmentation method to help our model concentrate on the target object.","Our experiments on Linemod, Linemod-Occlusion, and YCB-Video datasets demonstrate that our approach outperforms existing methods.","We will soon release the code."],"url":"http://arxiv.org/abs/2401.16284v1"}
{"created":"2024-01-29 16:39:39","title":"MAPLE: Micro Analysis of Pairwise Language Evolution for Few-Shot Claim Verification","abstract":"Claim verification is an essential step in the automated fact-checking pipeline which assesses the veracity of a claim against a piece of evidence. In this work, we explore the potential of few-shot claim verification, where only very limited data is available for supervision. We propose MAPLE (Micro Analysis of Pairwise Language Evolution), a pioneering approach that explores the alignment between a claim and its evidence with a small seq2seq model and a novel semantic measure. Its innovative utilization of micro language evolution path leverages unlabelled pairwise data to facilitate claim verification while imposing low demand on data annotations and computing resources. MAPLE demonstrates significant performance improvements over SOTA baselines SEED, PET and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and SciFact. Data and code are available here: https://github.com/XiaZeng0223/MAPLE","sentences":["Claim verification is an essential step in the automated fact-checking pipeline which assesses the veracity of a claim against a piece of evidence.","In this work, we explore the potential of few-shot claim verification, where only very limited data is available for supervision.","We propose MAPLE (Micro Analysis of Pairwise Language Evolution), a pioneering approach that explores the alignment between a claim and its evidence with a small seq2seq model and a novel semantic measure.","Its innovative utilization of micro language evolution path leverages unlabelled pairwise data to facilitate claim verification while imposing low demand on data annotations and computing resources.","MAPLE demonstrates significant performance improvements over SOTA baselines SEED, PET and LLaMA 2 across three fact-checking datasets: FEVER, Climate FEVER, and SciFact.","Data and code are available here: https://github.com/XiaZeng0223/MAPLE"],"url":"http://arxiv.org/abs/2401.16282v1"}
{"created":"2024-01-29 16:37:00","title":"Cutup and Detect: Human Fall Detection on Cutup Untrimmed Videos Using a Large Foundational Video Understanding Model","abstract":"This work explores the performance of a large video understanding foundation model on the downstream task of human fall detection on untrimmed video and leverages a pretrained vision transformer for multi-class action detection, with classes: \"Fall\", \"Lying\" and \"Other/Activities of daily living (ADL)\". A method for temporal action localization that relies on a simple cutup of untrimmed videos is demonstrated. The methodology includes a preprocessing pipeline that converts datasets with timestamp action annotations into labeled datasets of short action clips. Simple and effective clip-sampling strategies are introduced. The effectiveness of the proposed method has been empirically evaluated on the publicly available High-Quality Fall Simulation Dataset (HQFSD). The experimental results validate the performance of the proposed pipeline. The results are promising for real-time application, and the falls are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD dataset under the given experimental settings. The source code will be made available on GitHub.","sentences":["This work explores the performance of a large video understanding foundation model on the downstream task of human fall detection on untrimmed video and leverages a pretrained vision transformer for multi-class action detection, with classes: \"Fall\", \"Lying\" and \"Other/Activities of daily living (ADL)\".","A method for temporal action localization that relies on a simple cutup of untrimmed videos is demonstrated.","The methodology includes a preprocessing pipeline that converts datasets with timestamp action annotations into labeled datasets of short action clips.","Simple and effective clip-sampling strategies are introduced.","The effectiveness of the proposed method has been empirically evaluated on the publicly available High-Quality Fall Simulation Dataset (HQFSD).","The experimental results validate the performance of the proposed pipeline.","The results are promising for real-time application, and the falls are detected on video level with a state-of-the-art 0.96 F1 score on the HQFSD dataset under the given experimental settings.","The source code will be made available on GitHub."],"url":"http://arxiv.org/abs/2401.16280v1"}
{"created":"2024-01-29 16:34:12","title":"Rethinking the Producer-Consumer Relationship in Modern DRAM-Based Systems","abstract":"Generational improvements to commodity DRAM throughout half a century have long solidified its prevalence as main memory across the computing industry. However, overcoming today's DRAM technology scaling challenges requires new solutions driven by both DRAM producers and consumers. In this paper, we observe that the separation of concerns between producers and consumers specified by industry-wide DRAM standards is becoming a liability to progress in addressing scaling-related concerns.   To understand the problem, we study four key directions for overcoming DRAM scaling challenges using system-memory cooperation: (i) improving memory access latencies; (ii) reducing DRAM refresh overheads; (iii) securely defending against the RowHammer vulnerability; and (iv) addressing worsening memory errors. We find that the single most important barrier to advancement in all four cases is the consumer's lack of insight into DRAM reliability. Based on an analysis of DRAM reliability testing, we recommend revising the separation of concerns to incorporate limited information transparency between producers and consumers. Finally, we propose adopting this revision in a two-step plan, starting with immediate information release through crowdsourcing and publication and culminating in widespread modifications to DRAM standards.","sentences":["Generational improvements to commodity DRAM throughout half a century have long solidified its prevalence as main memory across the computing industry.","However, overcoming today's DRAM technology scaling challenges requires new solutions driven by both DRAM producers and consumers.","In this paper, we observe that the separation of concerns between producers and consumers specified by industry-wide DRAM standards is becoming a liability to progress in addressing scaling-related concerns.   ","To understand the problem, we study four key directions for overcoming DRAM scaling challenges using system-memory cooperation: (i) improving memory access latencies; (ii) reducing DRAM refresh overheads; (iii) securely defending against the RowHammer vulnerability; and (iv) addressing worsening memory errors.","We find that the single most important barrier to advancement in all four cases is the consumer's lack of insight into DRAM reliability.","Based on an analysis of DRAM reliability testing, we recommend revising the separation of concerns to incorporate limited information transparency between producers and consumers.","Finally, we propose adopting this revision in a two-step plan, starting with immediate information release through crowdsourcing and publication and culminating in widespread modifications to DRAM standards."],"url":"http://arxiv.org/abs/2401.16279v1"}
{"created":"2024-01-29 16:32:36","title":"SECOMP: Formally Secure Compilation of Compartmentalized C Programs","abstract":"Undefined behavior in C often causes devastating security vulnerabilities. One practical mitigation is compartmentalization, which allows developers to structure large programs into mutually distrustful compartments with clearly specified privileges and interactions. In this paper we introduce SECOMP, a compiler for compartmentalized C code that comes with machine-checked proofs guaranteeing that the scope of undefined behavior is restricted to the compartments that encounter it and become dynamically compromised. These guarantees are formalized as the preservation of safety properties against adversarial contexts, a secure compilation criterion similar to full abstraction, and this is the first time such a strong criterion is proven for a mainstream programming language. To achieve this we extend the languages of the CompCert verified C compiler with isolated compartments that can only interact via procedure calls and returns, as specified by cross-compartment interfaces. We adapt the passes and optimizations of CompCert as well as their correctness proofs to this compartment-aware setting. We then use compiler correctness as an ingredient in a larger secure compilation proof that involves several proof engineering novelties, needed to scale formally secure compilation up to a C compiler.","sentences":["Undefined behavior in C often causes devastating security vulnerabilities.","One practical mitigation is compartmentalization, which allows developers to structure large programs into mutually distrustful compartments with clearly specified privileges and interactions.","In this paper we introduce SECOMP, a compiler for compartmentalized C code that comes with machine-checked proofs guaranteeing that the scope of undefined behavior is restricted to the compartments that encounter it and become dynamically compromised.","These guarantees are formalized as the preservation of safety properties against adversarial contexts, a secure compilation criterion similar to full abstraction, and this is the first time such a strong criterion is proven for a mainstream programming language.","To achieve this we extend the languages of the CompCert verified C compiler with isolated compartments that can only interact via procedure calls and returns, as specified by cross-compartment interfaces.","We adapt the passes and optimizations of CompCert as well as their correctness proofs to this compartment-aware setting.","We then use compiler correctness as an ingredient in a larger secure compilation proof that involves several proof engineering novelties, needed to scale formally secure compilation up to a C compiler."],"url":"http://arxiv.org/abs/2401.16277v1"}
{"created":"2024-01-29 16:25:38","title":"The HSF Conditions Database Reference Implementation","abstract":"Conditions data is the subset of non-event data that is necessary to process event data. It poses a unique set of challenges, namely a heterogeneous structure and high access rates by distributed computing. The HSF Conditions Databases activity is a forum for cross-experiment discussions inviting as broad a participation as possible. It grew out of the HSF Community White Paper work to study conditions data access, where experts from ATLAS, Belle II, and CMS converged on a common language and proposed a schema that represents best practice. Following discussions with a broader community, including NP as well as HEP experiments, a core set of use cases, functionality and behaviour was defined with the aim to describe a core conditions database API. This paper will describe the reference implementation of both the conditions database service and the client which together encapsulate HSF best practice conditions data handling. Django was chosen for the service implementation, which uses an ORM instead of the direct use of SQL for all but one method. The simple relational database schema to organise conditions data is implemented in PostgreSQL. The task of storing conditions data payloads themselves is outsourced to any POSIX- compliant filesystem, allowing for transparent relocation and redundancy. Cru- cially this design provides a clear separation between retrieving the metadata describing which conditions data are needed for a data processing job, and retrieving the actual payloads from storage. The service deployment using Helm on OKD will be described together with scaling tests and operations experience from the sPHENIX experiment running more than 25k cores at BNL.","sentences":["Conditions data is the subset of non-event data that is necessary to process event data.","It poses a unique set of challenges, namely a heterogeneous structure and high access rates by distributed computing.","The HSF Conditions Databases activity is a forum for cross-experiment discussions inviting as broad a participation as possible.","It grew out of the HSF Community White Paper work to study conditions data access, where experts from ATLAS, Belle II, and CMS converged on a common language and proposed a schema that represents best practice.","Following discussions with a broader community, including NP as well as HEP experiments, a core set of use cases, functionality and behaviour was defined with the aim to describe a core conditions database API.","This paper will describe the reference implementation of both the conditions database service and the client which together encapsulate HSF best practice conditions data handling.","Django was chosen for the service implementation, which uses an ORM instead of the direct use of SQL for all but one method.","The simple relational database schema to organise conditions data is implemented in PostgreSQL.","The task of storing conditions data payloads themselves is outsourced to any POSIX- compliant filesystem, allowing for transparent relocation and redundancy.","Cru- cially this design provides a clear separation between retrieving the metadata describing which conditions data are needed for a data processing job, and retrieving the actual payloads from storage.","The service deployment using Helm on OKD will be described together with scaling tests and operations experience from the sPHENIX experiment running more than 25k cores at BNL."],"url":"http://arxiv.org/abs/2401.16274v1"}
{"created":"2024-01-29 16:18:54","title":"Capturing Knowledge Graphs and Rules with Octagon Embeddings","abstract":"Region based knowledge graph embeddings represent relations as geometric regions. This has the advantage that the rules which are captured by the model are made explicit, making it straightforward to incorporate prior knowledge and to inspect learned models. Unfortunately, existing approaches are severely restricted in their ability to model relational composition, and hence also their ability to model rules, thus failing to deliver on the main promise of region based models. With the aim of addressing these limitations, we investigate regions which are composed of axis-aligned octagons. Such octagons are particularly easy to work with, as intersections and compositions can be straightforwardly computed, while they are still sufficiently expressive to model arbitrary knowledge graphs. Among others, we also show that our octagon embeddings can properly capture a non-trivial class of rule bases. Finally, we show that our model achieves competitive experimental results.","sentences":["Region based knowledge graph embeddings represent relations as geometric regions.","This has the advantage that the rules which are captured by the model are made explicit, making it straightforward to incorporate prior knowledge and to inspect learned models.","Unfortunately, existing approaches are severely restricted in their ability to model relational composition, and hence also their ability to model rules, thus failing to deliver on the main promise of region based models.","With the aim of addressing these limitations, we investigate regions which are composed of axis-aligned octagons.","Such octagons are particularly easy to work with, as intersections and compositions can be straightforwardly computed, while they are still sufficiently expressive to model arbitrary knowledge graphs.","Among others, we also show that our octagon embeddings can properly capture a non-trivial class of rule bases.","Finally, we show that our model achieves competitive experimental results."],"url":"http://arxiv.org/abs/2401.16270v1"}
{"created":"2024-01-29 16:14:29","title":"A.I. In All The Wrong Places","abstract":"This text describes experiences gained across a two-year test period during which two generations of Generative Artificial Intelligence (A.I.) systems were incorpo-rated into an interdisciplinary, university level course on A.I. for art and design practices. The text uses the results from the courses to reflect on new opportuni-ties for generative systems in art and design while considering traps and limits.","sentences":["This text describes experiences gained across a two-year test period during which two generations of Generative Artificial Intelligence (A.I.) systems were incorpo-rated into an interdisciplinary, university level course on A.I. for art and design practices.","The text uses the results from the courses to reflect on new opportuni-ties for generative systems in art and design while considering traps and limits."],"url":"http://arxiv.org/abs/2401.16268v1"}
{"created":"2024-01-29 16:12:31","title":"CO2: Efficient Distributed Training with Full Communication-Computation Overlap","abstract":"The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques. Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities. In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters. We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmunication with COmputation. CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth. We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability. Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training. We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound. Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing. These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs. The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections.","sentences":["The fundamental success of large language models hinges upon the efficacious implementation of large-scale distributed training techniques.","Nevertheless, building a vast, high-performance cluster featuring high-speed communication interconnectivity is prohibitively costly, and accessible only to prominent entities.","In this work, we aim to lower this barrier and democratize large-scale training with limited bandwidth clusters.","We propose a new approach called CO2 that introduces local-updating and asynchronous communication to the distributed data-parallel training, thereby facilitating the full overlap of COmunication with COmputation.","CO2 is able to attain a high scalability even on extensive multi-node clusters constrained by very limited communication bandwidth.","We further propose the staleness gap penalty and outer momentum clipping techniques together with CO2 to bolster its convergence and training stability.","Besides, CO2 exhibits seamless integration with well-established ZeRO-series optimizers which mitigate memory consumption of model states with large model training.","We also provide a mathematical proof of convergence, accompanied by the establishment of a stringent upper bound.","Furthermore, we validate our findings through an extensive set of practical experiments encompassing a wide range of tasks in the fields of computer vision and natural language processing.","These experiments serve to demonstrate the capabilities of CO2 in terms of convergence, generalization, and scalability when deployed across configurations comprising up to 128 A100 GPUs.","The outcomes emphasize the outstanding capacity of CO2 to hugely improve scalability, no matter on clusters with 800Gbps RDMA or 80Gbps TCP/IP inter-node connections."],"url":"http://arxiv.org/abs/2401.16265v1"}
{"created":"2024-01-29 16:12:18","title":"Collaboration Petri Nets: Verification, Equivalence, and Discovery (Extended Version)","abstract":"Process modeling and discovery techniques aim to construct sound and valid process models for different types of processes, i.e., process orchestrations and collaboration processes. Orchestrations represent behavior of cases within one process. Collaboration processes represent behavior of collaborating cases within multiple process orchestrations that interact via collaboration concepts such as organizations, agents, objects, and services. The heterogeneity of collaboration concepts and types such as message exchange and resource sharing has led to different representations and discovery techniques for collaboration process models, but a standard model class is lacking. We propose collaboration Petri nets (cPN) to achieve comparability between techniques, to enable approach and property transfer, and to build a standardized collaboration mining pipeline similar to process mining. For cPN, we require desirable modeling power, decision power, modeling convenience, and relations to existing model classes. We show the representation of collaboration types, structural characterization as workflow nets, automatic verification of soundness, bisimulation equivalence to existing model classes, and application in a general discovery framework. As empirical evidence to discover cPN, we conduct a comparative evaluation between three discovery techniques on a set of existing collaboration event logs.","sentences":["Process modeling and discovery techniques aim to construct sound and valid process models for different types of processes, i.e., process orchestrations and collaboration processes.","Orchestrations represent behavior of cases within one process.","Collaboration processes represent behavior of collaborating cases within multiple process orchestrations that interact via collaboration concepts such as organizations, agents, objects, and services.","The heterogeneity of collaboration concepts and types such as message exchange and resource sharing has led to different representations and discovery techniques for collaboration process models, but a standard model class is lacking.","We propose collaboration Petri nets (cPN) to achieve comparability between techniques, to enable approach and property transfer, and to build a standardized collaboration mining pipeline similar to process mining.","For cPN, we require desirable modeling power, decision power, modeling convenience, and relations to existing model classes.","We show the representation of collaboration types, structural characterization as workflow nets, automatic verification of soundness, bisimulation equivalence to existing model classes, and application in a general discovery framework.","As empirical evidence to discover cPN, we conduct a comparative evaluation between three discovery techniques on a set of existing collaboration event logs."],"url":"http://arxiv.org/abs/2401.16263v1"}
{"created":"2024-01-29 16:08:18","title":"MosquIoT: A System Based on IoT and Machine Learning for the Monitoring of Aedes aegypti (Diptera: Culicidae)","abstract":"Millions of people around the world are infected with mosquito-borne diseases each year. One of the most dangerous species is Aedes aegypti, the main vector of viruses such as dengue, yellow fever, chikungunya, and Zika, among others. Mosquito prevention and eradication campaigns are essential to avoid major public health consequences. In this respect, entomological surveillance is an important tool. At present, this traditional monitoring tool is executed manually and requires digital transformation to help authorities make better decisions, improve their planning efforts, speed up execution, and better manage available resources. Therefore, new technological tools based on proven techniques need to be designed and developed. However, such tools should also be cost-effective, autonomous, reliable, and easy to implement, and should be enabled by connectivity and multi-platform software applications. This paper presents the design, development, and testing of an innovative system named MosquIoT. It is based on traditional ovitraps with embedded Internet of Things (IoT) and Tiny Machine Learning (TinyML) technologies, which enable the detection and quantification of Ae. aegypti eggs. This innovative and promising solution may help dynamically understand the behavior of Ae. aegypti populations in cities, shifting from the current reactive entomological monitoring model to a proactive and predictive digital one.","sentences":["Millions of people around the world are infected with mosquito-borne diseases each year.","One of the most dangerous species is Aedes aegypti, the main vector of viruses such as dengue, yellow fever, chikungunya, and Zika, among others.","Mosquito prevention and eradication campaigns are essential to avoid major public health consequences.","In this respect, entomological surveillance is an important tool.","At present, this traditional monitoring tool is executed manually and requires digital transformation to help authorities make better decisions, improve their planning efforts, speed up execution, and better manage available resources.","Therefore, new technological tools based on proven techniques need to be designed and developed.","However, such tools should also be cost-effective, autonomous, reliable, and easy to implement, and should be enabled by connectivity and multi-platform software applications.","This paper presents the design, development, and testing of an innovative system named MosquIoT.","It is based on traditional ovitraps with embedded Internet of Things (IoT) and Tiny Machine Learning (TinyML) technologies, which enable the detection and quantification of Ae. aegypti eggs.","This innovative and promising solution may help dynamically understand the behavior of Ae. aegypti populations in cities, shifting from the current reactive entomological monitoring model to a proactive and predictive digital one."],"url":"http://arxiv.org/abs/2401.16258v1"}
{"created":"2024-01-29 16:01:46","title":"Cross-silo Federated Learning with Record-level Personalized Differential Privacy","abstract":"Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process. Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement. In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy. We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements. A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\\epsilon}. We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation between q and {\\epsilon} and derive an elegant mathematical model to tackle the problem. Our evaluation demonstrates that our solution can provide significant performance gains over the baselines that do not consider personalized privacy preservation.","sentences":["Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process.","Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement.","In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy.","We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements.","A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\\epsilon}.","We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation between q and {\\epsilon} and derive an elegant mathematical model to tackle the problem.","Our evaluation demonstrates that our solution can provide significant performance gains over the baselines that do not consider personalized privacy preservation."],"url":"http://arxiv.org/abs/2401.16251v1"}
{"created":"2024-01-29 15:49:40","title":"Towards Red Teaming in Multimodal and Multilingual Translation","abstract":"Assessing performance in Natural Language Processing is becoming increasingly complex. One particular challenge is the potential for evaluation datasets to overlap with training data, either directly or indirectly, which can lead to skewed results and overestimation of model performance. As a consequence, human evaluation is gaining increasing interest as a means to assess the performance and reliability of models. One such method is the red teaming approach, which aims to generate edge cases where a model will produce critical errors. While this methodology is becoming standard practice for generative AI, its application to the realm of conditional AI remains largely unexplored. This paper presents the first study on human-based red teaming for Machine Translation (MT), marking a significant step towards understanding and improving the performance of translation models. We delve into both human-based red teaming and a study on automation, reporting lessons learned and providing recommendations for both translation models and red teaming drills. This pioneering work opens up new avenues for research and development in the field of MT.","sentences":["Assessing performance in Natural Language Processing is becoming increasingly complex.","One particular challenge is the potential for evaluation datasets to overlap with training data, either directly or indirectly, which can lead to skewed results and overestimation of model performance.","As a consequence, human evaluation is gaining increasing interest as a means to assess the performance and reliability of models.","One such method is the red teaming approach, which aims to generate edge cases where a model will produce critical errors.","While this methodology is becoming standard practice for generative AI, its application to the realm of conditional AI remains largely unexplored.","This paper presents the first study on human-based red teaming for Machine Translation (MT), marking a significant step towards understanding and improving the performance of translation models.","We delve into both human-based red teaming and a study on automation, reporting lessons learned and providing recommendations for both translation models and red teaming drills.","This pioneering work opens up new avenues for research and development in the field of MT."],"url":"http://arxiv.org/abs/2401.16247v1"}
{"created":"2024-01-29 15:43:31","title":"Channel Estimation and Hybrid Precoding for Frequency Selective Multiuser mmWave MIMO Systems","abstract":"Configuring the hybrid precoders and combiners in a millimeter wave (mmWave) multiuser (MU) multiple-input multiple-output (MIMO) system is challenging in frequency selective channels. In this paper, we develop a system that uses compressive estimation on the uplink to configure precoders and combiners for the downlink (DL). In the first step, the base station (BS) simultaneously estimates the channels from all the mobile stations (MSs) on each subcarrier. To reduce the number of measurements required, compressed sensing techniques are developed that exploit common support on the different subcarriers. In the second step, exploiting reciprocity and the channel estimates, the base station designs hybrid precoders and combiners. Two algorithms are developed for this purpose, with different performance and complexity tradeoffs: 1) a factorization of the purely digital solution, and 2) an iterative hybrid design. Extensive numerical experiments evaluate the proposed solutions comparing to state-of-the-art strategies, and illustrating design tradeoffs in overhead, complexity, and performance.","sentences":["Configuring the hybrid precoders and combiners in a millimeter wave (mmWave) multiuser (MU) multiple-input multiple-output (MIMO) system is challenging in frequency selective channels.","In this paper, we develop a system that uses compressive estimation on the uplink to configure precoders and combiners for the downlink (DL).","In the first step, the base station (BS) simultaneously estimates the channels from all the mobile stations (MSs) on each subcarrier.","To reduce the number of measurements required, compressed sensing techniques are developed that exploit common support on the different subcarriers.","In the second step, exploiting reciprocity and the channel estimates, the base station designs hybrid precoders and combiners.","Two algorithms are developed for this purpose, with different performance and complexity tradeoffs: 1) a factorization of the purely digital solution, and 2) an iterative hybrid design.","Extensive numerical experiments evaluate the proposed solutions comparing to state-of-the-art strategies, and illustrating design tradeoffs in overhead, complexity, and performance."],"url":"http://arxiv.org/abs/2401.16241v1"}
{"created":"2024-01-29 15:42:57","title":"Clinically meaningful timeline summarisation in social media for mental health monitoring","abstract":"We introduce the new task of clinically meaningful summarisation of social media user timelines, appropriate for mental health monitoring. We develop a novel approach for unsupervised abstractive summarisation that produces a two-layer summary consisting of both high-level information, covering aspects useful to clinical experts, as well as accompanying time sensitive evidence from a user's social media timeline. A key methodological novelty comes from the timeline summarisation component based on a version of hierarchical variational autoencoder (VAE) adapted to represent long texts and guided by LLM-annotated key phrases. The resulting timeline summary is input into a LLM (LLaMA-2) to produce the final summary containing both the high level information, obtained through instruction prompting, as well as corresponding evidence from the user's timeline. We assess the summaries generated by our novel architecture via automatic evaluation against expert written summaries and via human evaluation with clinical experts, showing that timeline summarisation by TH-VAE results in logically coherent summaries rich in clinical utility and superior to LLM-only approaches in capturing changes over time.","sentences":["We introduce the new task of clinically meaningful summarisation of social media user timelines, appropriate for mental health monitoring.","We develop a novel approach for unsupervised abstractive summarisation that produces a two-layer summary consisting of both high-level information, covering aspects useful to clinical experts, as well as accompanying time sensitive evidence from a user's social media timeline.","A key methodological novelty comes from the timeline summarisation component based on a version of hierarchical variational autoencoder (VAE) adapted to represent long texts and guided by LLM-annotated key phrases.","The resulting timeline summary is input into a LLM (LLaMA-2) to produce the final summary containing both the high level information, obtained through instruction prompting, as well as corresponding evidence from the user's timeline.","We assess the summaries generated by our novel architecture via automatic evaluation against expert written summaries and via human evaluation with clinical experts, showing that timeline summarisation by TH-VAE results in logically coherent summaries rich in clinical utility and superior to LLM-only approaches in capturing changes over time."],"url":"http://arxiv.org/abs/2401.16240v1"}
{"created":"2024-01-29 15:42:22","title":"Alternating Minimization for Wideband Multiuser IRS-aided MIMO Systems under Imperfect CSI","abstract":"This work focuses on wideband intelligent reflecting surface (IRS)-aided multiuser MIMO systems. One of the major challenges of this scenario is the joint design of the frequency-dependent base station (BS) precoder and user filters, and the IRS phase-shift matrix which is frequency flat and common to all the users. In addition, we consider that the channel state information (CSI) is imperfect at both the transmitter and the receivers. A statistical model for the imperfect CSI is developed and exploited for the system design. A minimum mean square error (MMSE) approach is followed to determine the IRS phase-shift matrix, the transmit precoders, and the receiving filters. The broadcast (BC)- multiple access channel (MAC) duality is used to solve the optimization problem following an alternating minimization approach. Numerical results show that the proposed approach leads to substantial performance gains with respect to baseline strategies that neglect the inter-user interference and do not optimize the IRS phase-shift matrix. Further performance gains are obtained when incorporating into the system design the statistical information of the channel estimation errors.","sentences":["This work focuses on wideband intelligent reflecting surface (IRS)-aided multiuser MIMO systems.","One of the major challenges of this scenario is the joint design of the frequency-dependent base station (BS) precoder and user filters, and the IRS phase-shift matrix which is frequency flat and common to all the users.","In addition, we consider that the channel state information (CSI) is imperfect at both the transmitter and the receivers.","A statistical model for the imperfect CSI is developed and exploited for the system design.","A minimum mean square error (MMSE) approach is followed to determine the IRS phase-shift matrix, the transmit precoders, and the receiving filters.","The broadcast (BC)- multiple access channel (MAC) duality is used to solve the optimization problem following an alternating minimization approach.","Numerical results show that the proposed approach leads to substantial performance gains with respect to baseline strategies that neglect the inter-user interference and do not optimize the IRS phase-shift matrix.","Further performance gains are obtained when incorporating into the system design the statistical information of the channel estimation errors."],"url":"http://arxiv.org/abs/2401.16238v1"}
{"created":"2024-01-29 15:35:05","title":"Effective Communication with Dynamic Feature Compression","abstract":"The remote wireless control of industrial systems is one of the major use cases for 5G and beyond systems: in these cases, the massive amounts of sensory information that need to be shared over the wireless medium may overload even high-capacity connections. Consequently, solving the effective communication problem by optimizing the transmission strategy to discard irrelevant information can provide a significant advantage, but is often a very complex task. In this work, we consider a prototypal system in which an observer must communicate its sensory data to a robot controlling a task (e.g., a mobile robot in a factory). We then model it as a remote Partially Observable Markov Decision Process (POMDP), considering the effect of adopting semantic and effective communication-oriented solutions on the overall system performance. We split the communication problem by considering an ensemble Vector Quantized Variational Autoencoder (VQ-VAE) encoding, and train a Deep Reinforcement Learning (DRL) agent to dynamically adapt the quantization level, considering both the current state of the environment and the memory of past messages. We tested the proposed approach on the well-known CartPole reference control problem, obtaining a significant performance increase over traditional approaches.","sentences":["The remote wireless control of industrial systems is one of the major use cases for 5G and beyond systems: in these cases, the massive amounts of sensory information that need to be shared over the wireless medium may overload even high-capacity connections.","Consequently, solving the effective communication problem by optimizing the transmission strategy to discard irrelevant information can provide a significant advantage, but is often a very complex task.","In this work, we consider a prototypal system in which an observer must communicate its sensory data to a robot controlling a task (e.g., a mobile robot in a factory).","We then model it as a remote Partially Observable Markov Decision Process (POMDP), considering the effect of adopting semantic and effective communication-oriented solutions on the overall system performance.","We split the communication problem by considering an ensemble Vector Quantized Variational Autoencoder (VQ-VAE) encoding, and train a Deep Reinforcement Learning (DRL) agent to dynamically adapt the quantization level, considering both the current state of the environment and the memory of past messages.","We tested the proposed approach on the well-known CartPole reference control problem, obtaining a significant performance increase over traditional approaches."],"url":"http://arxiv.org/abs/2401.16236v1"}
{"created":"2024-01-29 15:34:49","title":"Player Pressure Map - A Novel Representation of Pressure in Soccer for Evaluating Player Performance in Different Game Contexts","abstract":"In soccer, contextual player performance metrics are invaluable to coaches. For example, the ability to perform under pressure during matches distinguishes the elite from the average. Appropriate pressure metric enables teams to assess players' performance accurately under pressure and design targeted training scenarios to address their weaknesses. The primary objective of this paper is to leverage both tracking and event data and game footage to capture the pressure experienced by the possession team in a soccer game scene. We propose a player pressure map to represent a given game scene, which lowers the dimension of raw data and still contains rich contextual information. Not only does it serve as an effective tool for visualizing and evaluating the pressure on the team and each individual, but it can also be utilized as a backbone for accessing players' performance. Overall, our model provides coaches and analysts with a deeper understanding of players' performance under pressure so that they make data-oriented tactical decisions.","sentences":["In soccer, contextual player performance metrics are invaluable to coaches.","For example, the ability to perform under pressure during matches distinguishes the elite from the average.","Appropriate pressure metric enables teams to assess players' performance accurately under pressure and design targeted training scenarios to address their weaknesses.","The primary objective of this paper is to leverage both tracking and event data and game footage to capture the pressure experienced by the possession team in a soccer game scene.","We propose a player pressure map to represent a given game scene, which lowers the dimension of raw data and still contains rich contextual information.","Not only does it serve as an effective tool for visualizing and evaluating the pressure on the team and each individual, but it can also be utilized as a backbone for accessing players' performance.","Overall, our model provides coaches and analysts with a deeper understanding of players' performance under pressure so that they make data-oriented tactical decisions."],"url":"http://arxiv.org/abs/2401.16235v1"}
{"created":"2024-01-29 15:34:41","title":"DAEDALUS: Defense Against Firmware ROP Exploits Using Stochastic Software Diversity","abstract":"This paper presents DAEDALUS, a software diversity-based framework designed to resist ROP attacks on Linux-based IoT devices. DAEDALUS generates unique, semantically equivalent but syntactically different rewrites of IoT firmware, disrupting large-scale replication of ROP attacks. DAEDALUS employs STOKE, a stochastic optimizer for x86 binaries, as its core diversity engine but introduces significant extensions to address unique IoT firmware challenges. DAEDALUS's effectiveness is evaluated using DDoSim, a published botnet DDoS attack simulation testbed. Results demonstrate that DAEDALUS successfully neutralizes ROP payloads by diversifying critical basic blocks in the firmware, preventing attackers from compromising multiple devices for DDoS attacks via memory error vulnerabilities. The findings indicate that DAEDALUS not only mitigates the impact of ROP attacks on individual IoT devices through probabilistic protection but also thwarts large-scale ROP attacks across multiple devices.","sentences":["This paper presents DAEDALUS, a software diversity-based framework designed to resist ROP attacks on Linux-based IoT devices.","DAEDALUS generates unique, semantically equivalent but syntactically different rewrites of IoT firmware, disrupting large-scale replication of ROP attacks.","DAEDALUS employs STOKE, a stochastic optimizer for x86 binaries, as its core diversity engine","but introduces significant extensions to address unique IoT firmware challenges.","DAEDALUS's effectiveness is evaluated using DDoSim, a published botnet DDoS attack simulation testbed.","Results demonstrate that DAEDALUS successfully neutralizes ROP payloads by diversifying critical basic blocks in the firmware, preventing attackers from compromising multiple devices for DDoS attacks via memory error vulnerabilities.","The findings indicate that DAEDALUS not only mitigates the impact of ROP attacks on individual IoT devices through probabilistic protection but also thwarts large-scale ROP attacks across multiple devices."],"url":"http://arxiv.org/abs/2401.16234v1"}
{"created":"2024-01-29 15:32:18","title":"Cross-Database Liveness Detection: Insights from Comparative Biometric Analysis","abstract":"In an era where biometric security serves as a keystone of modern identity verification systems, ensuring the authenticity of these biometric samples is paramount. Liveness detection, the capability to differentiate between genuine and spoofed biometric samples, stands at the forefront of this challenge. This research presents a comprehensive evaluation of liveness detection models, with a particular focus on their performance in cross-database scenarios, a test paradigm notorious for its complexity and real-world relevance. Our study commenced by meticulously assessing models on individual datasets, revealing the nuances in their performance metrics. Delving into metrics such as the Half Total Error Rate, False Acceptance Rate, and False Rejection Rate, we unearthed invaluable insights into the models' strengths and weaknesses. Crucially, our exploration of cross-database testing provided a unique perspective, highlighting the chasm between training on one dataset and deploying on another. Comparative analysis with extant methodologies, ranging from convolutional networks to more intricate strategies, enriched our understanding of the current landscape. The variance in performance, even among state-of-the-art models, underscored the inherent challenges in this domain. In essence, this paper serves as both a repository of findings and a clarion call for more nuanced, data-diverse, and adaptable approaches in biometric liveness detection. In the dynamic dance between authenticity and deception, our work offers a blueprint for navigating the evolving rhythms of biometric security.","sentences":["In an era where biometric security serves as a keystone of modern identity verification systems, ensuring the authenticity of these biometric samples is paramount.","Liveness detection, the capability to differentiate between genuine and spoofed biometric samples, stands at the forefront of this challenge.","This research presents a comprehensive evaluation of liveness detection models, with a particular focus on their performance in cross-database scenarios, a test paradigm notorious for its complexity and real-world relevance.","Our study commenced by meticulously assessing models on individual datasets, revealing the nuances in their performance metrics.","Delving into metrics such as the Half Total Error Rate, False Acceptance Rate, and False Rejection Rate, we unearthed invaluable insights into the models' strengths and weaknesses.","Crucially, our exploration of cross-database testing provided a unique perspective, highlighting the chasm between training on one dataset and deploying on another.","Comparative analysis with extant methodologies, ranging from convolutional networks to more intricate strategies, enriched our understanding of the current landscape.","The variance in performance, even among state-of-the-art models, underscored the inherent challenges in this domain.","In essence, this paper serves as both a repository of findings and a clarion call for more nuanced, data-diverse, and adaptable approaches in biometric liveness detection.","In the dynamic dance between authenticity and deception, our work offers a blueprint for navigating the evolving rhythms of biometric security."],"url":"http://arxiv.org/abs/2401.16232v1"}
{"created":"2024-01-29 15:30:47","title":"Error Mitigation for Thermodynamic Computing","abstract":"While physics-based computing can offer speed and energy efficiency compared to digital computing, it also is subject to errors that must be mitigated. For example, many error mitigation methods have been proposed for quantum computing. However this error mitigation framework has yet to be applied to other physics-based computing paradigms. In this work, we consider thermodynamic computing, which has recently captured attention due to its relevance to artificial intelligence (AI) applications, such as probabilistic AI and generative AI. A key source of errors in this paradigm is the imprecision of the analog hardware components. Here, we introduce a method that reduces the overall error from a linear to a quadratic dependence (from $\\epsilon$ to $\\epsilon^2$) on the imprecision $\\epsilon$, for Gaussian sampling and linear algebra applications. The method involves sampling from an ensemble of imprecise distributions associated with various rounding events and then merging these samples. We numerically demonstrate the scalability of this method for dimensions greater than 1000. Finally, we implement this method on an actual thermodynamic computer and show $20\\%$ error reduction for matrix inversion; the first thermodynamic error mitigation experiment.","sentences":["While physics-based computing can offer speed and energy efficiency compared to digital computing, it also is subject to errors that must be mitigated.","For example, many error mitigation methods have been proposed for quantum computing.","However this error mitigation framework has yet to be applied to other physics-based computing paradigms.","In this work, we consider thermodynamic computing, which has recently captured attention due to its relevance to artificial intelligence (AI) applications, such as probabilistic AI and generative AI.","A key source of errors in this paradigm is the imprecision of the analog hardware components.","Here, we introduce a method that reduces the overall error from a linear to a quadratic dependence (from $\\epsilon$ to $\\epsilon^2$) on the imprecision $\\epsilon$, for Gaussian sampling and linear algebra applications.","The method involves sampling from an ensemble of imprecise distributions associated with various rounding events and then merging these samples.","We numerically demonstrate the scalability of this method for dimensions greater than 1000.","Finally, we implement this method on an actual thermodynamic computer and show $20\\%$ error reduction for matrix inversion; the first thermodynamic error mitigation experiment."],"url":"http://arxiv.org/abs/2401.16231v1"}
{"created":"2024-01-29 15:30:29","title":"Elementary first-order model checking for sparse graphs","abstract":"It is known that for subgraph-closed graph classes the first-order model checking problem is fixed-parameter tractable if and only if the class is nowhere dense [Grohe, Kreutzer, Siebertz, STOC 2014]. However, the dependency on the formula size is non-elementary, and in fact, this is unavoidable even for the class of all trees [Frick and Grohe, LICS 2002]. On the other hand, it is known that the dependency is elementary for classes of bounded degree [Frick and Grohe, LICS 2002] as well as for classes of bounded pathwidth [Lampis, ICALP 2023]. In this paper we generalise these results and almost completely characterise subgraph-closed graph classes for which the model checking problem is fixed-parameter tractable with an elementary dependency on the formula size. Those are the graph classes for which there exists a number $d$ such that for every $r$, some tree of depth $d$ and size bounded by an elementary function of $r$ is avoided as an $({\\leq} r)$-subdivision in all graphs in the class. In particular, this implies that if the class in question excludes a fixed tree as a topological minor, then first-order model checking for graphs in the class is fixed-parameter tractable with an elementary dependency on the formula size.","sentences":["It is known that for subgraph-closed graph classes the first-order model checking problem is fixed-parameter tractable if and only if the class is nowhere dense [Grohe, Kreutzer, Siebertz, STOC 2014].","However, the dependency on the formula size is non-elementary, and in fact, this is unavoidable even for the class of all trees [Frick and Grohe, LICS 2002].","On the other hand, it is known that the dependency is elementary for classes of bounded degree","[Frick and Grohe, LICS 2002] as well as for classes of bounded pathwidth [Lampis, ICALP 2023].","In this paper we generalise these results and almost completely characterise subgraph-closed graph classes for which the model checking problem is fixed-parameter tractable with an elementary dependency on the formula size.","Those are the graph classes for which there exists a number $d$ such that for every $r$, some tree of depth $d$ and size bounded by an elementary function of $r$ is avoided as an $({\\leq} r)$-subdivision in all graphs in the class.","In particular, this implies that if the class in question excludes a fixed tree as a topological minor, then first-order model checking for graphs in the class is fixed-parameter tractable with an elementary dependency on the formula size."],"url":"http://arxiv.org/abs/2401.16230v1"}
{"created":"2024-01-29 15:29:17","title":"On the Anatomy of Real-World R Code for Static Analysis","abstract":"CONTEXT The R programming language has a huge and active community, especially in the area of statistical computing. Its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of R programs. At the same time, there is a lack of existing research regarding how these features, or even the R language as a whole are used in practice. OBJECTIVE In this paper, we conduct a large-scale, static analysis of more than 50 million lines of real-world R programs and packages to identify their characteristics and the features that are actually used. Moreover, we compare the similarities and differences between the scripts of R users and the implementations of package authors. We provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research. METHOD We analyze 4230 R scripts submitted alongside publications and the sources of 19450 CRAN packages for over 350000 R files, collecting and summarizing quantitative information for features of interest. RESULTS We find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of R's reflective functions. Furthermore, we find neither testing functions nor many calls to R's foreign function interface (FFI) in the publication submissions. CONCLUSION R scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of R's reflective capabilities. We provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like load.","sentences":["CONTEXT The R programming language has a huge and active community, especially in the area of statistical computing.","Its interpreted nature allows for several interesting constructs, like the manipulation of functions at run-time, that hinder the static analysis of R programs.","At the same time, there is a lack of existing research regarding how these features, or even the R language as a whole are used in practice.","OBJECTIVE","In this paper, we conduct a large-scale, static analysis of more than 50 million lines of real-world R programs and packages to identify their characteristics and the features that are actually used.","Moreover, we compare the similarities and differences between the scripts of R users and the implementations of package authors.","We provide insights for static analysis tools like the lintr package as well as potential interpreter optimizations and uncover areas for future research.","METHOD We analyze 4230 R scripts submitted alongside publications and the sources of 19450 CRAN packages for over 350000 R files, collecting and summarizing quantitative information for features of interest.","RESULTS We find a high frequency of name-based indexing operations, assignments, and loops, but a low frequency for most of R's reflective functions.","Furthermore, we find neither testing functions nor many calls to R's foreign function interface (FFI) in the publication submissions.","CONCLUSION R scripts and package sources differ, for example, in their size, the way they include other packages, and their usage of R's reflective capabilities.","We provide features that are used frequently and should be prioritized by static analysis tools, like operator assignments, function calls, and certain reflective functions like load."],"url":"http://arxiv.org/abs/2401.16228v1"}
{"created":"2024-01-29 15:21:37","title":"Diffutoon: High-Resolution Editable Toon Shading via Diffusion Models","abstract":"Toon shading is a type of non-photorealistic rendering task of animation. Its primary purpose is to render objects with a flat and stylized appearance. As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles. In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and achieving high visual quality. In this paper, we model the toon shading problem as four subproblems: stylization, consistency enhancement, structure guidance, and colorization. To address the challenges in video stylization, we propose an effective toon shading approach called \\textit{Diffutoon}. Diffutoon is capable of rendering remarkably detailed, high-resolution, and extended-duration videos in anime style. It can also edit the content according to prompts via an additional branch. The efficacy of Diffutoon is evaluated through quantitive metrics and human evaluation. Notably, Diffutoon surpasses both open-source and closed-source baseline approaches in our experiments. Our work is accompanied by the release of both the source code and example videos on Github (Project page: https://ecnu-cilab.github.io/DiffutoonProjectPage/).","sentences":["Toon shading is a type of non-photorealistic rendering task of animation.","Its primary purpose is to render objects with a flat and stylized appearance.","As diffusion models have ascended to the forefront of image synthesis methodologies, this paper delves into an innovative form of toon shading based on diffusion models, aiming to directly render photorealistic videos into anime styles.","In video stylization, extant methods encounter persistent challenges, notably in maintaining consistency and achieving high visual quality.","In this paper, we model the toon shading problem as four subproblems: stylization, consistency enhancement, structure guidance, and colorization.","To address the challenges in video stylization, we propose an effective toon shading approach called \\textit{Diffutoon}.","Diffutoon is capable of rendering remarkably detailed, high-resolution, and extended-duration videos in anime style.","It can also edit the content according to prompts via an additional branch.","The efficacy of Diffutoon is evaluated through quantitive metrics and human evaluation.","Notably, Diffutoon surpasses both open-source and closed-source baseline approaches in our experiments.","Our work is accompanied by the release of both the source code and example videos on Github (Project page: https://ecnu-cilab.github.io/DiffutoonProjectPage/)."],"url":"http://arxiv.org/abs/2401.16224v1"}
{"created":"2024-01-29 15:10:09","title":"A mechanism for discovering semantic relationships among agent communication protocols","abstract":"One relevant aspect in the development of the Semantic Web framework is the achievement of a real inter-agents communication capability at the semantic level. Agents should be able to communicate with each other freely using different communication protocols, constituted by communication acts. For that scenario, we introduce in this paper an efficient mechanism presenting the following main features: - It promotes the description of the communication acts of protocols as classes that belong to a communication acts ontology, and associates to those acts a social commitment semantics formalized through predicates in the Event Calculus. - It is sustained on the idea that different protocols can be compared semantically by looking to the set of fluents associated to each branch of the protocols. Those sets are generated using Semantic Web technology rules. - It discovers the following types of protocol relationships: equivalence, specialization, restriction, prefix, suffix, infix and complement_to_infix.","sentences":["One relevant aspect in the development of the Semantic Web framework is the achievement of a real inter-agents communication capability at the semantic level.","Agents should be able to communicate with each other freely using different communication protocols, constituted by communication acts.","For that scenario, we introduce in this paper an efficient mechanism presenting the following main features: - It promotes the description of the communication acts of protocols as classes that belong to a communication acts ontology, and associates to those acts a social commitment semantics formalized through predicates in the Event Calculus.","- It is sustained on the idea that different protocols can be compared semantically by looking to the set of fluents associated to each branch of the protocols.","Those sets are generated using Semantic Web technology rules.","- It discovers the following types of protocol relationships: equivalence, specialization, restriction, prefix, suffix, infix and complement_to_infix."],"url":"http://arxiv.org/abs/2401.16216v1"}
