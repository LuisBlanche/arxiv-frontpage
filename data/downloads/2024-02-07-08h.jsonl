{"created":"2024-02-06 18:59:57","title":"AnyTool: Self-Reflective, Hierarchical Agents for Large-Scale API Calls","abstract":"We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries. We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries. AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable. AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules. We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate. By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench. Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization. For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench. Code will be available at https://github.com/dyabel/AnyTool.","sentences":["We introduce AnyTool, a large language model agent designed to revolutionize the utilization of a vast array of tools in addressing user queries.","We utilize over 16,000 APIs from Rapid API, operating under the assumption that a subset of these APIs could potentially resolve the queries.","AnyTool primarily incorporates three elements: an API retriever with a hierarchical structure, a solver aimed at resolving user queries using a selected set of API candidates, and a self-reflection mechanism, which re-activates AnyTool if the initial solution proves impracticable.","AnyTool is powered by the function calling feature of GPT-4, eliminating the need for training external modules.","We also revisit the evaluation protocol introduced by previous works and identify a limitation in this protocol that leads to an artificially high pass rate.","By revising the evaluation protocol to better reflect practical application scenarios, we introduce an additional benchmark, termed AnyToolBench.","Experiments across various datasets demonstrate the superiority of our AnyTool over strong baselines such as ToolLLM and a GPT-4 variant tailored for tool utilization.","For instance, AnyTool outperforms ToolLLM by +35.4% in terms of average pass rate on ToolBench.","Code will be available at https://github.com/dyabel/AnyTool."],"url":"http://arxiv.org/abs/2402.04253v1"}
{"created":"2024-02-06 18:59:48","title":"EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters","abstract":"Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models. We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18-billion parameters. With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin. Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion image-text pairs from LAION-2B and COYO-700M. This dataset is openly available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B) employed in other state-of-the-art CLIP models. EVA-CLIP-18B demonstrates the potential of EVA-style weak-to-strong visual model scaling. With our model weights made publicly available, we hope to facilitate future research in vision and multimodal foundation models.","sentences":["Scaling up contrastive language-image pretraining (CLIP) is critical for empowering both vision and multimodal models.","We present EVA-CLIP-18B, the largest and most powerful open-source CLIP model to date, with 18-billion parameters.","With only 6-billion training samples seen, EVA-CLIP-18B achieves an exceptional 80.7% zero-shot top-1 accuracy averaged across 27 widely recognized image classification benchmarks, outperforming its forerunner EVA-CLIP (5-billion parameters) and other open-source CLIP models by a large margin.","Remarkably, we observe a consistent performance improvement with the model size scaling of EVA-CLIP, despite maintaining a constant training dataset of 2-billion image-text pairs from LAION-2B and COYO-700M.","This dataset is openly available and much smaller than the in-house datasets (e.g., DFN-5B, WebLI-10B) employed in other state-of-the-art CLIP models.","EVA-CLIP-18B demonstrates the potential of EVA-style weak-to-strong visual model scaling.","With our model weights made publicly available, we hope to facilitate future research in vision and multimodal foundation models."],"url":"http://arxiv.org/abs/2402.04252v1"}
{"created":"2024-02-06 18:59:30","title":"Linear-time Minimum Bayes Risk Decoding with Reference Aggregation","abstract":"Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used. Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity. In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations. This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding. We release our source code at https://github.com/ZurichNLP/mbr","sentences":["Minimum Bayes Risk (MBR) decoding is a text generation technique that has been shown to improve the quality of machine translations, but is expensive, even if a sampling-based approximation is used.","Besides requiring a large number of sampled sequences, it requires the pairwise calculation of a utility metric, which has quadratic complexity.","In this paper, we propose to approximate pairwise metric scores with scores calculated against aggregated reference representations.","This changes the complexity of utility estimation from $O(n^2)$ to $O(n)$, while empirically preserving most of the quality gains of MBR decoding.","We release our source code at https://github.com/ZurichNLP/mbr"],"url":"http://arxiv.org/abs/2402.04251v1"}
{"created":"2024-02-06 18:59:08","title":"HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal","abstract":"Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods. To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming. We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria. Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights. We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses. We open source HarmBench at https://github.com/centerforaisafety/HarmBench.","sentences":["Automated red teaming holds substantial promise for uncovering and mitigating the risks associated with the malicious use of large language models (LLMs), yet the field lacks a standardized evaluation framework to rigorously assess new methods.","To address this issue, we introduce HarmBench, a standardized evaluation framework for automated red teaming.","We identify several desirable properties previously unaccounted for in red teaming evaluations and systematically design HarmBench to meet these criteria.","Using HarmBench, we conduct a large-scale comparison of 18 red teaming methods and 33 target LLMs and defenses, yielding novel insights.","We also introduce a highly efficient adversarial training method that greatly enhances LLM robustness across a wide range of attacks, demonstrating how HarmBench enables codevelopment of attacks and defenses.","We open source HarmBench at https://github.com/centerforaisafety/HarmBench."],"url":"http://arxiv.org/abs/2402.04249v1"}
{"created":"2024-02-06 18:56:35","title":"Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks","abstract":"State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, \\variant, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.","sentences":["State-space models (SSMs), such as Mamba Gu & Dao (2034), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention.","Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers.","In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks.","Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning.","However, SSMs fall short in tasks involving non-standard retrieval functionality.","To address these limitations, we introduce a hybrid model, \\variant, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently.","Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models."],"url":"http://arxiv.org/abs/2402.04248v1"}
{"created":"2024-02-06 18:54:07","title":"Prioritizing Safeguarding Over Autonomy: Risks of LLM Agents for Science","abstract":"Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines. While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety. However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities. This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures. We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment. Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works. Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks. Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively.","sentences":["Intelligent agents powered by large language models (LLMs) have demonstrated substantial promise in autonomously conducting experiments and facilitating scientific discoveries across various disciplines.","While their capabilities are promising, they also introduce novel vulnerabilities that demand careful consideration for safety.","However, there exists a notable gap in the literature, as there has been no comprehensive exploration of these vulnerabilities.","This position paper fills this gap by conducting a thorough examination of vulnerabilities in LLM-based agents within scientific domains, shedding light on potential risks associated with their misuse and emphasizing the need for safety measures.","We begin by providing a comprehensive overview of the potential risks inherent to scientific LLM agents, taking into account user intent, the specific scientific domain, and their potential impact on the external environment.","Then, we delve into the origins of these vulnerabilities and provide a scoping review of the limited existing works.","Based on our analysis, we propose a triadic framework involving human regulation, agent alignment, and an understanding of environmental feedback (agent regulation) to mitigate these identified risks.","Furthermore, we highlight the limitations and challenges associated with safeguarding scientific agents and advocate for the development of improved models, robust benchmarks, and comprehensive regulations to address these issues effectively."],"url":"http://arxiv.org/abs/2402.04247v1"}
{"created":"2024-02-06 18:47:52","title":"CAST: Clustering Self-Attention using Surrogate Tokens for Efficient Transformers","abstract":"The Transformer architecture has shown to be a powerful tool for a wide range of tasks. It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers. In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers. CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries. The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence. CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is constant according to the number of clusters and samples per cluster. We show that CAST performs better than or comparable to the baseline Transformers on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient transformers.","sentences":["The Transformer architecture has shown to be a powerful tool for a wide range of tasks.","It is based on the self-attention mechanism, which is an inherently computationally expensive operation with quadratic computational complexity: memory usage and compute time increase quadratically with the length of the input sequences, thus limiting the application of Transformers.","In this work, we propose a novel Clustering self-Attention mechanism using Surrogate Tokens (CAST), to optimize the attention computation and achieve efficient transformers.","CAST utilizes learnable surrogate tokens to construct a cluster affinity matrix, used to cluster the input sequence and generate novel cluster summaries.","The self-attention from within each cluster is then combined with the cluster summaries of other clusters, enabling information flow across the entire input sequence.","CAST improves efficiency by reducing the complexity from $O(N^2)$ to $O(\\alpha N)$ where N is the sequence length, and {\\alpha} is constant according to the number of clusters and samples per cluster.","We show that CAST performs better than or comparable to the baseline Transformers on long-range sequence modeling tasks, while also achieving higher results on time and memory efficiency than other efficient transformers."],"url":"http://arxiv.org/abs/2402.04239v1"}
{"created":"2024-02-06 18:43:48","title":"CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations","abstract":"Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers. However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses. In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in). This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths. We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism. Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance. The code and data are publicly available at https://github.com/THUDM/CogCoM.","sentences":["Vision-Language Models (VLMs) have demonstrated their widespread viability thanks to extensive training in aligning visual instructions to answers.","However, this conclusive alignment leads models to ignore critical visual reasoning, and further result in failures on meticulous visual problems and unfaithful responses.","In this paper, we propose Chain of Manipulations, a mechanism that enables VLMs to solve problems with a series of manipulations, where each manipulation refers to an operation on the visual input, either from intrinsic abilities (e.g., grounding) acquired through prior training or from imitating human-like behaviors (e.g., zoom in).","This mechanism encourages VLMs to generate faithful responses with evidential visual reasoning, and permits users to trace error causes in the interpretable paths.","We thus train CogCoM, a general 17B VLM with a memory-based compatible architecture endowed this reasoning mechanism.","Experiments show that our model achieves the state-of-the-art performance across 8 benchmarks from 3 categories, and a limited number of training steps with the data swiftly gains a competitive performance.","The code and data are publicly available at https://github.com/THUDM/CogCoM."],"url":"http://arxiv.org/abs/2402.04236v1"}
{"created":"2024-02-06 18:42:51","title":"LIPSTICK: Corruptibility-Aware and Explainable Graph Neural Network-based Oracle-Less Attack on Logic Locking","abstract":"In a zero-trust fabless paradigm, designers are increasingly concerned about hardware-based attacks on the semiconductor supply chain. Logic locking is a design-for-trust method that adds extra key-controlled gates in the circuits to prevent hardware intellectual property theft and overproduction. While attackers have traditionally relied on an oracle to attack logic-locked circuits, machine learning attacks have shown the ability to retrieve the secret key even without access to an oracle. In this paper, we first examine the limitations of state-of-the-art machine learning attacks and argue that the use of key hamming distance as the sole model-guiding structural metric is not always useful. Then, we develop, train, and test a corruptibility-aware graph neural network-based oracle-less attack on logic locking that takes into consideration both the structure and the behavior of the circuits. Our model is explainable in the sense that we analyze what the machine learning model has interpreted in the training process and how it can perform a successful attack. Chip designers may find this information beneficial in securing their designs while avoiding incremental fixes.","sentences":["In a zero-trust fabless paradigm, designers are increasingly concerned about hardware-based attacks on the semiconductor supply chain.","Logic locking is a design-for-trust method that adds extra key-controlled gates in the circuits to prevent hardware intellectual property theft and overproduction.","While attackers have traditionally relied on an oracle to attack logic-locked circuits, machine learning attacks have shown the ability to retrieve the secret key even without access to an oracle.","In this paper, we first examine the limitations of state-of-the-art machine learning attacks and argue that the use of key hamming distance as the sole model-guiding structural metric is not always useful.","Then, we develop, train, and test a corruptibility-aware graph neural network-based oracle-less attack on logic locking that takes into consideration both the structure and the behavior of the circuits.","Our model is explainable in the sense that we analyze what the machine learning model has interpreted in the training process and how it can perform a successful attack.","Chip designers may find this information beneficial in securing their designs while avoiding incremental fixes."],"url":"http://arxiv.org/abs/2402.04235v1"}
{"created":"2024-02-06 18:39:43","title":"Can Generative Agents Predict Emotion?","abstract":"Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans. In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories. Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation. First, the agent perceives new experiences as time series text data. After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm. Through this comparison we can analyse how the agent reacts to the new experience in context. The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event. Finally, the new experience is then added to the agents memory to be used in the creation of future norms. By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios. The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary. We hope that this paper is another step towards the alignment of generative agents.","sentences":["Large Language Models (LLMs) have demonstrated a number of human-like abilities, however the empathic understanding and emotional state of LLMs is yet to be aligned to that of humans.","In this work, we investigate how the emotional state of generative LLM agents evolves as they perceive new events, introducing a novel architecture in which new experiences are compared to past memories.","Through this comparison, the agent gains the ability to understand new experiences in context, which according to the appraisal theory of emotion is vital in emotion creation.","First, the agent perceives new experiences as time series text data.","After perceiving each new input, the agent generates a summary of past relevant memories, referred to as the norm, and compares the new experience to this norm.","Through this comparison we can analyse how the agent reacts to the new experience in context.","The PANAS, a test of affect, is administered to the agent, capturing the emotional state of the agent after the perception of the new event.","Finally, the new experience is then added to the agents memory to be used in the creation of future norms.","By creating multiple experiences in natural language from emotionally charged situations, we test the proposed architecture on a wide range of scenarios.","The mixed results suggests that introducing context can occasionally improve the emotional alignment of the agent, but further study and comparison with human evaluators is necessary.","We hope that this paper is another step towards the alignment of generative agents."],"url":"http://arxiv.org/abs/2402.04232v1"}
{"created":"2024-02-06 18:39:25","title":"Further Constructions of AMUBs for Non-prime power Composite Dimensions","abstract":"Construction of a large class of Mutually Unbiased Bases (MUBs) for non-prime power composite dimensions ($d = k\\times s$) is a long standing open problem, which leads to different construction methods for the class Approximate MUBs (AMUBs) by relaxing the criterion that the absolute value of the dot product between two vectors chosen from different bases should be $\\leq \\frac{\\beta}{\\sqrt{d}}$. In this chapter, we consider a more general class of AMUBs (ARMUBs, considering the real ones too), compared to our earlier work in [Cryptography and Communications, 14(3): 527--549, 2022]. We note that the quality of AMUBs (ARMUBs) constructed using RBD$(X,A)$ with $|X|= d$, critically depends on the parameters, $|s-k|$, $\\mu$ (maximum number of elements common between any pair of blocks), and the set of block sizes. We present the construction of $\\mathcal{O}(\\sqrt{d})$ many $\\beta$-AMUBs for composite $d$ when $|s-k|< \\sqrt{d}$, using RBDs having block sizes approximately $\\sqrt{d}$, such that $|\\braket{\\psi^l_i|\\psi^m_j}| \\leq \\frac{\\beta}{\\sqrt{d}}$ where $\\beta = 1 + \\frac{|s-k|}{2\\sqrt{d}}+ \\mathcal{O}(d^{-1}) \\leq 2$. Moreover, if real Hadamard matrix of order $k$ or $s$ exists, then one can construct at least $N(k)+1$ (or $N(s)+1$) many $\\beta$-ARMUBs for dimension $d$, with $\\beta \\leq 2 - \\frac{|s-k|}{2\\sqrt{d}}+ \\mathcal{O}(d^{-1})< 2$, where $N(w)$ is the number of MOLS$(w)$. This improves and generalizes some of our previous results for ARMUBs from two points, viz., the real cases are now extended to complex ones too. The earlier efforts use some existing RBDs, whereas here we consider new instances of RBDs that provide better results. Similar to the earlier cases, the AMUBs (ARMUBs) constructed using RBDs are in general very sparse, where the sparsity $(\\epsilon)$ is $1 - \\mathcal{O}(d^{-\\frac{1}{2}})$.","sentences":["Construction of a large class of Mutually Unbiased Bases (MUBs) for non-prime power composite dimensions ($d = k\\times s$) is a long standing open problem, which leads to different construction methods for the class Approximate MUBs (AMUBs) by relaxing the criterion that the absolute value of the dot product between two vectors chosen from different bases should be $\\leq \\frac{\\beta}{\\sqrt{d}}$.","In this chapter, we consider a more general class of AMUBs (ARMUBs, considering the real ones too), compared to our earlier work in [Cryptography and Communications, 14(3): 527--549, 2022].","We note that the quality of AMUBs (ARMUBs) constructed using RBD$(X,A)$ with $|X|= d$, critically depends on the parameters, $|s-k|$, $\\mu$ (maximum number of elements common between any pair of blocks), and the set of block sizes.","We present the construction of $\\mathcal{O}(\\sqrt{d})$ many $\\beta$-AMUBs for composite $d$ when $|s-k|< \\sqrt{d}$, using RBDs having block sizes approximately $\\sqrt{d}$, such that $|\\braket{\\psi^l_i|\\psi^m_j}| \\leq \\frac{\\beta}{\\sqrt{d}}$ where $\\beta = 1 + \\frac{|s-k|}{2\\sqrt{d}}+ \\mathcal{O}(d^{-1})","\\leq 2$.","Moreover, if real Hadamard matrix of order $k$ or $s$ exists, then one can construct at least $N(k)+1$ (or $N(s)+1$) many $\\beta$-ARMUBs for dimension $d$, with $\\beta \\leq 2 - \\frac{|s-k|}{2\\sqrt{d}}+ \\mathcal{O}(d^{-1})< 2$, where $N(w)$ is the number of MOLS$(w)$. This improves and generalizes some of our previous results for ARMUBs from two points, viz., the real cases are now extended to complex ones too.","The earlier efforts use some existing RBDs, whereas here we consider new instances of RBDs that provide better results.","Similar to the earlier cases, the AMUBs (ARMUBs) constructed using RBDs are in general very sparse, where the sparsity $(\\epsilon)$ is $1 - \\mathcal{O}(d^{-\\frac{1}{2}})$."],"url":"http://arxiv.org/abs/2402.04231v1"}
{"created":"2024-02-06 18:36:52","title":"MusicRL: Aligning Music Generation to Human Preferences","abstract":"We propose MusicRL, the first music generation system finetuned from human feedback. Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat). Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning. MusicRL is a pretrained autoregressive MusicLM (Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards. We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences. Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale. Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline. Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters. Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it. This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models.","sentences":["We propose MusicRL, the first music generation system finetuned from human feedback.","Appreciation of text-to-music models is particularly subjective since the concept of musicality as well as the specific intention behind a caption are user-dependent (e.g. a caption such as \"upbeat work-out music\" can map to a retro guitar solo or a techno pop beat).","Not only this makes supervised training of such models challenging, but it also calls for integrating continuous human feedback in their post-deployment finetuning.","MusicRL is a pretrained autoregressive MusicLM","(Agostinelli et al., 2023) model of discrete audio tokens finetuned with reinforcement learning to maximise sequence-level rewards.","We design reward functions related specifically to text-adherence and audio quality with the help from selected raters, and use those to finetune MusicLM into MusicRL-R. We deploy MusicLM to users and collect a substantial dataset comprising 300,000 pairwise preferences.","Using Reinforcement Learning from Human Feedback (RLHF), we train MusicRL-U, the first text-to-music model that incorporates human feedback at scale.","Human evaluations show that both MusicRL-R and MusicRL-U are preferred to the baseline.","Ultimately, MusicRL-RU combines the two approaches and results in the best model according to human raters.","Ablation studies shed light on the musical attributes influencing human preferences, indicating that text adherence and quality only account for a part of it.","This underscores the prevalence of subjectivity in musical appreciation and calls for further involvement of human listeners in the finetuning of music generation models."],"url":"http://arxiv.org/abs/2402.04229v1"}
{"created":"2024-02-06 18:36:44","title":"Intelligent Collective Escape of Swarm Robots Based on a Novel Fish-inspired Self-adaptive Approach with Neurodynamic Models","abstract":"Fish schools present high-efficiency group behaviors through simple individual interactions to collective migration and dynamic escape from the predator. The school behavior of fish is usually a good inspiration to design control architecture for swarm robots. In this paper, a novel fish-inspired self-adaptive approach is proposed for collective escape for the swarm robots. In addition, a bio-inspired neural network (BINN) is introduced to generate collision-free escape robot trajectories through the combination of attractive and repulsive forces. Furthermore, to cope with dynamic environments, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in the changing environment. Similar to fish escape maneuvers, simulation and experimental results show that the swarm robots are capable of collectively leaving away from the threats. Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness and efficiency of system performance, and the flexibility and robustness in complex environments.","sentences":["Fish schools present high-efficiency group behaviors through simple individual interactions to collective migration and dynamic escape from the predator.","The school behavior of fish is usually a good inspiration to design control architecture for swarm robots.","In this paper, a novel fish-inspired self-adaptive approach is proposed for collective escape for the swarm robots.","In addition, a bio-inspired neural network (BINN) is introduced to generate collision-free escape robot trajectories through the combination of attractive and repulsive forces.","Furthermore, to cope with dynamic environments, a neurodynamics-based self-adaptive mechanism is proposed to improve the self-adaptive performance of the swarm robots in the changing environment.","Similar to fish escape maneuvers, simulation and experimental results show that the swarm robots are capable of collectively leaving away from the threats.","Several comparison studies demonstrated that the proposed approach can significantly improve the effectiveness and efficiency of system performance, and the flexibility and robustness in complex environments."],"url":"http://arxiv.org/abs/2402.04228v1"}
{"created":"2024-02-06 18:29:39","title":"What is 'Typological Diversity' in NLP?","abstract":"The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP. However, these improvements only apply to a small subset of the world's languages. Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages. To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages. These selections are often described as being 'typologically diverse'. In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'. We find there are no set definitions or criteria for such claims. We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers. Furthermore, we show that skewed language selection can lead to overestimated multilingual performance. We recommend future work to include an operationalization of 'typological diversity' that empirically justifies the diversity of language samples.","sentences":["The NLP research community has devoted increased attention to languages beyond English, resulting in considerable improvements for multilingual NLP.","However, these improvements only apply to a small subset of the world's languages.","Aiming to extend this, an increasing number of papers aspires to enhance generalizable multilingual performance across languages.","To this end, linguistic typology is commonly used to motivate language selection, on the basis that a broad typological sample ought to imply generalization across a broad range of languages.","These selections are often described as being 'typologically diverse'.","In this work, we systematically investigate NLP research that includes claims regarding 'typological diversity'.","We find there are no set definitions or criteria for such claims.","We introduce metrics to approximate the diversity of language selection along several axes and find that the results vary considerably across papers.","Furthermore, we show that skewed language selection can lead to overestimated multilingual performance.","We recommend future work to include an operationalization of 'typological diversity' that empirically justifies the diversity of language samples."],"url":"http://arxiv.org/abs/2402.04222v1"}
{"created":"2024-02-06 18:17:02","title":"Resource-Aware Hierarchical Federated Learning in Wireless Video Caching Networks","abstract":"Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks. Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs). As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands. Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests. A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content. Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive the convergence bound of the proposed algorithm. Based on this bound, we minimize a weighted utility function for jointly configuring the controllable parameters to train the RawHFL energy efficiently under practical resource constraints. Our extensive simulation results validate the proposed algorithm's superiority, in terms of test accuracy and energy cost, over existing baselines.","sentences":["Backhaul traffic congestion caused by the video traffic of a few popular files can be alleviated by storing the to-be-requested content at various levels in wireless video caching networks.","Typically, content service providers (CSPs) own the content, and the users request their preferred content from the CSPs using their (wireless) internet service providers (ISPs).","As these parties do not reveal their private information and business secrets, traditional techniques may not be readily used to predict the dynamic changes in users' future demands.","Motivated by this, we propose a novel resource-aware hierarchical federated learning (RawHFL) solution for predicting user's future content requests.","A practical data acquisition technique is used that allows the user to update its local training dataset based on its requested content.","Besides, since networking and other computational resources are limited, considering that only a subset of the users participate in the model training, we derive the convergence bound of the proposed algorithm.","Based on this bound, we minimize a weighted utility function for jointly configuring the controllable parameters to train the RawHFL energy efficiently under practical resource constraints.","Our extensive simulation results validate the proposed algorithm's superiority, in terms of test accuracy and energy cost, over existing baselines."],"url":"http://arxiv.org/abs/2402.04216v1"}
{"created":"2024-02-06 18:09:05","title":"Variational Shapley Network: A Probabilistic Approach to Self-Explaining Shapley values with Uncertainty Quantification","abstract":"Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes. Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations. We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass. Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations. Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a novel masked neural network architecture. Evaluations on simulated and real datasets underscore our technique's robust predictive and explanatory performance.","sentences":["Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes.","Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations.","We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass.","Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations.","Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a novel masked neural network architecture.","Evaluations on simulated and real datasets underscore our technique's robust predictive and explanatory performance."],"url":"http://arxiv.org/abs/2402.04211v1"}
{"created":"2024-02-06 18:07:43","title":"\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors","abstract":"Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences. Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback. In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met. Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations). However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving. This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos? To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies. Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes. Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement. The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic.","sentences":["Large-scale generative models are shown to be useful for sampling meaningful candidate solutions, yet they often overlook task constraints and user preferences.","Their full power is better harnessed when the models are coupled with external verifiers and the final solutions are derived iteratively or progressively according to the verification feedback.","In the context of embodied AI, verification often solely involves assessing whether goal conditions specified in the instructions have been met.","Nonetheless, for these agents to be seamlessly integrated into daily life, it is crucial to account for a broader range of constraints and preferences beyond bare task success (e.g., a robot should grasp bread with care to avoid significant deformations).","However, given the unbounded scope of robot tasks, it is infeasible to construct scripted verifiers akin to those used for explicit-knowledge tasks like the game of Go and theorem proving.","This begs the question: when no sound verifier is available, can we use large vision and language models (VLMs), which are approximately omniscient, as scalable Behavior Critics to catch undesirable robot behaviors in videos?","To answer this, we first construct a benchmark that contains diverse cases of goal-reaching yet undesirable robot policies.","Then, we comprehensively evaluate VLM critics to gain a deeper understanding of their strengths and failure modes.","Based on the evaluation, we provide guidelines on how to effectively utilize VLM critiques and showcase a practical way to integrate the feedback into an iterative process of policy refinement.","The dataset and codebase are released at: https://guansuns.github.io/pages/vlm-critic."],"url":"http://arxiv.org/abs/2402.04210v1"}
{"created":"2024-02-06 18:05:30","title":"Acute kidney injury prediction for non-critical care patients: a retrospective external and internal validation study","abstract":"Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions. Progression of AKI may lead to irreversible kidney damage. Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202). We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours. We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model). We internally and externally validated the models on each site and performed subgroup analyses across sex and race. Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively. Area under the receiver operating curve values (AUROC) for the UFH test cohort ranged between 0.77 (UPMC Model) and 0.81 (UFH Model), while AUROC values ranged between 0.79 (UFH Model) and 0.83 (UPMC Model) for the UPMC test cohort. UFH-UPMC Model achieved an AUROC of 0.81 (95% confidence interval [CI] [0.80, 0.83]) for UFH and 0.82 (95% CI [0.81,0.84]) for UPMC test cohorts; an area under the precision recall curve values (AUPRC) of 0.6 (95% CI, [0.05, 0.06]) for UFH and 0.13 (95% CI, [0.11,0.15]) for UPMC test cohorts. Kinetic estimated glomerular filtration rate, nephrotoxic drug burden and blood urea nitrogen remained the top three features with the highest influence across the models and health centers. Conclusion: Locally developed models displayed marginally reduced discrimination when tested on another institution, while the top set of influencing features remained the same across the models and sites.","sentences":["Background: Acute kidney injury (AKI), the decline of kidney excretory function, occurs in up to 18% of hospitalized admissions.","Progression of AKI may lead to irreversible kidney damage.","Methods: This retrospective cohort study includes adult patients admitted to a non-intensive care unit at the University of Pittsburgh Medical Center (UPMC) (n = 46,815) and University of Florida Health (UFH) (n = 127,202).","We developed and compared deep learning and conventional machine learning models to predict progression to Stage 2 or higher AKI within the next 48 hours.","We trained local models for each site (UFH Model trained on UFH, UPMC Model trained on UPMC) and a separate model with a development cohort of patients from both sites (UFH-UPMC Model).","We internally and externally validated the models on each site and performed subgroup analyses across sex and race.","Results: Stage 2 or higher AKI occurred in 3% (n=3,257) and 8% (n=2,296) of UFH and UPMC patients, respectively.","Area under the receiver operating curve values (AUROC) for the UFH test cohort ranged between 0.77 (UPMC Model) and 0.81 (UFH Model), while AUROC values ranged between 0.79 (UFH Model) and 0.83 (UPMC Model) for the UPMC test cohort.","UFH-UPMC Model achieved an AUROC of 0.81 (95% confidence interval","[CI]","[0.80, 0.83]) for UFH and 0.82 (95% CI","[0.81,0.84]) for UPMC test cohorts; an area under the precision recall curve values (AUPRC) of 0.6 (95% CI, [0.05, 0.06]) for UFH and 0.13 (95% CI, [0.11,0.15]) for UPMC test cohorts.","Kinetic estimated glomerular filtration rate, nephrotoxic drug burden and blood urea nitrogen remained the top three features with the highest influence across the models and health centers.","Conclusion: Locally developed models displayed marginally reduced discrimination when tested on another institution, while the top set of influencing features remained the same across the models and sites."],"url":"http://arxiv.org/abs/2402.04209v1"}
{"created":"2024-02-06 18:02:19","title":"Production-inventory games and pmas games: characterizations of the Owen point","abstract":"Production-inventory games were introduced in Guardiola et al. (2007) as a new class of totally balanced combinatorial optimization games. From among all core-allocations, the Owen point was proposed as a specifically appealing solution. In this paper we study some relationships of the class of production-inventory games and other classes of new and known games. In addition, we propose three axiomatic characterizations of the Owen point. We use eight axioms for these characterizations, among those, inessentiality and additivity of players' demands are used for the first time in this paper.","sentences":["Production-inventory games were introduced in Guardiola et al.","(2007) as a new class of totally balanced combinatorial optimization games.","From among all core-allocations, the Owen point was proposed as a specifically appealing solution.","In this paper we study some relationships of the class of production-inventory games and other classes of new and known games.","In addition, we propose three axiomatic characterizations of the Owen point.","We use eight axioms for these characterizations, among those, inessentiality and additivity of players' demands are used for the first time in this paper."],"url":"http://arxiv.org/abs/2402.04208v1"}
{"created":"2024-02-06 17:59:46","title":"Human-Like Geometric Abstraction in Large Pre-trained Neural Networks","abstract":"Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry. Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations. However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data. In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations. We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing.","sentences":["Humans possess a remarkable capacity to recognize and manipulate abstract structure, which is especially apparent in the domain of geometry.","Recent research in cognitive science suggests neural networks do not share this capacity, concluding that human geometric abilities come from discrete symbolic structure in human mental representations.","However, progress in artificial intelligence (AI) suggests that neural networks begin to demonstrate more human-like reasoning after scaling up standard architectures in both model size and amount of training data.","In this study, we revisit empirical results in cognitive science on geometric visual processing and identify three key biases in geometric visual processing: a sensitivity towards complexity, regularity, and the perception of parts and relations.","We test tasks from the literature that probe these biases in humans and find that large pre-trained neural network models used in AI demonstrate more human-like abstract geometric processing."],"url":"http://arxiv.org/abs/2402.04203v1"}
{"created":"2024-02-06 17:55:42","title":"Information Systems and Software Engineering: The Case for Convergence","abstract":"The Information Systems (IS) and Software Engineering (SE) fields share a remarkable number of similarities in their historical evolution to date. These similarities are briefly outlined below. An analysis of 10 years (2001-2010) of publications in the primary journals in both fields also reveals a good deal of overlap in research topics. Given the challenges faced by both as young disciplines, there is potentially much to gain from a closer interaction between both fields than has traditionally been the case. This article seeks to encourage such interaction, and illustrates how this might usefully occur in the area of design. It concludes by proposing a number of practical initiatives that could stimulate and facilitate interaction between the IS and SE fields","sentences":["The Information Systems (IS) and Software Engineering (SE) fields share a remarkable number of similarities in their historical evolution to date.","These similarities are briefly outlined below.","An analysis of 10 years (2001-2010) of publications in the primary journals in both fields also reveals a good deal of overlap in research topics.","Given the challenges faced by both as young disciplines, there is potentially much to gain from a closer interaction between both fields than has traditionally been the case.","This article seeks to encourage such interaction, and illustrates how this might usefully occur in the area of design.","It concludes by proposing a number of practical initiatives that could stimulate and facilitate interaction between the IS and SE fields"],"url":"http://arxiv.org/abs/2402.04200v1"}
{"created":"2024-02-06 17:50:30","title":"Instance by Instance: An Iterative Framework for Multi-instance 3D Registration","abstract":"Multi-instance registration is a challenging problem in computer vision and robotics, where multiple instances of an object need to be registered in a standard coordinate system. In this work, we propose the first iterative framework called instance-by-instance (IBI) for multi-instance 3D registration (MI-3DReg). It successively registers all instances in a given scenario, starting from the easiest and progressing to more challenging ones. Throughout the iterative process, outliers are eliminated continuously, leading to an increasing inlier rate for the remaining and more challenging instances. Under the IBI framework, we further propose a sparse-to-dense-correspondence-based multi-instance registration method (IBI-S2DC) to achieve robust MI-3DReg. Experiments on the synthetic and real datasets have demonstrated the effectiveness of IBI and suggested the new state-of-the-art performance of IBI-S2DC, e.g., our MHF1 is 12.02%/12.35% higher than the existing state-of-the-art method ECC on the synthetic/real datasets.","sentences":["Multi-instance registration is a challenging problem in computer vision and robotics, where multiple instances of an object need to be registered in a standard coordinate system.","In this work, we propose the first iterative framework called instance-by-instance (IBI) for multi-instance 3D registration (MI-3DReg).","It successively registers all instances in a given scenario, starting from the easiest and progressing to more challenging ones.","Throughout the iterative process, outliers are eliminated continuously, leading to an increasing inlier rate for the remaining and more challenging instances.","Under the IBI framework, we further propose a sparse-to-dense-correspondence-based multi-instance registration method (IBI-S2DC) to achieve robust MI-3DReg.","Experiments on the synthetic and real datasets have demonstrated the effectiveness of IBI and suggested the new state-of-the-art performance of IBI-S2DC, e.g., our MHF1 is 12.02%/12.35% higher than the existing state-of-the-art method ECC on the synthetic/real datasets."],"url":"http://arxiv.org/abs/2402.04195v1"}
{"created":"2024-02-06 17:49:02","title":"Gradient Coding in Decentralized Learning for Evading Stragglers","abstract":"In this paper, we consider a decentralized learning problem in the presence of stragglers. Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios. To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO). In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner. We analyze the convergence performance of GOCO for strongly convex loss functions. And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods.","sentences":["In this paper, we consider a decentralized learning problem in the presence of stragglers.","Although gradient coding techniques have been developed for distributed learning to evade stragglers, where the devices send encoded gradients with redundant training data, it is difficult to apply those techniques directly to decentralized learning scenarios.","To deal with this problem, we propose a new gossip-based decentralized learning method with gradient coding (GOCO).","In the proposed method, to avoid the negative impact of stragglers, the parameter vectors are updated locally using encoded gradients based on the framework of stochastic gradient coding and then averaged in a gossip-based manner.","We analyze the convergence performance of GOCO for strongly convex loss functions.","And we also provide simulation results to demonstrate the superiority of the proposed method in terms of learning performance compared with the baseline methods."],"url":"http://arxiv.org/abs/2402.04193v1"}
{"created":"2024-02-06 17:46:41","title":"Start Stop Bit Method for Efficient Data Communication in 6G Mobile Radio Systems","abstract":"In this article, a novel approach for mobile radio communications is proposed and analysed, which is promising for future 6G cooperative distributed MIMO systems. The fundamental idea is a new mechanism namely start stop bit method, which transmits bit sequences as the start/stop bits of a synchronized counter instead of transmitting the full encoded bit sequence itself. In that way, theoretically, we can transmit infinitely long data messages with only one bit for starting and one bit for stopping the counter. The value of the counter, as identified by the stop bit, is then used to reconstruct and remap the one and unique transmitted bit sequence. The start stop bit method is characterized by a high signal sparsity as only two bits are transmitted, independently of the bit sequence length for the message. Among the benefits of the start stop bit method are energy efficient data transmission, and effective distributed MIMO systems, which exploit the sparse inter cooperation area interference as well as the low processing complexity for the sparse precoder calculation. Moreover, for the next mobile wireless generation, we propose an advanced scheme of the start stop bit method which enhances its resource usage. We call the resulting method a sparse dMIMO system.","sentences":["In this article, a novel approach for mobile radio communications is proposed and analysed, which is promising for future 6G cooperative distributed MIMO systems.","The fundamental idea is a new mechanism namely start stop bit method, which transmits bit sequences as the start/stop bits of a synchronized counter instead of transmitting the full encoded bit sequence itself.","In that way, theoretically, we can transmit infinitely long data messages with only one bit for starting and one bit for stopping the counter.","The value of the counter, as identified by the stop bit, is then used to reconstruct and remap the one and unique transmitted bit sequence.","The start stop bit method is characterized by a high signal sparsity as only two bits are transmitted, independently of the bit sequence length for the message.","Among the benefits of the start stop bit method are energy efficient data transmission, and effective distributed MIMO systems, which exploit the sparse inter cooperation area interference as well as the low processing complexity for the sparse precoder calculation.","Moreover, for the next mobile wireless generation, we propose an advanced scheme of the start stop bit method which enhances its resource usage.","We call the resulting method a sparse dMIMO system."],"url":"http://arxiv.org/abs/2402.04187v1"}
{"created":"2024-02-06 17:43:27","title":"Incivility in Open Source Projects: A Comprehensive Annotated Dataset of Locked GitHub Issue Threads","abstract":"In the dynamic landscape of open source software (OSS) development, understanding and addressing incivility within issue discussions is crucial for fostering healthy and productive collaborations. This paper presents a curated dataset of 404 locked GitHub issue discussion threads and 5961 individual comments, collected from 213 OSS projects. We annotated the comments with various categories of incivility using Tone Bearing Discussion Features (TBDFs), and, for each issue thread, we annotated the triggers, targets, and consequences of incivility. We observed that Bitter frustration, Impatience, and Mocking are the most prevalent TBDFs exhibited in our dataset. The most common triggers, targets, and consequences of incivility include Failed use of tool/code or error messages, People, and Discontinued further discussion, respectively. This dataset can serve as a valuable resource for analyzing incivility in OSS and improving automated tools to detect and mitigate such behavior.","sentences":["In the dynamic landscape of open source software (OSS) development, understanding and addressing incivility within issue discussions is crucial for fostering healthy and productive collaborations.","This paper presents a curated dataset of 404 locked GitHub issue discussion threads and 5961 individual comments, collected from 213 OSS projects.","We annotated the comments with various categories of incivility using Tone Bearing Discussion Features (TBDFs), and, for each issue thread, we annotated the triggers, targets, and consequences of incivility.","We observed that Bitter frustration, Impatience, and Mocking are the most prevalent TBDFs exhibited in our dataset.","The most common triggers, targets, and consequences of incivility include Failed use of tool/code or error messages, People, and Discontinued further discussion, respectively.","This dataset can serve as a valuable resource for analyzing incivility in OSS and improving automated tools to detect and mitigate such behavior."],"url":"http://arxiv.org/abs/2402.04183v1"}
{"created":"2024-02-06 17:42:39","title":"Reinforcement Learning with Ensemble Model Predictive Safety Certification","abstract":"Reinforcement learning algorithms need exploration to learn. However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment. In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning. Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller. Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods.","sentences":["Reinforcement learning algorithms need exploration to learn.","However, unsupervised exploration prevents the deployment of such algorithms on safety-critical tasks and limits real-world deployment.","In this paper, we propose a new algorithm called Ensemble Model Predictive Safety Certification that combines model-based deep reinforcement learning with tube-based model predictive control to correct the actions taken by a learning agent, keeping safety constraint violations at a minimum through planning.","Our approach aims to reduce the amount of prior knowledge about the actual system by requiring only offline data generated by a safe controller.","Our results show that we can achieve significantly fewer constraint violations than comparable reinforcement learning methods."],"url":"http://arxiv.org/abs/2402.04182v1"}
{"created":"2024-02-06 17:38:41","title":"Deep-Learning Estimation of Weight Distribution Using Joint Kinematics for Lower-Limb Exoskeleton Control","abstract":"In the control of lower-limb exoskeletons with feet, the phase in the gait cycle can be identified by monitoring the weight distribution at the feet. This phase information can be used in the exoskeleton's controller to compensate the dynamics of the exoskeleton and to assign impedance parameters. Typically the weight distribution is calculated using data from sensors such as treadmill force plates or insole force sensors. However, these solutions increase both the setup complexity and cost. For this reason, we propose a deep-learning approach that uses a short time window of joint kinematics to predict the weight distribution of an exoskeleton in real time. The model was trained on treadmill walking data from six users wearing a four-degree-of-freedom exoskeleton and tested in real time on three different users wearing the same device. This test set includes two users not present in the training set to demonstrate the model's ability to generalize across individuals. Results show that the proposed method is able to fit the actual weight distribution with R2=0.9 and is suitable for real-time control with prediction times less than 1 ms. Experiments in closed-loop exoskeleton control show that deep-learning-based weight distribution estimation can be used to replace force sensors in overground and treadmill walking.","sentences":["In the control of lower-limb exoskeletons with feet, the phase in the gait cycle can be identified by monitoring the weight distribution at the feet.","This phase information can be used in the exoskeleton's controller to compensate the dynamics of the exoskeleton and to assign impedance parameters.","Typically the weight distribution is calculated using data from sensors such as treadmill force plates or insole force sensors.","However, these solutions increase both the setup complexity and cost.","For this reason, we propose a deep-learning approach that uses a short time window of joint kinematics to predict the weight distribution of an exoskeleton in real time.","The model was trained on treadmill walking data from six users wearing a four-degree-of-freedom exoskeleton and tested in real time on three different users wearing the same device.","This test set includes two users not present in the training set to demonstrate the model's ability to generalize across individuals.","Results show that the proposed method is able to fit the actual weight distribution with R2=0.9 and is suitable for real-time control with prediction times less than 1 ms.","Experiments in closed-loop exoskeleton control show that deep-learning-based weight distribution estimation can be used to replace force sensors in overground and treadmill walking."],"url":"http://arxiv.org/abs/2402.04180v1"}
{"created":"2024-02-06 17:31:36","title":"SHIELD : An Evaluation Benchmark for Face Spoofing and Forgery Detection with Multimodal Large Language Models","abstract":"Multimodal large language models (MLLMs) have demonstrated remarkable problem-solving capabilities in various vision fields (e.g., generic object recognition and grounding) based on strong visual semantic representation and language reasoning ability. However, whether MLLMs are sensitive to subtle visual spoof/forged clues and how they perform in the domain of face attack detection (e.g., face spoofing and forgery detection) is still unexplored. In this paper, we introduce a new benchmark, namely SHIELD, to evaluate the ability of MLLMs on face spoofing and forgery detection. Specifically, we design true/false and multiple-choice questions to evaluate multimodal face data in these two face security tasks. For the face anti-spoofing task, we evaluate three different modalities (i.e., RGB, infrared, depth) under four types of presentation attacks (i.e., print attack, replay attack, rigid mask, paper mask). For the face forgery detection task, we evaluate GAN-based and diffusion-based data with both visual and acoustic modalities. Each question is subjected to both zero-shot and few-shot tests under standard and chain of thought (COT) settings. The results indicate that MLLMs hold substantial potential in the face security domain, offering advantages over traditional specific models in terms of interpretability, multimodal flexible reasoning, and joint face spoof and forgery detection. Additionally, we develop a novel Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images, which provides rich task-related knowledge for subtle spoof/forged clue mining. Extensive experiments in separate face anti-spoofing, separate face forgery detection, and joint detection tasks demonstrate the effectiveness of the proposed MA-COT. The project is available at https$:$//github.com/laiyingxin2/SHIELD","sentences":["Multimodal large language models (MLLMs) have demonstrated remarkable problem-solving capabilities in various vision fields (e.g., generic object recognition and grounding) based on strong visual semantic representation and language reasoning ability.","However, whether MLLMs are sensitive to subtle visual spoof/forged clues and how they perform in the domain of face attack detection (e.g., face spoofing and forgery detection) is still unexplored.","In this paper, we introduce a new benchmark, namely SHIELD, to evaluate the ability of MLLMs on face spoofing and forgery detection.","Specifically, we design true/false and multiple-choice questions to evaluate multimodal face data in these two face security tasks.","For the face anti-spoofing task, we evaluate three different modalities (i.e., RGB, infrared, depth) under four types of presentation attacks (i.e., print attack, replay attack, rigid mask, paper mask).","For the face forgery detection task, we evaluate GAN-based and diffusion-based data with both visual and acoustic modalities.","Each question is subjected to both zero-shot and few-shot tests under standard and chain of thought (COT) settings.","The results indicate that MLLMs hold substantial potential in the face security domain, offering advantages over traditional specific models in terms of interpretability, multimodal flexible reasoning, and joint face spoof and forgery detection.","Additionally, we develop a novel Multi-Attribute Chain of Thought (MA-COT) paradigm for describing and judging various task-specific and task-irrelevant attributes of face images, which provides rich task-related knowledge for subtle spoof/forged clue mining.","Extensive experiments in separate face anti-spoofing, separate face forgery detection, and joint detection tasks demonstrate the effectiveness of the proposed MA-COT.","The project is available at https$:$//github.com/laiyingxin2/SHIELD"],"url":"http://arxiv.org/abs/2402.04178v1"}
{"created":"2024-02-06 17:31:20","title":"Scaling Laws for Downstream Task Performance of Large Language Models","abstract":"Scaling laws provide important insights that can guide the design of large language models (LLMs). Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss. However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance. In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks. Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score. Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior. With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with more pretraining data. In such cases, we show that it is possible to predict the downstream BLEU score with good accuracy using a log-law. However, there are also cases where moderate misalignment causes the BLEU score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves. By analyzing these observations, we provide new practical insights for choosing appropriate pretraining data.","sentences":["Scaling laws provide important insights that can guide the design of large language models (LLMs).","Existing work has primarily focused on studying scaling laws for pretraining (upstream) loss.","However, in transfer learning settings, in which LLMs are pretrained on an unsupervised dataset and then finetuned on a downstream task, we often also care about the downstream performance.","In this work, we study the scaling behavior in a transfer learning setting, where LLMs are finetuned for machine translation tasks.","Specifically, we investigate how the choice of the pretraining data and its size affect downstream performance (translation quality) as judged by two metrics: downstream cross-entropy and BLEU score.","Our experiments indicate that the size of the finetuning dataset and the distribution alignment between the pretraining and downstream data significantly influence the scaling behavior.","With sufficient alignment, both downstream cross-entropy and BLEU score improve monotonically with more pretraining data.","In such cases, we show that it is possible to predict the downstream BLEU score with good accuracy using a log-law.","However, there are also cases where moderate misalignment causes the BLEU score to fluctuate or get worse with more pretraining, whereas downstream cross-entropy monotonically improves.","By analyzing these observations, we provide new practical insights for choosing appropriate pretraining data."],"url":"http://arxiv.org/abs/2402.04177v1"}
{"created":"2024-02-06 17:27:12","title":"COPS: A Compact On-device Pipeline for real-time Smishing detection","abstract":"Smartphones have become indispensable in our daily lives and can do almost everything, from communication to online shopping. However, with the increased usage, cybercrime aimed at mobile devices is rocketing. Smishing attacks, in particular, have observed a significant upsurge in recent years. This problem is further exacerbated by the perpetrator creating new deceptive websites daily, with an average life cycle of under 15 hours. This renders the standard practice of keeping a database of malicious URLs ineffective. To this end, we propose a novel on-device pipeline: COPS that intelligently identifies features of fraudulent messages and URLs to alert the user in real-time. COPS is a lightweight pipeline with a detection module based on the Disentangled Variational Autoencoder of size 3.46MB for smishing and URL phishing detection, and we benchmark it on open datasets. We achieve an accuracy of 98.15% and 99.5%, respectively, for both tasks, with a false negative and false positive rate of a mere 0.037 and 0.015, outperforming previous works with the added advantage of ensuring real-time alerts on resource-constrained devices.","sentences":["Smartphones have become indispensable in our daily lives and can do almost everything, from communication to online shopping.","However, with the increased usage, cybercrime aimed at mobile devices is rocketing.","Smishing attacks, in particular, have observed a significant upsurge in recent years.","This problem is further exacerbated by the perpetrator creating new deceptive websites daily, with an average life cycle of under 15 hours.","This renders the standard practice of keeping a database of malicious URLs ineffective.","To this end, we propose a novel on-device pipeline: COPS that intelligently identifies features of fraudulent messages and URLs to alert the user in real-time.","COPS is a lightweight pipeline with a detection module based on the Disentangled Variational Autoencoder of size 3.46MB for smishing and URL phishing detection, and we benchmark it on open datasets.","We achieve an accuracy of 98.15% and 99.5%, respectively, for both tasks, with a false negative and false positive rate of a mere 0.037 and 0.015, outperforming previous works with the added advantage of ensuring real-time alerts on resource-constrained devices."],"url":"http://arxiv.org/abs/2402.04173v1"}
{"created":"2024-02-06 17:24:06","title":"Informed Reinforcement Learning for Situation-Aware Traffic Rule Exceptions","abstract":"Reinforcement Learning is a highly active research field with promising advancements. In the field of autonomous driving, however, often very simple scenarios are being examined. Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure. In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source. We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions. Our method is applicable to arbitrary RL models. We successfully demonstrate high completion rates of complex scenarios with recent model-based agents.","sentences":["Reinforcement Learning is a highly active research field with promising advancements.","In the field of autonomous driving, however, often very simple scenarios are being examined.","Common approaches use non-interpretable control commands as the action space and unstructured reward designs which lack structure.","In this work, we introduce Informed Reinforcement Learning, where a structured rulebook is integrated as a knowledge source.","We learn trajectories and asses them with a situation-aware reward design, leading to a dynamic reward which allows the agent to learn situations which require controlled traffic rule exceptions.","Our method is applicable to arbitrary RL models.","We successfully demonstrate high completion rates of complex scenarios with recent model-based agents."],"url":"http://arxiv.org/abs/2402.04168v1"}
{"created":"2024-02-06 17:22:45","title":"Mind the Gap: Securely modeling cyber risk based on security deviations from a peer group","abstract":"There are two strategic and longstanding questions about cyber risk that organizations largely have been unable to answer: What is an organization's estimated risk exposure and how does its security compare with peers? Answering both requires industry-wide data on security posture, incidents, and losses that, until recently, have been too sensitive for organizations to share. Now, privacy enhancing technologies (PETs) such as cryptographic computing can enable the secure computation of aggregate cyber risk metrics from a peer group of organizations while leaving sensitive input data undisclosed. As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group. This paper proposes a new framework for benchmarking cyber posture against peers and estimating cyber risk within specific economic sectors using the new variables emerging from secure computations. We introduce a new top-line variable called the Defense Gap Index representing the weighted security gap between an organization and its peers that can be used to forecast an organization's own security risk based on historical industry data. We apply this approach in a specific sector using data collected from 25 large firms, in partnership with an industry ISAO, to build an industry risk model and provide tools back to participants to estimate their own risk exposure and privately compare their security posture with their peers.","sentences":["There are two strategic and longstanding questions about cyber risk that organizations largely have been unable to answer: What is an organization's estimated risk exposure and how does its security compare with peers?","Answering both requires industry-wide data on security posture, incidents, and losses that, until recently, have been too sensitive for organizations to share.","Now, privacy enhancing technologies (PETs) such as cryptographic computing can enable the secure computation of aggregate cyber risk metrics from a peer group of organizations while leaving sensitive input data undisclosed.","As these new aggregate data become available, analysts need ways to integrate them into cyber risk models that can produce more reliable risk assessments and allow comparison to a peer group.","This paper proposes a new framework for benchmarking cyber posture against peers and estimating cyber risk within specific economic sectors using the new variables emerging from secure computations.","We introduce a new top-line variable called the Defense Gap Index representing the weighted security gap between an organization and its peers that can be used to forecast an organization's own security risk based on historical industry data.","We apply this approach in a specific sector using data collected from 25 large firms, in partnership with an industry ISAO, to build an industry risk model and provide tools back to participants to estimate their own risk exposure and privately compare their security posture with their peers."],"url":"http://arxiv.org/abs/2402.04166v1"}
{"created":"2024-02-06 17:21:06","title":"Tempered Calculus for ML: Application to Hyperbolic Model Embedding","abstract":"Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc. In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements. We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics. Notably, this recovers Volterra's product integral as a special case. We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative. This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the metricity, hyperbolicity, and encoding. We show how to apply it to a problem that has recently gained traction in ML: hyperbolic embeddings with a \"cheap\" and accurate encoding along the hyperbolic vs Euclidean scale. We unveil a new application for which the Poincar\\'e disk model has very appealing features, and our theory comes in handy: \\textit{model} embeddings for boosted combinations of decision trees, trained using the log-loss (trees) and logistic loss (combinations).","sentences":["Most mathematical distortions used in ML are fundamentally integral in nature: $f$-divergences, Bregman divergences, (regularized) optimal transport distances, integral probability metrics, geodesic distances, etc.","In this paper, we unveil a grounded theory and tools which can help improve these distortions to better cope with ML requirements.","We start with a generalization of Riemann integration that also encapsulates functions that are not strictly additive but are, more generally, $t$-additive, as in nonextensive statistical mechanics.","Notably, this recovers Volterra's product integral as a special case.","We then generalize the Fundamental Theorem of calculus using an extension of the (Euclidean) derivative.","This, along with a series of more specific Theorems, serves as a basis for results showing how one can specifically design, alter, or change fundamental properties of distortion measures in a simple way, with a special emphasis on geometric- and ML-related properties that are the metricity, hyperbolicity, and encoding.","We show how to apply it to a problem that has recently gained traction in ML: hyperbolic embeddings with a \"cheap\" and accurate encoding along the hyperbolic vs Euclidean scale.","We unveil a new application for which the Poincar\\'e disk model has very appealing features, and our theory comes in handy: \\textit{model} embeddings for boosted combinations of decision trees, trained using the log-loss (trees) and logistic loss (combinations)."],"url":"http://arxiv.org/abs/2402.04163v1"}
{"created":"2024-02-06 17:18:59","title":"Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains","abstract":"In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data characteristics and the transformer architecture. Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results. We further investigate these findings in the broader context of higher order Markov chains and deeper architectures, and outline open problems in this arena. Code is available at \\url{https://github.com/Bond1995/Markov}.","sentences":["In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages.","A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner.","To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains.","Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance.","In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data characteristics and the transformer architecture.","Backed by experiments, we demonstrate that our theoretical findings are in congruence with the empirical results.","We further investigate these findings in the broader context of higher order Markov chains and deeper architectures, and outline open problems in this arena.","Code is available at \\url{https://github.com/Bond1995/Markov}."],"url":"http://arxiv.org/abs/2402.04161v1"}
{"created":"2024-02-06 17:18:25","title":"Harnessing the Plug-and-Play Controller by Prompting","abstract":"Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications. Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner. However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation. Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model. This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs). The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs. The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute. To enable smooth cooperation between the PLM and the PPC, our work innovatively proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model's parameters based on the generating actions taken during the PPC control process. The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference. Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency.","sentences":["Controllable text generation is a growing field within natural language generation (NLG) that focuses on producing text that meets specific constraints in real-world applications.","Previous approaches, such as plug-and-play controllers (PPCs), aimed to steer the properties of generated text in a flexible manner.","However, these methods often compromised the integrity of the language model's decoding process, resulting in less smooth text generation.","Alternatively, other techniques utilized multiple attribute prompts to align the generated text with desired attributes, but this approach required prompt design for each attribute and was dependent on the size of the language model.","This paper introduces a novel method for flexible attribute control in text generation using pre-trained language models (PLMs).","The proposed approach aims to enhance the fluency of generated text by guiding the generation process with PPCs.","The key idea is to dynamically adjust the distribution of generated text by modifying prompts, effectively constraining the output space of the language model and influencing the desired attribute.","To enable smooth cooperation between the PLM and the PPC, our work innovatively proposes a new model fine-tuning method: Reinforcement Learning with Dynamic Adjust Feedback (RLDAF).This fine-tuning process adapts a small subset of the language model's parameters based on the generating actions taken during the PPC control process.","The resulting harmonious collaboration between the PLM and PPC leads to improved smoothness in text generation during inference.","Extensive experiments were conducted on the SST2 dataset, and the proposed method outperformed previous approaches in various evaluation metrics, including text fluency and attribute consistency."],"url":"http://arxiv.org/abs/2402.04160v1"}
{"created":"2024-02-06 17:09:25","title":"Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction","abstract":"Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.However, these works encounter challenges in extending their capabilities to new tasks.Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a \"read-to-play\" capability.Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer.Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities.","sentences":["Developing a generalist agent is a longstanding objective in artificial intelligence.","Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning.","However, these works encounter challenges in extending their capabilities to new tasks.","Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction.","However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks.","This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a \"read-to-play\" capability.","Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer.","Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities."],"url":"http://arxiv.org/abs/2402.04154v1"}
{"created":"2024-02-06 17:01:10","title":"Dynamic Realization Games in Newsvendor Inventory Centralization","abstract":"Consider a set N of n (>1) stores with single-item and single-period nondeterministic demands like in a classic newsvendor setting with holding and penalty costs only. Assume a risk-pooling single-warehouse centralized inventory ordering option. Allocation of costs in the centralized inventory ordering corresponds to modelling it as a cooperative cost game whose players are the stores. It has been shown that when holding and penalty costs are identical for all subsets of stores, the game based on optimal expected costs has a non empty core (Hartman et. al., 2000, Muller \\textit{et. al.}, 2002). In this paper we examine a related inventory centralization game based on demand realizations that has, in general, an empty core even with identical penalty and holding costs (Hartman and Dror, 2005). We propose a repeated cost allocation scheme for dynamic realization games based on allocation processes introduced by Lehrer (2002a). We prove that the cost subsequences of the dynamic realization game process, based on Lehrer's rules, converge almost surely to either a least square value or the core of the expected game. We extend the above results to more general dynamic cost games and relax the independence hypothesis of the sequence of players' demands at different stages.","sentences":["Consider a set N of n (>1) stores with single-item and single-period nondeterministic demands like in a classic newsvendor setting with holding and penalty costs only.","Assume a risk-pooling single-warehouse centralized inventory ordering option.","Allocation of costs in the centralized inventory ordering corresponds to modelling it as a cooperative cost game whose players are the stores.","It has been shown that when holding and penalty costs are identical for all subsets of stores, the game based on optimal expected costs has a non empty core (Hartman et.","al., 2000, Muller \\textit{et.","al.}, 2002).","In this paper we examine a related inventory centralization game based on demand realizations that has, in general, an empty core even with identical penalty and holding costs (Hartman and Dror, 2005).","We propose a repeated cost allocation scheme for dynamic realization games based on allocation processes introduced by Lehrer (2002a).","We prove that the cost subsequences of the dynamic realization game process, based on Lehrer's rules, converge almost surely to either a least square value or the core of the expected game.","We extend the above results to more general dynamic cost games and relax the independence hypothesis of the sequence of players' demands at different stages."],"url":"http://arxiv.org/abs/2402.04149v1"}
{"created":"2024-02-06 16:48:58","title":"Human Emotions Analysis and Recognition Using EEG Signals in Response to 360$^\\circ$ Videos","abstract":"Emotion recognition (ER) technology is an integral part for developing innovative applications such as drowsiness detection and health monitoring that plays a pivotal role in contemporary society. This study delves into ER using electroencephalography (EEG), within immersive virtual reality (VR) environments. There are four main stages in our proposed methodology including data acquisition, pre-processing, feature extraction, and emotion classification. Acknowledging the limitations of existing 2D datasets, we introduce a groundbreaking 3D VR dataset to elevate the precision of emotion elicitation. Leveraging the Interaxon Muse headband for EEG recording and Oculus Quest 2 for VR stimuli, we meticulously recorded data from 40 participants, prioritizing subjects without reported mental illnesses. Pre-processing entails rigorous cleaning, uniform truncation, and the application of a Savitzky-Golay filter to the EEG data. Feature extraction encompasses a comprehensive analysis of metrics such as power spectral density, correlation, rational and divisional asymmetry, and power spectrum. To ensure the robustness of our model, we employed a 10-fold cross-validation, revealing an average validation accuracy of 85.54\\%, with a noteworthy maximum accuracy of 90.20\\% in the best fold. Subsequently, the trained model demonstrated a commendable test accuracy of 82.03\\%, promising favorable outcomes.","sentences":["Emotion recognition (ER) technology is an integral part for developing innovative applications such as drowsiness detection and health monitoring that plays a pivotal role in contemporary society.","This study delves into ER using electroencephalography (EEG), within immersive virtual reality (VR) environments.","There are four main stages in our proposed methodology including data acquisition, pre-processing, feature extraction, and emotion classification.","Acknowledging the limitations of existing 2D datasets, we introduce a groundbreaking 3D VR dataset to elevate the precision of emotion elicitation.","Leveraging the Interaxon Muse headband for EEG recording and Oculus Quest 2 for VR stimuli, we meticulously recorded data from 40 participants, prioritizing subjects without reported mental illnesses.","Pre-processing entails rigorous cleaning, uniform truncation, and the application of a Savitzky-Golay filter to the EEG data.","Feature extraction encompasses a comprehensive analysis of metrics such as power spectral density, correlation, rational and divisional asymmetry, and power spectrum.","To ensure the robustness of our model, we employed a 10-fold cross-validation, revealing an average validation accuracy of 85.54\\%, with a noteworthy maximum accuracy of 90.20\\% in the best fold.","Subsequently, the trained model demonstrated a commendable test accuracy of 82.03\\%, promising favorable outcomes."],"url":"http://arxiv.org/abs/2402.04142v1"}
{"created":"2024-02-06 16:48:50","title":"Multi-line AI-assisted Code Authoring","abstract":"CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions to 10's of thousands of developers at Meta. In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions. This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.   First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction.   Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users. These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.   Finally, we conduct experiments on 10's of thousands of engineers to understand how multi-line suggestions impact the user experience and contrast this with single-line suggestions. Our experiments reveal that (i) multi-line suggestions account for 42% of total characters accepted (despite only accounting for 16% for displayed suggestions) (ii) multi-line suggestions almost doubled the percentage of keystrokes saved for users from 9% to 17%. Multi-line CodeCompose has been rolled out to all engineers at Meta, and less than 1% of engineers have opted out of multi-line suggestions.","sentences":["CodeCompose is an AI-assisted code authoring tool powered by large language models (LLMs) that provides inline suggestions to 10's of thousands of developers at Meta.","In this paper, we present how we scaled the product from displaying single-line suggestions to multi-line suggestions.","This evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers.   ","First, we discuss how multi-line suggestions can have a 'jarring' effect, as the LLM's suggestions constantly move around the developer's existing code, which would otherwise result in decreased productivity and satisfaction.   ","Second, multi-line suggestions take significantly longer to generate; hence we present several innovative investments we made to reduce the perceived latency for users.","These model-hosting optimizations sped up multi-line suggestion latency by 2.5x.   ","Finally, we conduct experiments on 10's of thousands of engineers to understand how multi-line suggestions impact the user experience and contrast this with single-line suggestions.","Our experiments reveal that (i) multi-line suggestions account for 42% of total characters accepted (despite only accounting for 16% for displayed suggestions) (ii) multi-line suggestions almost doubled the percentage of keystrokes saved for users from 9% to 17%.","Multi-line CodeCompose has been rolled out to all engineers at Meta, and less than 1% of engineers have opted out of multi-line suggestions."],"url":"http://arxiv.org/abs/2402.04141v1"}
{"created":"2024-02-06 16:47:34","title":"Advancing Legal Reasoning: The Integration of AI to Navigate Complexities and Biases in Global Jurisprudence with Semi-Automated Arbitration Processes (SAAPs)","abstract":"This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong. This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions. By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law. SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions. SHIRLEY analysis is aggregated and is accompanied by a comparison-oriented AI-based application called SAM (also an ALM) to identify relative deviations in SHIRLEY bias detections. Further, a CRITIC is generated within semi-autonomous arbitration process via the ALM, SARA. A novel approach is introduced in the utilization of an AI arbitrator to critically evaluate biases and qualitative-in-nature nuances identified by the aforementioned AI applications (SAM in concert with SHIRLEY), based on the Hague Rules on Business and Human Rights Arbitration. This Semi-Automated Arbitration Process (SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring a nuanced debate-resultant \"understanding\" through a hybrid system of AI and human-based collaborative analysis.","sentences":["This study consists of a novel approach toward the analysis of court judgments spanning five countries, including the United States, the United Kingdom, Rwanda, Sweden and Hong Kong.","This study also explores the intersection of the latest advancements in artificial intelligence (AI) and legal analysis, emphasizing the role of AI (specifically generative AI) in identifying human biases and facilitating automated, valid, and coherent multisided argumentation of court judgments with the goal of ensuring consistent application of laws in and across various jurisdictions.","By incorporating Advanced Language Models (ALMs) and a newly introduced human-AI collaborative framework, this paper seeks to analyze Grounded Theory-based research design with Advanced Language Models (ALMs) in the practice of law.","SHIRLEY is the name of the AI-based application (built on top of OpenAI's GPT technology), focusing on detecting logical inconsistencies and biases across various legal decisions.","SHIRLEY analysis is aggregated and is accompanied by a comparison-oriented AI-based application called SAM (also an ALM) to identify relative deviations in SHIRLEY bias detections.","Further, a CRITIC is generated within semi-autonomous arbitration process via the ALM, SARA.","A novel approach is introduced in the utilization of an AI arbitrator to critically evaluate biases and qualitative-in-nature nuances identified by the aforementioned AI applications (SAM in concert with SHIRLEY), based on the Hague Rules on Business and Human Rights Arbitration.","This Semi-Automated Arbitration Process (SAAP) aims to uphold the integrity and fairness of legal judgments by ensuring a nuanced debate-resultant \"understanding\" through a hybrid system of AI and human-based collaborative analysis."],"url":"http://arxiv.org/abs/2402.04140v1"}
{"created":"2024-02-06 16:46:28","title":"U-shaped Vision Mamba for Single Image Dehazing","abstract":"Currently, Transformer is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices. To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network. Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies. Extensive experimental results demonstrate the effectiveness of our method. Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks. The URL of the code is \\url{https://github.com/zzr-idam}.","sentences":["Currently, Transformer is the most popular architecture for image dehazing, but due to its large computational complexity, its ability to handle long-range dependency is limited on resource-constrained devices.","To tackle this challenge, we introduce the U-shaped Vision Mamba (UVM-Net), an efficient single-image dehazing network.","Inspired by the State Space Sequence Models (SSMs), a new deep sequence model known for its power to handle long sequences, we design a Bi-SSM block that integrates the local feature extraction ability of the convolutional layer with the ability of the SSM to capture long-range dependencies.","Extensive experimental results demonstrate the effectiveness of our method.","Our method provides a more highly efficient idea of long-range dependency modeling for image dehazing as well as other image restoration tasks.","The URL of the code is \\url{https://github.com/zzr-idam}."],"url":"http://arxiv.org/abs/2402.04139v1"}
{"created":"2024-02-06 16:38:11","title":"A quasi-optimal lower bound for skew polynomial multiplication","abstract":"We establish a lower bound for the complexity of multiplying two skew polynomials. The lower bound coincides with the upper bound conjectured by Caruso and Borgne in 2017, up to a log factor. We present algorithms for three special cases, indicating that the aforementioned lower bound is quasi-optimal. In fact, our lower bound is quasi-optimal in the sense of bilinear complexity. In addition, we discuss the average bilinear complexity of simultaneous multiplication of skew polynomials and the complexity of skew polynomial multiplication in the case of towers of extensions.","sentences":["We establish a lower bound for the complexity of multiplying two skew polynomials.","The lower bound coincides with the upper bound conjectured by Caruso and Borgne in 2017, up to a log factor.","We present algorithms for three special cases, indicating that the aforementioned lower bound is quasi-optimal.","In fact, our lower bound is quasi-optimal in the sense of bilinear complexity.","In addition, we discuss the average bilinear complexity of simultaneous multiplication of skew polynomials and the complexity of skew polynomial multiplication in the case of towers of extensions."],"url":"http://arxiv.org/abs/2402.04134v1"}
{"created":"2024-02-06 16:31:11","title":"OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning","abstract":"Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost. Our regularization method has demonstrated its compatibility with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and CIFAR-100 benchmarks. Our source code is available at https://github.com/jpmorganchase/ovor.","sentences":["Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones.","Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together.","In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated.","Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool.","This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper.","We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the-art (SOTA) methods equipped with a prompt pool, using much less learnable parameters and lower inference cost.","Our regularization method has demonstrated its compatibility with different prompt-based methods, boosting those previous SOTA rehearsal-free CIL methods' accuracy on the ImageNet-R and CIFAR-100 benchmarks.","Our source code is available at https://github.com/jpmorganchase/ovor."],"url":"http://arxiv.org/abs/2402.04129v1"}
{"created":"2024-02-06 16:12:36","title":"Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science","abstract":"Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering. Our pioneering analysis offers an exploration of the learning mechanism and paves the way for advancing SLM in molecular science.","sentences":["Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field.","In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM).","However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models.","To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition.","Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks.","Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by localized feature filtering.","Our pioneering analysis offers an exploration of the learning mechanism and paves the way for advancing SLM in molecular science."],"url":"http://arxiv.org/abs/2402.04119v1"}
{"created":"2024-02-06 16:05:18","title":"Vector Approximate Message Passing With Arbitrary I.I.D. Noise Priors","abstract":"Approximate message passing (AMP) algorithms are devised under the Gaussianity assumption of the measurement noise vector. In this work, we relax this assumption within the vector AMP (VAMP) framework to arbitrary independent and identically distributed (i.i.d.) noise priors. We do so by rederiving the linear minimum mean square error (LMMSE) to accommodate both the noise and signal estimations within the message passing steps of VAMP. Numerical results demonstrate how our proposed algorithm handles non-Gaussian noise models as compared to VAMP. This extension to general noise priors enables the use of AMP algorithms in a wider range of engineering applications where non-Gaussian noise models are more appropriate.","sentences":["Approximate message passing (AMP) algorithms are devised under the Gaussianity assumption of the measurement noise vector.","In this work, we relax this assumption within the vector AMP (VAMP) framework to arbitrary independent and identically distributed (i.i.d.) noise priors.","We do so by rederiving the linear minimum mean square error (LMMSE) to accommodate both the noise and signal estimations within the message passing steps of VAMP.","Numerical results demonstrate how our proposed algorithm handles non-Gaussian noise models as compared to VAMP.","This extension to general noise priors enables the use of AMP algorithms in a wider range of engineering applications where non-Gaussian noise models are more appropriate."],"url":"http://arxiv.org/abs/2402.04111v1"}
{"created":"2024-02-06 16:03:57","title":"Behind the Screen: Investigating ChatGPT's Dark Personality Traits and Conspiracy Beliefs","abstract":"ChatGPT is notorious for its intransparent behavior. This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4. Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale. The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4. For traits that have shown to be interdependent in human studies, correlations were considered. Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses. Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differences between GPT-3.5 and GPT-4. However, GPT-4 showed a pronounced tendency to believe in information withholding. This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5. Apparently, in this case an increased data exposure correlates with a greater belief in the control of information. An assignment of extreme political affiliations increased the belief in conspiracy theories. Test sequencing affected the models' responses and the observed correlations, indicating a form of contextual memory.","sentences":["ChatGPT is notorious for its intransparent behavior.","This paper tries to shed light on this, providing an in-depth analysis of the dark personality traits and conspiracy beliefs of GPT-3.5 and GPT-4.","Different psychological tests and questionnaires were employed, including the Dark Factor Test, the Mach-IV Scale, the Generic Conspiracy Belief Scale, and the Conspiracy Mentality Scale.","The responses were analyzed computing average scores, standard deviations, and significance tests to investigate differences between GPT-3.5 and GPT-4.","For traits that have shown to be interdependent in human studies, correlations were considered.","Additionally, system roles corresponding to groups that have shown distinct answering behavior in the corresponding questionnaires were applied to examine the models' ability to reflect characteristics associated with these roles in their responses.","Dark personality traits and conspiracy beliefs were not particularly pronounced in either model with little differences between GPT-3.5 and GPT-4.","However, GPT-4 showed a pronounced tendency to believe in information withholding.","This is particularly intriguing given that GPT-4 is trained on a significantly larger dataset than GPT-3.5.","Apparently, in this case an increased data exposure correlates with a greater belief in the control of information.","An assignment of extreme political affiliations increased the belief in conspiracy theories.","Test sequencing affected the models' responses and the observed correlations, indicating a form of contextual memory."],"url":"http://arxiv.org/abs/2402.04110v1"}
{"created":"2024-02-06 16:02:17","title":"Hierarchical Delay Attribution Classification using Unstructured Text in Train Management Systems","abstract":"EU directives stipulate a systematic follow-up of train delays. In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code. However, this delay attribution code is assigned manually, which is a complex task. In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated. The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration. Further, the problem is modeled as both a hierarchical and flat approach. The results indicate that a hierarchical approach performs better than a flat approach. Both approaches perform better than the random uniform classifier but perform worse than the manual classification.","sentences":["EU directives stipulate a systematic follow-up of train delays.","In Sweden, the Swedish Transport Administration registers and assigns an appropriate delay attribution code.","However, this delay attribution code is assigned manually, which is a complex task.","In this paper, a machine learning-based decision support for assigning delay attribution codes based on event descriptions is investigated.","The text is transformed using TF-IDF, and two models, Random Forest and Support Vector Machine, are evaluated against a random uniform classifier and the classification performance of the Swedish Transport Administration.","Further, the problem is modeled as both a hierarchical and flat approach.","The results indicate that a hierarchical approach performs better than a flat approach.","Both approaches perform better than the random uniform classifier but perform worse than the manual classification."],"url":"http://arxiv.org/abs/2402.04108v1"}
{"created":"2024-02-06 15:59:23","title":"Measuring Implicit Bias in Explicitly Unbiased Large Language Models","abstract":"Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases. Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make. We address both of these challenges by introducing two measures of bias inspired by psychology: LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks. Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others). Our prompt-based measure of implicit bias correlates with embedding-based methods but better predicts downstream behaviors measured by LLM Decision Bias. This measure is based on asking the LLM to decide between individuals, motivated by psychological results indicating that relative not absolute evaluations are more related to implicit biases. Using prompt-based measures informed by psychology allows us to effectively expose nuanced biases and subtle discrimination in proprietary LLMs that do not show explicit bias on standard benchmarks.","sentences":["Large language models (LLMs) can pass explicit bias tests but still harbor implicit biases, similar to humans who endorse egalitarian beliefs yet exhibit subtle biases.","Measuring such implicit biases can be a challenge: as LLMs become increasingly proprietary, it may not be possible to access their embeddings and apply existing bias measures; furthermore, implicit biases are primarily a concern if they affect the actual decisions that these systems make.","We address both of these challenges by introducing two measures of bias inspired by psychology:","LLM Implicit Association Test (IAT) Bias, which is a prompt-based method for revealing implicit bias; and LLM Decision Bias for detecting subtle discrimination in decision-making tasks.","Using these measures, we found pervasive human-like stereotype biases in 6 LLMs across 4 social domains (race, gender, religion, health) and 21 categories (weapons, guilt, science, career among others).","Our prompt-based measure of implicit bias correlates with embedding-based methods but better predicts downstream behaviors measured by LLM Decision Bias.","This measure is based on asking the LLM to decide between individuals, motivated by psychological results indicating that relative not absolute evaluations are more related to implicit biases.","Using prompt-based measures informed by psychology allows us to effectively expose nuanced biases and subtle discrimination in proprietary LLMs that do not show explicit bias on standard benchmarks."],"url":"http://arxiv.org/abs/2402.04105v1"}
{"created":"2024-02-06 15:58:14","title":"An Exploration of Clustering Algorithms for Customer Segmentation in the UK Retail Market","abstract":"Recently, peoples awareness of online purchases has significantly risen. This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour. Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation. Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability. In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry. To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository. The retail dataset consists of 541,909 customer records and eight features. Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values. Thereafter, we compared several state-of-the-art (SOTA) clustering algorithms, namely, K-means clustering, the Gaussian mixture model (GMM), density-based spatial clustering of applications with noise (DBSCAN), agglomerative clustering, and balanced iterative reducing and clustering using hierarchies (BIRCH). The results showed the GMM outperformed other approaches, with a Silhouette Score of 0.80.","sentences":["Recently, peoples awareness of online purchases has significantly risen.","This has given rise to online retail platforms and the need for a better understanding of customer purchasing behaviour.","Retail companies are pressed with the need to deal with a high volume of customer purchases, which requires sophisticated approaches to perform more accurate and efficient customer segmentation.","Customer segmentation is a marketing analytical tool that aids customer-centric service and thus enhances profitability.","In this paper, we aim to develop a customer segmentation model to improve decision-making processes in the retail market industry.","To achieve this, we employed a UK-based online retail dataset obtained from the UCI machine learning repository.","The retail dataset consists of 541,909 customer records and eight features.","Our study adopted the RFM (recency, frequency, and monetary) framework to quantify customer values.","Thereafter, we compared several state-of-the-art (SOTA) clustering algorithms, namely, K-means clustering, the Gaussian mixture model (GMM), density-based spatial clustering of applications with noise (DBSCAN), agglomerative clustering, and balanced iterative reducing and clustering using hierarchies (BIRCH).","The results showed the GMM outperformed other approaches, with a Silhouette Score of 0.80."],"url":"http://arxiv.org/abs/2402.04103v1"}
{"created":"2024-02-06 15:57:08","title":"Use of Multi-CNNs for Section Analysis in Static Malware Detection","abstract":"Existing research on malware detection focuses almost exclusively on the detection rate. However, in some cases, it is also important to understand the results of our algorithm, or to obtain more information, such as where to investigate in the file for an analyst. In this aim, we propose a new model to analyze Portable Executable files. Our method consists in splitting the files in different sections, then transform each section into an image, in order to train convolutional neural networks to treat specifically each identified section. Then we use all these scores returned by CNNs to compute a final detection score, using models that enable us to improve our analysis of the importance of each section in the final score.","sentences":["Existing research on malware detection focuses almost exclusively on the detection rate.","However, in some cases, it is also important to understand the results of our algorithm, or to obtain more information, such as where to investigate in the file for an analyst.","In this aim, we propose a new model to analyze Portable Executable files.","Our method consists in splitting the files in different sections, then transform each section into an image, in order to train convolutional neural networks to treat specifically each identified section.","Then we use all these scores returned by CNNs to compute a final detection score, using models that enable us to improve our analysis of the importance of each section in the final score."],"url":"http://arxiv.org/abs/2402.04102v1"}
{"created":"2024-02-06 15:55:46","title":"VRMM: A Volumetric Relightable Morphable Head Model","abstract":"In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling. While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions. Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations. This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice. The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions. We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation. Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM. Such an approach enables accurate 3D face reconstruction from even a single portrait input. Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling.","sentences":["In this paper, we introduce the Volumetric Relightable Morphable Model (VRMM), a novel volumetric and parametric facial prior for 3D face modeling.","While recent volumetric prior models offer improvements over traditional methods like 3D Morphable Models (3DMMs), they face challenges in model learning and personalized reconstructions.","Our VRMM overcomes these by employing a novel training framework that efficiently disentangles and encodes latent spaces of identity, expression, and lighting into low-dimensional representations.","This framework, designed with self-supervised learning, significantly reduces the constraints for training data, making it more feasible in practice.","The learned VRMM offers relighting capabilities and encompasses a comprehensive range of expressions.","We demonstrate the versatility and effectiveness of VRMM through various applications like avatar generation, facial reconstruction, and animation.","Additionally, we address the common issue of overfitting in generative volumetric models with a novel prior-preserving personalization framework based on VRMM.","Such an approach enables accurate 3D face reconstruction from even a single portrait input.","Our experiments showcase the potential of VRMM to significantly enhance the field of 3D face modeling."],"url":"http://arxiv.org/abs/2402.04101v1"}
{"created":"2024-02-06 15:52:23","title":"Analysis of Deep Image Prior and Exploiting Self-Guidance for Image Reconstruction","abstract":"The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI). However, conventional DIP suffers from severe overfitting and spectral bias effects.In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.This study sheds light on important underlying properties for DIP-based recovery.Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs. However, obtaining suitable reference images requires supervision, and raises practical difficulties. In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data. Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting.","sentences":["The ability of deep image prior (DIP) to recover high-quality images from incomplete or corrupted measurements has made it popular in inverse problems in image restoration and medical imaging including magnetic resonance imaging (MRI).","However, conventional DIP suffers from severe overfitting and spectral bias effects.","In this work, we first provide an analysis of how DIP recovers information from undersampled imaging measurements by analyzing the training dynamics of the underlying networks in the kernel regime for different architectures.","This study sheds light on important underlying properties for DIP-based recovery.","Current research suggests that incorporating a reference image as network input can enhance DIP's performance in image reconstruction compared to using random inputs.","However, obtaining suitable reference images requires supervision, and raises practical difficulties.","In an attempt to overcome this obstacle, we further introduce a self-driven reconstruction process that concurrently optimizes both the network weights and the input while eliminating the need for training data.","Our method incorporates a novel denoiser regularization term which enables robust and stable joint estimation of both the network input and reconstructed image.","We demonstrate that our self-guided method surpasses both the original DIP and modern supervised methods in terms of MR image reconstruction performance and outperforms previous DIP-based schemes for image inpainting."],"url":"http://arxiv.org/abs/2402.04097v1"}
{"created":"2024-02-06 15:46:48","title":"Acceleration and energy consumption optimization in cascading classifiers for face detection on low-cost ARM big.LITTLE asymmetric architectures","abstract":"This paper proposes a mechanism to accelerate and optimize the energy consumption of a face detection software based on Haar-like cascading classifiers, taking advantage of the features of low-cost Asymmetric Multicore Processors (AMPs) with limited power budget. A modelling and task scheduling/allocation is proposed in order to efficiently make use of the existing features on big.LITTLE ARM processors, including: (I) source-code adaptation for parallel computing, which enables code acceleration by applying the OmpSs programming model, a task-based programming model that handles data-dependencies between tasks in a transparent fashion; (II) different OmpSs task allocation policies which take into account the processor asymmetry and can dynamically set processing resources in a more efficient way based on their particular features. The proposed mechanism can be efficiently applied to take advantage of the processing elements existing on low-cost and low-energy multi-core embedded devices executing object detection algorithms based on cascading classifiers. Although these classifiers yield the best results for detection algorithms in the field of computer vision, their high computational requirements prevent them from being used on these devices under real-time requirements. Finally, we compare the energy efficiency of a heterogeneous architecture based on asymmetric multicore processors with a suitable task scheduling, with that of a homogeneous symmetric architecture.","sentences":["This paper proposes a mechanism to accelerate and optimize the energy consumption of a face detection software based on Haar-like cascading classifiers, taking advantage of the features of low-cost Asymmetric Multicore Processors (AMPs) with limited power budget.","A modelling and task scheduling/allocation is proposed in order to efficiently make use of the existing features on big.","LITTLE ARM processors, including: (I) source-code adaptation for parallel computing, which enables code acceleration by applying the OmpSs programming model, a task-based programming model that handles data-dependencies between tasks in a transparent fashion; (II) different OmpSs task allocation policies which take into account the processor asymmetry and can dynamically set processing resources in a more efficient way based on their particular features.","The proposed mechanism can be efficiently applied to take advantage of the processing elements existing on low-cost and low-energy multi-core embedded devices executing object detection algorithms based on cascading classifiers.","Although these classifiers yield the best results for detection algorithms in the field of computer vision, their high computational requirements prevent them from being used on these devices under real-time requirements.","Finally, we compare the energy efficiency of a heterogeneous architecture based on asymmetric multicore processors with a suitable task scheduling, with that of a homogeneous symmetric architecture."],"url":"http://arxiv.org/abs/2402.04090v1"}
{"created":"2024-02-06 15:46:31","title":"The Use of a Large Language Model for Cyberbullying Detection","abstract":"The dominance of social media has added to the channels of bullying for perpetrators. Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens. This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society. Several machine learning (ML) algorithms have been proposed for this purpose. However, their performances are not consistent due to high class imbalance and generalisation issues. In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks. Unfortunately, the LLMs have not been applied extensively for CB detection. In our paper, we explored the use of these models for cyberbullying (CB) detection. We have prepared a new dataset (D2) from existing studies (Formspring and Twitter). Our experimental results for dataset D1 and D2 showed that RoBERTa outperformed other models.","sentences":["The dominance of social media has added to the channels of bullying for perpetrators.","Unfortunately, cyberbullying (CB) is the most prevalent phenomenon in todays cyber world, and is a severe threat to the mental and physical health of citizens.","This opens the need to develop a robust system to prevent bullying content from online forums, blogs, and social media platforms to manage the impact in our society.","Several machine learning (ML) algorithms have been proposed for this purpose.","However, their performances are not consistent due to high class imbalance and generalisation issues.","In recent years, large language models (LLMs) like BERT and RoBERTa have achieved state-of-the-art (SOTA) results in several natural language processing (NLP) tasks.","Unfortunately, the LLMs have not been applied extensively for CB detection.","In our paper, we explored the use of these models for cyberbullying (CB) detection.","We have prepared a new dataset (D2) from existing studies (Formspring and Twitter).","Our experimental results for dataset D1 and D2 showed that RoBERTa outperformed other models."],"url":"http://arxiv.org/abs/2402.04088v1"}
{"created":"2024-02-06 15:45:27","title":"A Hard-to-Beat Baseline for Training-free CLIP-based Adaptation","abstract":"Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity. Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks. However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources. In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP. Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance. By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training. To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP. Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization. In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches. Our code is publicly available at \\url{https://github.com/mrflogs/ICLR24}.","sentences":["Contrastive Language-Image Pretraining (CLIP) has gained popularity for its remarkable zero-shot capacity.","Recent research has focused on developing efficient fine-tuning methods, such as prompt learning and adapter, to enhance CLIP's performance in downstream tasks.","However, these methods still require additional training time and computational resources, which is undesirable for devices with limited resources.","In this paper, we revisit a classical algorithm, Gaussian Discriminant Analysis (GDA), and apply it to the downstream classification of CLIP.","Typically, GDA assumes that features of each class follow Gaussian distributions with identical covariance.","By leveraging Bayes' formula, the classifier can be expressed in terms of the class means and covariance, which can be estimated from the data without the need for training.","To integrate knowledge from both visual and textual modalities, we ensemble it with the original zero-shot classifier within CLIP.","Extensive results on 17 datasets validate that our method surpasses or achieves comparable results with state-of-the-art methods on few-shot classification, imbalanced learning, and out-of-distribution generalization.","In addition, we extend our method to base-to-new generalization and unsupervised learning, once again demonstrating its superiority over competing approaches.","Our code is publicly available at \\url{https://github.com/mrflogs/ICLR24}."],"url":"http://arxiv.org/abs/2402.04087v1"}
{"created":"2024-02-06 15:39:09","title":"Provably learning a multi-head attention layer","abstract":"The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models. Given a sequence length $k$, attention matrices $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and projection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times d}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to \\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X}) \\triangleq \\sum^m_{i=1} \\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$. In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns $F$ to small error given random labeled examples drawn uniformly from $\\{\\pm 1\\}^{k\\times d}$.   - We prove computational lower bounds showing that in the worst case, exponential dependence on $m$ is unavoidable.   We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. Gaussian. Our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, is a significant departure from existing provable algorithms for learning feedforward networks, which predominantly exploit algebraic and rotation invariance properties of the Gaussian distribution. In contrast, our analysis is more flexible as it primarily relies on various upper and lower tail bounds for the input distribution and \"slices\" thereof.","sentences":["The multi-head attention layer is one of the key components of the transformer architecture that sets it apart from traditional feed-forward models.","Given a sequence length $k$, attention matrices $\\mathbf{\\Theta}_1,\\ldots,\\mathbf{\\Theta}_m\\in\\mathbb{R}^{d\\times d}$, and projection matrices $\\mathbf{W}_1,\\ldots,\\mathbf{W}_m\\in\\mathbb{R}^{d\\times d}$, the corresponding multi-head attention layer $F: \\mathbb{R}^{k\\times d}\\to \\mathbb{R}^{k\\times d}$ transforms length-$k$ sequences of $d$-dimensional tokens $\\mathbf{X}\\in\\mathbb{R}^{k\\times d}$ via $F(\\mathbf{X})","\\triangleq \\sum^m_{i=1} \\mathrm{softmax}(\\mathbf{X}\\mathbf{\\Theta}_i\\mathbf{X}^\\top)\\mathbf{X}\\mathbf{W}_i$.","In this work, we initiate the study of provably learning a multi-head attention layer from random examples and give the first nontrivial upper and lower bounds for this problem:   - Provided $\\{\\mathbf{W}_i, \\mathbf{\\Theta}_i\\}$ satisfy certain non-degeneracy conditions, we give a $(dk)^{O(m^3)}$-time algorithm that learns $F$ to small error given random labeled examples drawn uniformly from $\\{\\pm 1\\}^{k\\times d}$.   - We prove computational lower bounds showing that in the worst case, exponential dependence on $m$ is unavoidable.   ","We focus on Boolean $\\mathbf{X}$ to mimic the discrete nature of tokens in large language models, though our techniques naturally extend to standard continuous settings, e.g. Gaussian.","Our algorithm, which is centered around using examples to sculpt a convex body containing the unknown parameters, is a significant departure from existing provable algorithms for learning feedforward networks, which predominantly exploit algebraic and rotation invariance properties of the Gaussian distribution.","In contrast, our analysis is more flexible as it primarily relies on various upper and lower tail bounds for the input distribution and \"slices\" thereof."],"url":"http://arxiv.org/abs/2402.04084v1"}
{"created":"2024-02-06 15:38:03","title":"Cooperation and profit allocation in distribution chains","abstract":"We study the coordination of actions and the allocation of profit in supply chains under decentralized control in which a single supplier supplies several retailers with goods for replenishment of stocks. The goal of the supplier and the retailers is to maximize their individual profits. Since the outcome under decentralized control is inefficient, cooperation among firms by means of coordination of actions may improve the individual profits. Cooperation is studied by means of cooperative game theory. Among others we show that the corresponding games are balanced and we propose a stable solution concept for these games.","sentences":["We study the coordination of actions and the allocation of profit in supply chains under decentralized control in which a single supplier supplies several retailers with goods for replenishment of stocks.","The goal of the supplier and the retailers is to maximize their individual profits.","Since the outcome under decentralized control is inefficient, cooperation among firms by means of coordination of actions may improve the individual profits.","Cooperation is studied by means of cooperative game theory.","Among others we show that the corresponding games are balanced and we propose a stable solution concept for these games."],"url":"http://arxiv.org/abs/2402.04083v1"}
{"created":"2024-02-06 15:36:06","title":"An Optimal House Price Prediction Algorithm: XGBoost","abstract":"An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending. It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood. Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers. To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables. We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction. Afterwards, we identified the key factors that influence housing costs. Our results show that XGBoost is the best performing model for house price prediction.","sentences":["An accurate prediction of house prices is a fundamental requirement for various sectors including real estate and mortgage lending.","It is widely recognized that a property value is not solely determined by its physical attributes but is significantly influenced by its surrounding neighbourhood.","Meeting the diverse housing needs of individuals while balancing budget constraints is a primary concern for real estate developers.","To this end, we addressed the house price prediction problem as a regression task and thus employed various machine learning techniques capable of expressing the significance of independent variables.","We made use of the housing dataset of Ames City in Iowa, USA to compare support vector regressor, random forest regressor, XGBoost, multilayer perceptron and multiple linear regression algorithms for house price prediction.","Afterwards, we identified the key factors that influence housing costs.","Our results show that XGBoost is the best performing model for house price prediction."],"url":"http://arxiv.org/abs/2402.04082v1"}
{"created":"2024-02-06 15:34:44","title":"Improved Generalization of Weight Space Networks via Augmentations","abstract":"Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks. Unfortunately, weight space models tend to suffer from substantial overfitting. We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets. While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object. To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces. We demonstrate the effectiveness of these methods in two setups. In classification, they improve performance similarly to having up to 10 times more data. In self-supervised contrastive learning, they yield substantial 5-10% gains in downstream classification.","sentences":["Learning in deep weight spaces (DWS), where neural networks process the weights of other neural networks, is an emerging research direction, with applications to 2D and 3D neural fields (INRs, NeRFs), as well as making inferences about other types of neural networks.","Unfortunately, weight space models tend to suffer from substantial overfitting.","We empirically analyze the reasons for this overfitting and find that a key reason is the lack of diversity in DWS datasets.","While a given object can be represented by many different weight configurations, typical INR training sets fail to capture variability across INRs that represent the same object.","To address this, we explore strategies for data augmentation in weight spaces and propose a MixUp method adapted for weight spaces.","We demonstrate the effectiveness of these methods in two setups.","In classification, they improve performance similarly to having up to 10 times more data.","In self-supervised contrastive learning, they yield substantial 5-10% gains in downstream classification."],"url":"http://arxiv.org/abs/2402.04081v1"}
{"created":"2024-02-06 15:34:30","title":"Entropy-regularized Diffusion Policy with Q-Ensembles for Offline Reinforcement Learning","abstract":"This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL). At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy. We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets. To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement. By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks. Code is available at \\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}.","sentences":["This paper presents advanced techniques of training diffusion policies for offline reinforcement learning (RL).","At the core is a mean-reverting stochastic differential equation (SDE) that transfers a complex action distribution into a standard Gaussian and then samples actions conditioned on the environment state with a corresponding reverse-time SDE, like a typical diffusion policy.","We show that such an SDE has a solution that we can use to calculate the log probability of the policy, yielding an entropy regularizer that improves the exploration of offline datasets.","To mitigate the impact of inaccurate value functions from out-of-distribution data points, we further propose to learn the lower confidence bound of Q-ensembles for more robust policy improvement.","By combining the entropy-regularized diffusion policy with Q-ensembles in offline RL, our method achieves state-of-the-art performance on most tasks in D4RL benchmarks.","Code is available at \\href{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}{https://github.com/ruoqizzz/Entropy-Regularized-Diffusion-Policy-with-QEnsemble}."],"url":"http://arxiv.org/abs/2402.04080v1"}
{"created":"2024-02-06 15:25:09","title":"Iterative Prompt Refinement for Radiation Oncology Symptom Extraction Using Teacher-Student Large Language Models","abstract":"This study introduces a novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes. Mixtral, the student model, initially extracts symptoms, followed by GPT-4, the teacher model, which refines prompts based on Mixtral's performance. This iterative process involved 294 single symptom clinical notes across 12 symptoms, with up to 16 rounds of refinement per epoch. Results showed significant improvements in extracting symptoms from both single and multi-symptom notes. For 59 single symptom notes, accuracy increased from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and F1 score from 0.49 to 0.73. In 375 multi-symptom notes, accuracy rose from 0.24 to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score from 0.20 to 0.44. These results demonstrate the effectiveness of advanced prompt engineering in LLMs for radiation oncology use.","sentences":["This study introduces a novel teacher-student architecture utilizing Large Language Models (LLMs) to improve prostate cancer radiotherapy symptom extraction from clinical notes.","Mixtral, the student model, initially extracts symptoms, followed by GPT-4, the teacher model, which refines prompts based on Mixtral's performance.","This iterative process involved 294 single symptom clinical notes across 12 symptoms, with up to 16 rounds of refinement per epoch.","Results showed significant improvements in extracting symptoms from both single and multi-symptom notes.","For 59 single symptom notes, accuracy increased from 0.51 to 0.71, precision from 0.52 to 0.82, recall from 0.52 to 0.72, and F1 score from 0.49 to 0.73.","In 375 multi-symptom notes, accuracy rose from 0.24 to 0.43, precision from 0.6 to 0.76, recall from 0.24 to 0.43, and F1 score from 0.20 to 0.44.","These results demonstrate the effectiveness of advanced prompt engineering in LLMs for radiation oncology use."],"url":"http://arxiv.org/abs/2402.04075v1"}
{"created":"2024-02-06 15:17:09","title":"Spatial Assisted Human-Drone Collaborative Navigation and Interaction through Immersive Mixed Reality","abstract":"Aerial robots have the potential to play a crucial role in assisting humans with complex and dangerous tasks. Nevertheless, the future industry demands innovative solutions to streamline the interaction process between humans and drones to enable seamless collaboration and efficient co-working. In this paper, we present a novel tele-immersive framework that promotes cognitive and physical collaboration between humans and robots through Mixed Reality (MR). This framework incorporates a novel bi-directional spatial awareness and a multi-modal virtual-physical interaction approaches. The former seamlessly integrates the physical and virtual worlds, offering bidirectional egocentric and exocentric environmental representations. The latter, leveraging the proposed spatial representation, further enhances the collaboration combining a robot planning algorithm for obstacle avoidance with a variable admittance control. This allows users to issue commands based on virtual forces while maintaining compatibility with the environment map. We validate the proposed approach by performing several collaborative planning and exploration tasks involving a drone and an user equipped with a MR headset.","sentences":["Aerial robots have the potential to play a crucial role in assisting humans with complex and dangerous tasks.","Nevertheless, the future industry demands innovative solutions to streamline the interaction process between humans and drones to enable seamless collaboration and efficient co-working.","In this paper, we present a novel tele-immersive framework that promotes cognitive and physical collaboration between humans and robots through Mixed Reality (MR).","This framework incorporates a novel bi-directional spatial awareness and a multi-modal virtual-physical interaction approaches.","The former seamlessly integrates the physical and virtual worlds, offering bidirectional egocentric and exocentric environmental representations.","The latter, leveraging the proposed spatial representation, further enhances the collaboration combining a robot planning algorithm for obstacle avoidance with a variable admittance control.","This allows users to issue commands based on virtual forces while maintaining compatibility with the environment map.","We validate the proposed approach by performing several collaborative planning and exploration tasks involving a drone and an user equipped with a MR headset."],"url":"http://arxiv.org/abs/2402.04070v1"}
{"created":"2024-02-06 15:13:17","title":"Retrieve to Explain: Evidence-driven Predictions with Language Models","abstract":"Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.","sentences":["Machine learning models, particularly language models, are notoriously difficult to introspect.","Black-box models can mask both issues in model training and harmful biases.","For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively.","To address these issues, we introduce Retrieve to Explain (R2E).","R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction.","R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language.","We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes."],"url":"http://arxiv.org/abs/2402.04068v1"}
{"created":"2024-02-06 15:09:50","title":"Multi-class Road Defect Detection and Segmentation using Spatial and Channel-wise Attention for Autonomous Road Repairing","abstract":"Road pavement detection and segmentation are critical for developing autonomous road repair systems. However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes. We propose a novel end-to-end method for multi-class road defect detection and segmentation. The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions. Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned. To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes. The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods.","sentences":["Road pavement detection and segmentation are critical for developing autonomous road repair systems.","However, developing an instance segmentation method that simultaneously performs multi-class defect detection and segmentation is challenging due to the textural simplicity of road pavement image, the diversity of defect geometries, and the morphological ambiguity between classes.","We propose a novel end-to-end method for multi-class road defect detection and segmentation.","The proposed method comprises multiple spatial and channel-wise attention blocks available to learn global representations across spatial and channel-wise dimensions.","Through these attention blocks, more globally generalised representations of morphological information (spatial characteristics) of road defects and colour and depth information of images can be learned.","To demonstrate the effectiveness of our framework, we conducted various ablation studies and comparisons with prior methods on a newly collected dataset annotated with nine road defect classes.","The experiments show that our proposed method outperforms existing state-of-the-art methods for multi-class road defect detection and segmentation methods."],"url":"http://arxiv.org/abs/2402.04064v1"}
{"created":"2024-02-06 15:05:40","title":"Link Prediction with Relational Hypergraphs","abstract":"Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model architectures substantially outperform every baseline for inductive link prediction, and lead to state-of-the-art results for transductive link prediction. Our study therefore unlocks applications of graph neural networks to fully relational structures.","sentences":["Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications.","Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs.","The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$).","In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms.","Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks.","The resulting model architectures substantially outperform every baseline for inductive link prediction, and lead to state-of-the-art results for transductive link prediction.","Our study therefore unlocks applications of graph neural networks to fully relational structures."],"url":"http://arxiv.org/abs/2402.04062v1"}
{"created":"2024-02-06 15:05:25","title":"TopoNav: Topological Navigation for Efficient Exploration in Sparse Reward Environments","abstract":"Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback. This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail. In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration. TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization. Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways. It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings. To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immediate tasks like obstacle avoidance while maintaining focus on the overall goal. We demonstrate TopoNav's effectiveness in simulated environments that replicate real-world conditions. Our results reveal significant improvements in exploration efficiency, navigational accuracy, and adaptability to unforeseen obstacles, showcasing its potential to revolutionize autonomous exploration in a wide range of applications, including search and rescue, environmental monitoring, and planetary exploration.","sentences":["Autonomous robots exploring unknown areas face a significant challenge -- navigating effectively without prior maps and with limited external feedback.","This challenge intensifies in sparse reward environments, where traditional exploration techniques often fail.","In this paper, we introduce TopoNav, a novel framework that empowers robots to overcome these constraints and achieve efficient, adaptable, and goal-oriented exploration.","TopoNav's fundamental building blocks are active topological mapping, intrinsic reward mechanisms, and hierarchical objective prioritization.","Throughout its exploration, TopoNav constructs a dynamic topological map that captures key locations and pathways.","It utilizes intrinsic rewards to guide the robot towards designated sub-goals within this map, fostering structured exploration even in sparse reward settings.","To ensure efficient navigation, TopoNav employs the Hierarchical Objective-Driven Active Topologies framework, enabling the robot to prioritize immediate tasks like obstacle avoidance while maintaining focus on the overall goal.","We demonstrate TopoNav's effectiveness in simulated environments that replicate real-world conditions.","Our results reveal significant improvements in exploration efficiency, navigational accuracy, and adaptability to unforeseen obstacles, showcasing its potential to revolutionize autonomous exploration in a wide range of applications, including search and rescue, environmental monitoring, and planetary exploration."],"url":"http://arxiv.org/abs/2402.04061v1"}
{"created":"2024-02-06 15:03:53","title":"Deep Learning for Multivariate Time Series Imputation: A Survey","abstract":"The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis. Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks. In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods. First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations. We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks. Finally, the open issues for future research on multivariate time series imputation are pointed out. All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}.","sentences":["The ubiquitous missing values cause the multivariate time series data to be partially observed, destroying the integrity of time series and hindering the effective time series data analysis.","Recently deep learning imputation methods have demonstrated remarkable success in elevating the quality of corrupted time series data, subsequently enhancing performance in downstream tasks.","In this paper, we conduct a comprehensive survey on the recently proposed deep learning imputation methods.","First, we propose a taxonomy for the reviewed methods, and then provide a structured review of these methods by highlighting their strengths and limitations.","We also conduct empirical experiments to study different methods and compare their enhancement for downstream tasks.","Finally, the open issues for future research on multivariate time series imputation are pointed out.","All code and configurations of this work, including a regularly maintained multivariate time series imputation paper list, can be found in the GitHub repository~\\url{https://github.com/WenjieDu/Awesome\\_Imputation}."],"url":"http://arxiv.org/abs/2402.04059v1"}
{"created":"2024-02-06 15:00:08","title":"More Flexible PAC-Bayesian Meta-Learning by Learning Learning Algorithms","abstract":"We introduce a new framework for studying meta-learning methods using PAC-Bayesian theory. Its main advantage over previous work is that it allows for more flexibility in how the transfer of knowledge between tasks is realized. For previous approaches, this could only happen indirectly, by means of learning prior distributions over models. In contrast, the new generalization bounds that we prove express the process of meta-learning much more directly as learning the learning algorithm that should be used for future tasks. The flexibility of our framework makes it suitable to analyze a wide range of meta-learning mechanisms and even design new mechanisms. Other than our theoretical contributions we also show empirically that our framework improves the prediction quality in practical meta-learning mechanisms.","sentences":["We introduce a new framework for studying meta-learning methods using PAC-Bayesian theory.","Its main advantage over previous work is that it allows for more flexibility in how the transfer of knowledge between tasks is realized.","For previous approaches, this could only happen indirectly, by means of learning prior distributions over models.","In contrast, the new generalization bounds that we prove express the process of meta-learning much more directly as learning the learning algorithm that should be used for future tasks.","The flexibility of our framework makes it suitable to analyze a wide range of meta-learning mechanisms and even design new mechanisms.","Other than our theoretical contributions we also show empirically that our framework improves the prediction quality in practical meta-learning mechanisms."],"url":"http://arxiv.org/abs/2402.04054v1"}
{"created":"2024-02-06 14:53:28","title":"Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching","abstract":"Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly align the directions of singular vectors associated with large singular values across models. This alignment brings the singular vectors with large singular values, which determine the model functionality, closer between pre-merged and post-merged models, so that the post-merged model retains functionality similar to the pre-merged models, making it easy to satisfy LMC. Finally, we analyze the difference between WM and straight-through estimator (STE), a dataset-dependent permutation search method, and show that WM outperforms STE, especially when merging three or more models.","sentences":["Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant.","This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging.","We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself.","We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer.","This finding shows that permutations found by WM mainly align the directions of singular vectors associated with large singular values across models.","This alignment brings the singular vectors with large singular values, which determine the model functionality, closer between pre-merged and post-merged models, so that the post-merged model retains functionality similar to the pre-merged models, making it easy to satisfy LMC.","Finally, we analyze the difference between WM and straight-through estimator (STE), a dataset-dependent permutation search method, and show that WM outperforms STE, especially when merging three or more models."],"url":"http://arxiv.org/abs/2402.04051v1"}
{"created":"2024-02-06 14:53:19","title":"Connecting the Dots: Collaborative Fine-tuning for Black-Box Vision-Language Models","abstract":"With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks. Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership. This paper proposes a \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model. CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style. Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules. These modules are optimized by a novel collaborative training algorithm. Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT. The results show that CraFT achieves a decent gain of about 12\\% with 16-shot datasets and only 8,000 queries. Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62\\% compared to the white-box method.","sentences":["With the emergence of pretrained vision-language models (VLMs), considerable efforts have been devoted to fine-tuning them for downstream tasks.","Despite the progress made in designing efficient fine-tuning methods, such methods require access to the model's parameters, which can be challenging as model owners often opt to provide their models as a black box to safeguard model ownership.","This paper proposes a \\textbf{C}ollabo\\textbf{ra}tive \\textbf{F}ine-\\textbf{T}uning (\\textbf{CraFT}) approach for fine-tuning black-box VLMs to downstream tasks, where one only has access to the input prompts and the output predictions of the model.","CraFT comprises two modules, a prompt generation module for learning text prompts and a prediction refinement module for enhancing output predictions in residual style.","Additionally, we introduce an auxiliary prediction-consistent loss to promote consistent optimization across these modules.","These modules are optimized by a novel collaborative training algorithm.","Extensive experiments on few-shot classification over 15 datasets demonstrate the superiority of CraFT.","The results show that CraFT achieves a decent gain of about 12\\% with 16-shot datasets and only 8,000 queries.","Moreover, CraFT trains faster and uses only about 1/80 of the memory footprint for deployment, while sacrificing only 1.62\\% compared to the white-box method."],"url":"http://arxiv.org/abs/2402.04050v1"}
