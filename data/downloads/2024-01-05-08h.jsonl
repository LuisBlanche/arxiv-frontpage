{"created":"2024-01-04 18:59:49","title":"Learning to Prompt with Text Only Supervision for Vision-Language Models","abstract":"Foundational vision-language models such as CLIP are becoming a new paradigm in vision, due to their excellent generalization abilities. However, adapting these models for downstream tasks while maintaining their generalization remains a challenge. In literature, one branch of methods adapts CLIP by learning prompts using visual information. While effective, most of these works require labeled data which is not practical, and often struggle to generalize towards new datasets due to over-fitting on the source data. An alternative approach resorts to training-free methods by generating class descriptions from large language models (LLMs) and perform prompt ensembling. However, these methods often generate class specific prompts that cannot be transferred to other classes, which incur higher costs by generating LLM descriptions for each class separately. In this work, we propose to combine the strengths of these both streams of methods by learning prompts using only text data derived from LLMs. As supervised training of prompts is not trivial due to absence of images, we develop a training approach that allows prompts to extract rich contextual knowledge from LLM data. Moreover, with LLM contextual data mapped within the learned prompts, it enables zero-shot transfer of prompts to new classes and datasets potentially cutting the LLM prompt engineering cost. To the best of our knowledge, this is the first work that learns generalized prompts using text only data. We perform extensive evaluations on 4 benchmarks where our method improves over prior ensembling works while being competitive to those utilizing labeled images. Our code and pre-trained models are available at https://github.com/muzairkhattak/ProText.","sentences":["Foundational vision-language models such as CLIP are becoming a new paradigm in vision, due to their excellent generalization abilities.","However, adapting these models for downstream tasks while maintaining their generalization remains a challenge.","In literature, one branch of methods adapts CLIP by learning prompts using visual information.","While effective, most of these works require labeled data which is not practical, and often struggle to generalize towards new datasets due to over-fitting on the source data.","An alternative approach resorts to training-free methods by generating class descriptions from large language models (LLMs) and perform prompt ensembling.","However, these methods often generate class specific prompts that cannot be transferred to other classes, which incur higher costs by generating LLM descriptions for each class separately.","In this work, we propose to combine the strengths of these both streams of methods by learning prompts using only text data derived from LLMs.","As supervised training of prompts is not trivial due to absence of images, we develop a training approach that allows prompts to extract rich contextual knowledge from LLM data.","Moreover, with LLM contextual data mapped within the learned prompts, it enables zero-shot transfer of prompts to new classes and datasets potentially cutting the LLM prompt engineering cost.","To the best of our knowledge, this is the first work that learns generalized prompts using text only data.","We perform extensive evaluations on 4 benchmarks where our method improves over prior ensembling works while being competitive to those utilizing labeled images.","Our code and pre-trained models are available at https://github.com/muzairkhattak/ProText."],"url":"http://arxiv.org/abs/2401.02418v1"}
{"created":"2024-01-04 18:59:25","title":"ODIN: A Single Model for 2D and 3D Perception","abstract":"State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It outperforms all previous works by a wide margin when the sensed 3D point cloud is used in place of the point cloud sampled from 3D mesh. When used as the 3D perception engine in an instructable embodied agent architecture, it sets a new state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and checkpoints can be found at the project website: https://odin-seg.github.io.","sentences":["State-of-the-art models on contemporary 3D perception benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images.","They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead.","The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures.","In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion.","Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coordinates for 3D feature tokens.","ODIN achieves state-of-the-art performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation benchmarks, and competitive performance on ScanNet, S3DIS and COCO.","It outperforms all previous works by a wide margin when the sensed 3D point cloud is used in place of the point cloud sampled from 3D mesh.","When used as the 3D perception engine in an instructable embodied agent architecture, it sets a new state-of-the-art on the TEACh action-from-dialogue benchmark.","Our code and checkpoints can be found at the project website: https://odin-seg.github.io."],"url":"http://arxiv.org/abs/2401.02416v1"}
{"created":"2024-01-04 18:59:12","title":"LLaMA Pro: Progressive LLaMA with Block Expansion","abstract":"Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.","sentences":["Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA.","To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks.","We tune the expanded blocks using only new corpus, efficiently and effectively improving the model's knowledge without catastrophic forgetting.","In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics.","LLaMA Pro and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent.","Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments."],"url":"http://arxiv.org/abs/2401.02415v1"}
{"created":"2024-01-04 18:55:01","title":"Bring Metric Functions into Diffusion Models","abstract":"We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising Diffusion Probabilistic Model (DDPM) by effectively incorporating additional metric functions in training. Metric functions such as the LPIPS loss have been proven highly effective in consistency models derived from the score matching. However, for the diffusion counterparts, the methodology and efficacy of adding extra metric functions remain unclear. One major challenge is the mismatch between the noise predicted by a DDPM at each step and the desired clean image that the metric function works well on. To address this problem, we propose Cas-DM, a network architecture that cascades two network modules to effectively apply metric functions to the diffusion model training. The first module, similar to a standard DDPM, learns to predict the added noise and is unaffected by the metric function. The second cascaded module learns to predict the clean image, thereby facilitating the metric function computation. Experiment results show that the proposed diffusion model backbone enables the effective use of the LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on various established benchmarks.","sentences":["We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising Diffusion Probabilistic Model (DDPM) by effectively incorporating additional metric functions in training.","Metric functions such as the LPIPS loss have been proven highly effective in consistency models derived from the score matching.","However, for the diffusion counterparts, the methodology and efficacy of adding extra metric functions remain unclear.","One major challenge is the mismatch between the noise predicted by a DDPM at each step and the desired clean image that the metric function works well on.","To address this problem, we propose Cas-DM, a network architecture that cascades two network modules to effectively apply metric functions to the diffusion model training.","The first module, similar to a standard DDPM, learns to predict the added noise and is unaffected by the metric function.","The second cascaded module learns to predict the clean image, thereby facilitating the metric function computation.","Experiment results show that the proposed diffusion model backbone enables the effective use of the LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on various established benchmarks."],"url":"http://arxiv.org/abs/2401.02414v1"}
{"created":"2024-01-04 18:53:01","title":"LLM Augmented LLMs: Expanding Capabilities through Composition","abstract":"Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains. However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills. On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks. In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities. To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities. Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings. We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\\% on tasks like translation into English and arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts.","sentences":["Foundational models with billions of parameters which have been trained on large corpora of data have demonstrated non-trivial skills in a variety of domains.","However, due to their monolithic structure, it is challenging and expensive to augment them or impart new skills.","On the other hand, due to their adaptation abilities, several new instances of these models are being trained towards new domains and tasks.","In this work, we study the problem of efficient and practical composition of existing foundation models with more specific models to enable newer capabilities.","To this end, we propose CALM -- Composition to Augment Language Models -- which introduces cross-attention between models to compose their representations and enable new capabilities.","Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using' existing LLMs along with a few additional parameters and data, (ii) Existing model weights are kept intact, and hence preserves existing capabilities, and (iii) Applies to diverse domains and settings.","We illustrate that augmenting PaLM2-S with a smaller model trained on low-resource languages results in an absolute improvement of up to 13\\% on tasks like translation into English and arithmetic reasoning for low-resource languages.","Similarly, when PaLM2-S is augmented with a code-specific model, we see a relative improvement of 40\\% over the base model for code generation and explanation tasks -- on-par with fully fine-tuned counterparts."],"url":"http://arxiv.org/abs/2401.02412v1"}
{"created":"2024-01-04 18:50:38","title":"What You See is What You GAN: Rendering Every Pixel for High-Fidelity Geometry in 3D GANs","abstract":"3D-aware Generative Adversarial Networks (GANs) have shown remarkable progress in learning to generate multi-view-consistent images and 3D geometries of scenes from collections of 2D images via neural volume rendering. Yet, the significant memory and computational costs of dense sampling in volume rendering have forced 3D GANs to adopt patch-based training or employ low-resolution rendering with post-processing 2D super resolution, which sacrifices multiview consistency and the quality of resolved geometry. Consequently, 3D GANs have not yet been able to fully resolve the rich 3D geometry present in 2D images. In this work, we propose techniques to scale neural volume rendering to the much higher resolution of native 2D images, thereby resolving fine-grained 3D geometry with unprecedented detail. Our approach employs learning-based samplers for accelerating neural rendering for 3D GAN training using up to 5 times fewer depth samples. This enables us to explicitly \"render every pixel\" of the full-resolution image during training and inference without post-processing superresolution in 2D. Together with our strategy to learn high-quality surface geometry, our method synthesizes high-resolution 3D geometry and strictly view-consistent images while maintaining image quality on par with baselines relying on post-processing super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D GANs.","sentences":["3D-aware Generative Adversarial Networks (GANs) have shown remarkable progress in learning to generate multi-view-consistent images and 3D geometries of scenes from collections of 2D images via neural volume rendering.","Yet, the significant memory and computational costs of dense sampling in volume rendering have forced 3D GANs to adopt patch-based training or employ low-resolution rendering with post-processing 2D super resolution, which sacrifices multiview consistency and the quality of resolved geometry.","Consequently, 3D GANs have not yet been able to fully resolve the rich 3D geometry present in 2D images.","In this work, we propose techniques to scale neural volume rendering to the much higher resolution of native 2D images, thereby resolving fine-grained 3D geometry with unprecedented detail.","Our approach employs learning-based samplers for accelerating neural rendering for 3D GAN training using up to 5 times fewer depth samples.","This enables us to explicitly \"render every pixel\" of the full-resolution image during training and inference without post-processing superresolution in 2D. Together with our strategy to learn high-quality surface geometry, our method synthesizes high-resolution 3D geometry and strictly view-consistent images while maintaining image quality on par with baselines relying on post-processing super resolution.","We demonstrate state-of-the-art 3D gemetric quality on FFHQ and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D GANs."],"url":"http://arxiv.org/abs/2401.02411v1"}
{"created":"2024-01-04 18:43:26","title":"Correctness Comparison of ChatGPT-4, Bard, Claude-2, and Copilot for Spatial Tasks","abstract":"Generative AI including large language models (LLMs) have recently gained significant interest in the geo-science community through its versatile task-solving capabilities including coding, spatial computations, generation of sample data, time-series forecasting, toponym recognition, or image classification. So far, the assessment of LLMs for spatial tasks has primarily focused on ChatGPT, arguably the most prominent AI chatbot, whereas other chatbots received less attention. To narrow this research gap, this study evaluates the correctness of responses for a set of 54 spatial tasks assigned to four prominent chatbots, i.e., ChatGPT-4, Bard, Claude-2, and Copilot. Overall, the chatbots performed well on spatial literacy, GIS theory, and interpretation of programming code and given functions, but revealed weaknesses in mapping, code generation, and code translation. ChatGPT-4 outperformed other chatbots across most task categories.","sentences":["Generative AI including large language models (LLMs) have recently gained significant interest in the geo-science community through its versatile task-solving capabilities including coding, spatial computations, generation of sample data, time-series forecasting, toponym recognition, or image classification.","So far, the assessment of LLMs for spatial tasks has primarily focused on ChatGPT, arguably the most prominent AI chatbot, whereas other chatbots received less attention.","To narrow this research gap, this study evaluates the correctness of responses for a set of 54 spatial tasks assigned to four prominent chatbots, i.e., ChatGPT-4, Bard, Claude-2, and Copilot.","Overall, the chatbots performed well on spatial literacy, GIS theory, and interpretation of programming code and given functions, but revealed weaknesses in mapping, code generation, and code translation.","ChatGPT-4 outperformed other chatbots across most task categories."],"url":"http://arxiv.org/abs/2401.02404v1"}
{"created":"2024-01-04 18:42:28","title":"Real-Time 2D Temperature Field Prediction in Metal Additive Manufacturing Using Physics-Informed Neural Networks","abstract":"Accurately predicting the temperature field in metal additive manufacturing (AM) processes is critical to preventing overheating, adjusting process parameters, and ensuring process stability. While physics-based computational models offer precision, they are often time-consuming and unsuitable for real-time predictions and online control in iterative design scenarios. Conversely, machine learning models rely heavily on high-quality datasets, which can be costly and challenging to obtain within the metal AM domain. Our work addresses this by introducing a physics-informed neural network framework specifically designed for temperature field prediction in metal AM. This framework incorporates a physics-informed input, physics-informed loss function, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture. Utilizing real-time temperature data from the process, our model predicts 2D temperature fields for future timestamps across diverse geometries, deposition patterns, and process parameters. We validate the proposed framework in two scenarios: full-field temperature prediction for a thin wall and 2D temperature field prediction for cylinder and cubic parts, demonstrating errors below 3% and 1%, respectively. Our proposed framework exhibits the flexibility to be applied across diverse scenarios with varying process parameters, geometries, and deposition patterns.","sentences":["Accurately predicting the temperature field in metal additive manufacturing (AM) processes is critical to preventing overheating, adjusting process parameters, and ensuring process stability.","While physics-based computational models offer precision, they are often time-consuming and unsuitable for real-time predictions and online control in iterative design scenarios.","Conversely, machine learning models rely heavily on high-quality datasets, which can be costly and challenging to obtain within the metal AM domain.","Our work addresses this by introducing a physics-informed neural network framework specifically designed for temperature field prediction in metal AM.","This framework incorporates a physics-informed input, physics-informed loss function, and a Convolutional Long Short-Term Memory (ConvLSTM) architecture.","Utilizing real-time temperature data from the process, our model predicts 2D temperature fields for future timestamps across diverse geometries, deposition patterns, and process parameters.","We validate the proposed framework in two scenarios: full-field temperature prediction for a thin wall and 2D temperature field prediction for cylinder and cubic parts, demonstrating errors below 3% and 1%, respectively.","Our proposed framework exhibits the flexibility to be applied across diverse scenarios with varying process parameters, geometries, and deposition patterns."],"url":"http://arxiv.org/abs/2401.02403v1"}
{"created":"2024-01-04 18:39:32","title":"3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language Distillation","abstract":"3D panoptic segmentation is a challenging perception task, which aims to predict both semantic and instance annotations for 3D points in a scene. Although prior 3D panoptic segmentation approaches have achieved great performance on closed-set benchmarks, generalizing to novel categories remains an open problem. For unseen object categories, 2D open-vocabulary segmentation has achieved promising results that solely rely on frozen CLIP backbones and ensembling multiple classification outputs. However, we find that simply extending these 2D models to 3D does not achieve good performance due to poor per-mask classification quality on novel categories. In this paper, we propose the first method to tackle 3D open-vocabulary panoptic segmentation. Our model takes advantage of the fusion between learnable LiDAR features and dense frozen vision CLIP features, using a single classification head to make predictions for both base and novel classes. To further improve the classification performance on novel classes and leverage the CLIP model, we propose two novel loss functions: object-level distillation loss and voxel-level distillation loss. Our experiments on the nuScenes and SemanticKITTI datasets show that our method outperforms strong baselines by a large margin.","sentences":["3D panoptic segmentation is a challenging perception task, which aims to predict both semantic and instance annotations for 3D points in a scene.","Although prior 3D panoptic segmentation approaches have achieved great performance on closed-set benchmarks, generalizing to novel categories remains an open problem.","For unseen object categories, 2D open-vocabulary segmentation has achieved promising results that solely rely on frozen CLIP backbones and ensembling multiple classification outputs.","However, we find that simply extending these 2D models to 3D does not achieve good performance due to poor per-mask classification quality on novel categories.","In this paper, we propose the first method to tackle 3D open-vocabulary panoptic segmentation.","Our model takes advantage of the fusion between learnable LiDAR features and dense frozen vision CLIP features, using a single classification head to make predictions for both base and novel classes.","To further improve the classification performance on novel classes and leverage the CLIP model, we propose two novel loss functions: object-level distillation loss and voxel-level distillation loss.","Our experiments on the nuScenes and SemanticKITTI datasets show that our method outperforms strong baselines by a large margin."],"url":"http://arxiv.org/abs/2401.02402v1"}
{"created":"2024-01-04 18:32:48","title":"Learning the 3D Fauna of the Web","abstract":"Learning 3D models of all animals on the Earth requires massively scaling up existing solutions. With this ultimate goal in mind, we develop 3D-Fauna, an approach that learns a pan-category deformable 3D animal model for more than 100 animal species jointly. One crucial bottleneck of modeling animals is the limited availability of training data, which we overcome by simply learning from 2D Internet images. We show that prior category-specific attempts fail to generalize to rare species with limited training images. We address this challenge by introducing the Semantic Bank of Skinned Models (SBSM), which automatically discovers a small set of base animal shapes by combining geometric inductive priors with semantic knowledge implicitly captured by an off-the-shelf self-supervised feature extractor. To train such a model, we also contribute a new large-scale dataset of diverse animal species. At inference time, given a single image of any quadruped animal, our model reconstructs an articulated 3D mesh in a feed-forward fashion within seconds.","sentences":["Learning 3D models of all animals on the Earth requires massively scaling up existing solutions.","With this ultimate goal in mind, we develop 3D-Fauna, an approach that learns a pan-category deformable 3D animal model for more than 100 animal species jointly.","One crucial bottleneck of modeling animals is the limited availability of training data, which we overcome by simply learning from 2D Internet images.","We show that prior category-specific attempts fail to generalize to rare species with limited training images.","We address this challenge by introducing the Semantic Bank of Skinned Models (SBSM), which automatically discovers a small set of base animal shapes by combining geometric inductive priors with semantic knowledge implicitly captured by an off-the-shelf self-supervised feature extractor.","To train such a model, we also contribute a new large-scale dataset of diverse animal species.","At inference time, given a single image of any quadruped animal, our model reconstructs an articulated 3D mesh in a feed-forward fashion within seconds."],"url":"http://arxiv.org/abs/2401.02400v1"}
{"created":"2024-01-04 18:31:21","title":"Generating synthetic data for neural operators","abstract":"Numerous developments in the recent literature show the promising potential of deep learning in obtaining numerical solutions to partial differential equations (PDEs) beyond the reach of current numerical solvers. However, data-driven neural operators all suffer from the same problem: the data needed to train a network depends on classical numerical solvers such as finite difference or finite element, among others. In this paper, we propose a new approach to generating synthetic functional training data that does not require solving a PDE numerically. The way we do this is simple: we draw a large number $N$ of independent and identically distributed `random functions' $u_j$ from the underlying solution space (e.g., $H_0^1(\\Omega)$) in which we know the solution lies according to classical theory. We then plug each such random candidate solution into the equation and get a corresponding right-hand side function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervised training data for learning the underlying inverse problem $f \\rightarrow u$. This `backwards' approach to generating training data only requires derivative computations, in contrast to standard `forward' approaches, which require a numerical PDE solver, enabling us to generate a large number of such data points quickly and efficiently. While the idea is simple, we hope that this method will expand the potential for developing neural PDE solvers that do not depend on classical numerical solvers.","sentences":["Numerous developments in the recent literature show the promising potential of deep learning in obtaining numerical solutions to partial differential equations (PDEs) beyond the reach of current numerical solvers.","However, data-driven neural operators all suffer from the same problem: the data needed to train a network depends on classical numerical solvers such as finite difference or finite element, among others.","In this paper, we propose a new approach to generating synthetic functional training data that does not require solving a PDE numerically.","The way we do this is simple: we draw a large number $N$ of independent and identically distributed `random functions' $u_j$ from the underlying solution space (e.g., $H_0^1(\\Omega)$) in which we know the solution lies according to classical theory.","We then plug each such random candidate solution into the equation and get a corresponding right-hand side function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervised training data for learning the underlying inverse problem $f \\rightarrow u$.","This `backwards' approach to generating training data only requires derivative computations, in contrast to standard `forward' approaches, which require a numerical PDE solver, enabling us to generate a large number of such data points quickly and efficiently.","While the idea is simple, we hope that this method will expand the potential for developing neural PDE solvers that do not depend on classical numerical solvers."],"url":"http://arxiv.org/abs/2401.02398v1"}
{"created":"2024-01-04 18:18:32","title":"Analyzing Misinformation Claims During the 2022 Brazilian General Election on WhatsApp, Twitter, and Kwai","abstract":"This study analyzes misinformation from WhatsApp, Twitter, and Kwai during the 2022 Brazilian general election. Given the democratic importance of accurate information during elections, multiple fact-checking organizations collaborated to identify and respond to misinformation via WhatsApp tiplines and power a fact-checking feature within a chatbot operated by Brazil's election authority, the TSE. WhatsApp is installed on over 99% of smartphones in Brazil, and the TSE chatbot was used by millions of citizens in the run-up to the elections. During the same period, we collected social media data from Twitter (now X) and Kwai (a popular video-sharing app similar to TikTok). Using the WhatsApp, Kwai, and Twitter data along with fact-checks from three Brazilian fact-checking organizations, we find unique claims on each platform. Even when the same claims are present on different platforms, they often differ in format, detail, length, or other characteristics. Our research highlights the limitations of current claim matching algorithms to match claims across platforms with such differences and identifies areas for further algorithmic development. Finally, we perform a descriptive analysis examining the formats (image, video, audio, text) and content themes of popular misinformation claims.","sentences":["This study analyzes misinformation from WhatsApp, Twitter, and Kwai during the 2022 Brazilian general election.","Given the democratic importance of accurate information during elections, multiple fact-checking organizations collaborated to identify and respond to misinformation via WhatsApp tiplines and power a fact-checking feature within a chatbot operated by Brazil's election authority, the TSE.","WhatsApp is installed on over 99% of smartphones in Brazil, and the TSE chatbot was used by millions of citizens in the run-up to the elections.","During the same period, we collected social media data from Twitter (now X) and Kwai (a popular video-sharing app similar to TikTok).","Using the WhatsApp, Kwai, and Twitter data along with fact-checks from three Brazilian fact-checking organizations, we find unique claims on each platform.","Even when the same claims are present on different platforms, they often differ in format, detail, length, or other characteristics.","Our research highlights the limitations of current claim matching algorithms to match claims across platforms with such differences and identifies areas for further algorithmic development.","Finally, we perform a descriptive analysis examining the formats (image, video, audio, text) and content themes of popular misinformation claims."],"url":"http://arxiv.org/abs/2401.02395v1"}
{"created":"2024-01-04 17:54:59","title":"TinyLlama: An Open-Source Small Language Model","abstract":"We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs. Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency. Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks. It significantly outperforms existing open-source language models with comparable sizes. Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama.","sentences":["We present TinyLlama, a compact 1.1B language model pretrained on around 1 trillion tokens for approximately 3 epochs.","Building on the architecture and tokenizer of Llama 2, TinyLlama leverages various advances contributed by the open-source community (e.g., FlashAttention), achieving better computational efficiency.","Despite its relatively small size, TinyLlama demonstrates remarkable performance in a series of downstream tasks.","It significantly outperforms existing open-source language models with comparable sizes.","Our model checkpoints and code are publicly available on GitHub at https://github.com/jzhang38/TinyLlama."],"url":"http://arxiv.org/abs/2401.02385v1"}
{"created":"2024-01-04 17:51:48","title":"ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning","abstract":"Charts play a vital role in data visualization, understanding data patterns, and informed decision-making. However, their unique combination of graphical elements (e.g., bars, lines) and textual components (e.g., labels, legends) poses challenges for general-purpose multimodal models. While vision-language models trained on chart data excel in comprehension, they struggle with generalization and require task-specific fine-tuning. To address these challenges, we propose ChartAssistant, a chart-based vision-language model for universal chart comprehension and reasoning. ChartAssistant leverages ChartSFT, a comprehensive dataset covering diverse chart-related tasks with basic and specialized chart types. It undergoes a two-stage training process, starting with pre-training on chart-to-table parsing to align chart and text, followed by multitask instruction-following fine-tuning. This approach enables ChartAssistant to achieve competitive performance across various chart tasks without task-specific fine-tuning. Experimental results demonstrate significant performance gains over the state-of-the-art UniChart method, outperforming OpenAI's GPT-4V(ision) on real-world chart data. The code and data are available at https://github.com/OpenGVLab/ChartAst.","sentences":["Charts play a vital role in data visualization, understanding data patterns, and informed decision-making.","However, their unique combination of graphical elements (e.g., bars, lines) and textual components (e.g., labels, legends) poses challenges for general-purpose multimodal models.","While vision-language models trained on chart data excel in comprehension, they struggle with generalization and require task-specific fine-tuning.","To address these challenges, we propose ChartAssistant, a chart-based vision-language model for universal chart comprehension and reasoning.","ChartAssistant leverages ChartSFT, a comprehensive dataset covering diverse chart-related tasks with basic and specialized chart types.","It undergoes a two-stage training process, starting with pre-training on chart-to-table parsing to align chart and text, followed by multitask instruction-following fine-tuning.","This approach enables ChartAssistant to achieve competitive performance across various chart tasks without task-specific fine-tuning.","Experimental results demonstrate significant performance gains over the state-of-the-art UniChart method, outperforming OpenAI's GPT-4V(ision) on real-world chart data.","The code and data are available at https://github.com/OpenGVLab/ChartAst."],"url":"http://arxiv.org/abs/2401.02384v1"}
{"created":"2024-01-04 17:51:44","title":"Survey of 3D Human Body Pose and Shape Estimation Methods for Contemporary Dance Applications","abstract":"3D human body shape and pose estimation from RGB images is a challenging problem with potential applications in augmented/virtual reality, healthcare and fitness technology and virtual retail. Recent solutions have focused on three types of inputs: i) single images, ii) multi-view images and iii) videos. In this study, we surveyed and compared 3D body shape and pose estimation methods for contemporary dance and performing arts, with a special focus on human body pose and dressing, camera viewpoint, illumination conditions and background conditions. We demonstrated that multi-frame methods, such as PHALP, provide better results than single-frame method for pose estimation when dancers are performing contemporary dances.","sentences":["3D human body shape and pose estimation from RGB images is a challenging problem with potential applications in augmented/virtual reality, healthcare and fitness technology and virtual retail.","Recent solutions have focused on three types of inputs: i) single images, ii) multi-view images and iii) videos.","In this study, we surveyed and compared 3D body shape and pose estimation methods for contemporary dance and performing arts, with a special focus on human body pose and dressing, camera viewpoint, illumination conditions and background conditions.","We demonstrated that multi-frame methods, such as PHALP, provide better results than single-frame method for pose estimation when dancers are performing contemporary dances."],"url":"http://arxiv.org/abs/2401.02383v1"}
{"created":"2024-01-04 17:49:20","title":"Faster optimal univariate microgaggregation","abstract":"Microaggregation is a method to coarsen a dataset, by optimally clustering data points in groups of at least $k$ points, thereby providing a $k$-anonymity type disclosure guarantee for each point in the dataset. Previous algorithms for univariate microaggregation had a $O(k n)$ time complexity. By rephrasing microaggregation as an instance of the concave least weight subsequence problem, in this work we provide improved algorithms that provide an optimal univariate microaggregation on sorted data in $O(n)$ time and space. We further show that our algorithms work not only for sum of squares cost functions, as typically considered, but seamlessly extend to many other cost functions used for univariate microaggregation tasks. In experiments we show that the presented algorithms lead to real world performance improvements.","sentences":["Microaggregation is a method to coarsen a dataset, by optimally clustering data points in groups of at least $k$ points, thereby providing a $k$-anonymity type disclosure guarantee for each point in the dataset.","Previous algorithms for univariate microaggregation had a $O(k n)$ time complexity.","By rephrasing microaggregation as an instance of the concave least weight subsequence problem, in this work we provide improved algorithms that provide an optimal univariate microaggregation on sorted data in $O(n)$ time and space.","We further show that our algorithms work not only for sum of squares cost functions, as typically considered, but seamlessly extend to many other cost functions used for univariate microaggregation tasks.","In experiments we show that the presented algorithms lead to real world performance improvements."],"url":"http://arxiv.org/abs/2401.02381v1"}
{"created":"2024-01-04 17:47:44","title":"Byzantine-Resilient Gradient Coding through Local Gradient Computations","abstract":"We consider gradient coding in the presence of an adversary controlling so-called malicious workers trying to corrupt the computations. Previous works propose the use of MDS codes to treat the responses from malicious workers as errors and correct them using the error-correction properties of the code. This comes at the expense of increasing the replication, i.e., the number of workers each partial gradient is computed by. In this work, we propose a way to reduce the replication to $s+1$ instead of $2s+1$ in the presence of $s$ malicious workers. Our method detects erroneous inputs from the malicious workers, transforming them into erasures. This comes at the expense of $s$ additional local computations at the main node and additional rounds of light communication between the main node and the workers. We define a general framework and give fundamental limits for fractional repetition data allocations. Our scheme is optimal in terms of replication and local computation and incurs a communication cost that is asymptotically, in the size of the dataset, a multiplicative factor away from the derived bound. We furthermore show how additional redundancy can be exploited to reduce the number of local computations and communication cost, or, alternatively, tolerate straggling workers.","sentences":["We consider gradient coding in the presence of an adversary controlling so-called malicious workers trying to corrupt the computations.","Previous works propose the use of MDS codes to treat the responses from malicious workers as errors and correct them using the error-correction properties of the code.","This comes at the expense of increasing the replication, i.e., the number of workers each partial gradient is computed by.","In this work, we propose a way to reduce the replication to $s+1$ instead of $2s+1$ in the presence of $s$ malicious workers.","Our method detects erroneous inputs from the malicious workers, transforming them into erasures.","This comes at the expense of $s$ additional local computations at the main node and additional rounds of light communication between the main node and the workers.","We define a general framework and give fundamental limits for fractional repetition data allocations.","Our scheme is optimal in terms of replication and local computation and incurs a communication cost that is asymptotically, in the size of the dataset, a multiplicative factor away from the derived bound.","We furthermore show how additional redundancy can be exploited to reduce the number of local computations and communication cost, or, alternatively, tolerate straggling workers."],"url":"http://arxiv.org/abs/2401.02380v1"}
{"created":"2024-01-04 17:47:36","title":"Detection and Discovery of Misinformation Sources using Attributed Webgraphs","abstract":"Website reliability labels underpin almost all research in misinformation detection. However, misinformation sources often exhibit transient behavior, which makes many such labeled lists obsolete over time. We demonstrate that Search Engine Optimization (SEO) attributes provide strong signals for predicting news site reliability. We introduce a novel attributed webgraph dataset with labeled news domains and their connections to outlinking and backlinking domains. We demonstrate the success of graph neural networks in detecting news site reliability using these attributed webgraphs, and show that our baseline news site reliability classifier outperforms current SoTA methods on the PoliticalNews dataset, achieving an F1 score of 0.96. Finally, we introduce and evaluate a novel graph-based algorithm for discovering previously unknown misinformation news sources.","sentences":["Website reliability labels underpin almost all research in misinformation detection.","However, misinformation sources often exhibit transient behavior, which makes many such labeled lists obsolete over time.","We demonstrate that Search Engine Optimization (SEO) attributes provide strong signals for predicting news site reliability.","We introduce a novel attributed webgraph dataset with labeled news domains and their connections to outlinking and backlinking domains.","We demonstrate the success of graph neural networks in detecting news site reliability using these attributed webgraphs, and show that our baseline news site reliability classifier outperforms current SoTA methods on the PoliticalNews dataset, achieving an F1 score of 0.96.","Finally, we introduce and evaluate a novel graph-based algorithm for discovering previously unknown misinformation news sources."],"url":"http://arxiv.org/abs/2401.02379v1"}
{"created":"2024-01-04 17:37:09","title":"Machine Learning in Robotic Ultrasound Imaging: Challenges and Perspectives","abstract":"This article reviews the recent advances in intelligent robotic ultrasound (US) imaging systems. We commence by presenting the commonly employed robotic mechanisms and control techniques in robotic US imaging, along with their clinical applications. Subsequently, we focus on the deployment of machine learning techniques in the development of robotic sonographers, emphasizing crucial developments aimed at enhancing the intelligence of these systems. The methods for achieving autonomous action reasoning are categorized into two sets of approaches: those relying on implicit environmental data interpretation and those using explicit interpretation. Throughout this exploration, we also discuss practical challenges, including those related to the scarcity of medical data, the need for a deeper understanding of the physical aspects involved, and effective data representation approaches. Moreover, we conclude by highlighting the open problems in the field and analyzing different possible perspectives on how the community could move forward in this research area.","sentences":["This article reviews the recent advances in intelligent robotic ultrasound (US) imaging systems.","We commence by presenting the commonly employed robotic mechanisms and control techniques in robotic US imaging, along with their clinical applications.","Subsequently, we focus on the deployment of machine learning techniques in the development of robotic sonographers, emphasizing crucial developments aimed at enhancing the intelligence of these systems.","The methods for achieving autonomous action reasoning are categorized into two sets of approaches: those relying on implicit environmental data interpretation and those using explicit interpretation.","Throughout this exploration, we also discuss practical challenges, including those related to the scarcity of medical data, the need for a deeper understanding of the physical aspects involved, and effective data representation approaches.","Moreover, we conclude by highlighting the open problems in the field and analyzing different possible perspectives on how the community could move forward in this research area."],"url":"http://arxiv.org/abs/2401.02376v1"}
{"created":"2024-01-04 17:23:44","title":"SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded Entity Retrieval","abstract":"Clinician must write a lengthy summary each time a patient is discharged from the hospital. This task is time-consuming due to the sheer number of unique clinical concepts covered in the admission. Identifying and covering salient entities is vital for the summary to be clinically useful. We fine-tune open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\\b{eta}) on the task and find that they generate incomplete and unfaithful summaries. To increase entity coverage, we train a smaller, encoder-only model to predict salient entities, which are treated as content-plans to guide the LLM. To encourage the LLM to focus on specific mentions in the source notes, we propose SPEER: Sentence-level Planning via Embedded Entity Retrieval. Specifically, we mark each salient entity span with special \"{{ }}\" boundary tags and instruct the LLM to retrieve marked spans before generating each sentence. Sentence-level planning acts as a form of state tracking in that the model is explicitly recording the entities it uses. We fine-tune Mistral and Zephyr variants on a large-scale, diverse dataset of ~167k in-patient hospital admissions and evaluate on 3 datasets. SPEER shows gains in both coverage and faithfulness metrics over non-guided and guided baselines.","sentences":["Clinician must write a lengthy summary each time a patient is discharged from the hospital.","This task is time-consuming due to the sheer number of unique clinical concepts covered in the admission.","Identifying and covering salient entities is vital for the summary to be clinically useful.","We fine-tune open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\\b{eta}) on the task and find that they generate incomplete and unfaithful summaries.","To increase entity coverage, we train a smaller, encoder-only model to predict salient entities, which are treated as content-plans to guide the LLM.","To encourage the LLM to focus on specific mentions in the source notes, we propose SPEER:","Sentence-level Planning via Embedded Entity Retrieval.","Specifically, we mark each salient entity span with special \"{{ }}\" boundary tags and instruct the LLM to retrieve marked spans before generating each sentence.","Sentence-level planning acts as a form of state tracking in that the model is explicitly recording the entities it uses.","We fine-tune Mistral and Zephyr variants on a large-scale, diverse dataset of ~167k in-patient hospital admissions and evaluate on 3 datasets.","SPEER shows gains in both coverage and faithfulness metrics over non-guided and guided baselines."],"url":"http://arxiv.org/abs/2401.02369v1"}
{"created":"2024-01-04 17:01:54","title":"Integration of physics-informed operator learning and finite element method for parametric learning of partial differential equations","abstract":"We present a method that employs physics-informed deep learning techniques for parametrically solving partial differential equations. The focus is on the steady-state heat equations within heterogeneous solids exhibiting significant phase contrast. Similar equations manifest in diverse applications like chemical diffusion, electrostatics, and Darcy flow. The neural network aims to establish the link between the complex thermal conductivity profiles and temperature distributions, as well as heat flux components within the microstructure, under fixed boundary conditions. A distinctive aspect is our independence from classical solvers like finite element methods for data. A noteworthy contribution lies in our novel approach to defining the loss function, based on the discretized weak form of the governing equation. This not only reduces the required order of derivatives but also eliminates the need for automatic differentiation in the construction of loss terms, accepting potential numerical errors from the chosen discretization method. As a result, the loss function in this work is an algebraic equation that significantly enhances training efficiency. We benchmark our methodology against the standard finite element method, demonstrating accurate yet faster predictions using the trained neural network for temperature and flux profiles. We also show higher accuracy by using the proposed method compared to purely data-driven approaches for unforeseen scenarios.","sentences":["We present a method that employs physics-informed deep learning techniques for parametrically solving partial differential equations.","The focus is on the steady-state heat equations within heterogeneous solids exhibiting significant phase contrast.","Similar equations manifest in diverse applications like chemical diffusion, electrostatics, and Darcy flow.","The neural network aims to establish the link between the complex thermal conductivity profiles and temperature distributions, as well as heat flux components within the microstructure, under fixed boundary conditions.","A distinctive aspect is our independence from classical solvers like finite element methods for data.","A noteworthy contribution lies in our novel approach to defining the loss function, based on the discretized weak form of the governing equation.","This not only reduces the required order of derivatives but also eliminates the need for automatic differentiation in the construction of loss terms, accepting potential numerical errors from the chosen discretization method.","As a result, the loss function in this work is an algebraic equation that significantly enhances training efficiency.","We benchmark our methodology against the standard finite element method, demonstrating accurate yet faster predictions using the trained neural network for temperature and flux profiles.","We also show higher accuracy by using the proposed method compared to purely data-driven approaches for unforeseen scenarios."],"url":"http://arxiv.org/abs/2401.02363v1"}
{"created":"2024-01-04 17:01:40","title":"Historical Review of Fluid Antenna and Movable Antenna","abstract":"Recently, significant attention has been drawn to the development of two antenna technologies known as \"Fluid Antenna\" and \"Movable Antenna\" in wireless communication research community, owing to their flexibility and reconfigurability for improving the wireless system performance in various applications. However, some confusions/concerns have also ensued on their nomenclature. In fact, both \"Fluid Antenna\" and \"Movable Antenna\" are not newly-made terms, while they have a longstanding presence in the field of antenna technology. This article thus aims to review the historical evolution of these technologies for fostering a clear understanding of their origins and recent development in the realm of wireless communication. It is hoped that this article will help dispel any confusion, concern or even dispute on the appropriate use of their names in the literature and motivate more research endeavors to focus on resolving their technical issues in the future.","sentences":["Recently, significant attention has been drawn to the development of two antenna technologies known as \"Fluid Antenna\" and \"Movable Antenna\" in wireless communication research community, owing to their flexibility and reconfigurability for improving the wireless system performance in various applications.","However, some confusions/concerns have also ensued on their nomenclature.","In fact, both \"Fluid Antenna\" and \"Movable Antenna\" are not newly-made terms, while they have a longstanding presence in the field of antenna technology.","This article thus aims to review the historical evolution of these technologies for fostering a clear understanding of their origins and recent development in the realm of wireless communication.","It is hoped that this article will help dispel any confusion, concern or even dispute on the appropriate use of their names in the literature and motivate more research endeavors to focus on resolving their technical issues in the future."],"url":"http://arxiv.org/abs/2401.02362v1"}
{"created":"2024-01-04 17:00:49","title":"An Open and Comprehensive Pipeline for Unified Object Grounding and Detection","abstract":"Grounding-DINO is a state-of-the-art open-set detection model that tackles multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness has led to its widespread adoption as a mainstream architecture for various downstream applications. However, despite its significance, the original Grounding-DINO model lacks comprehensive public technical details due to the unavailability of its training code. To bridge this gap, we present MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline, which is built with the MMDetection toolbox. It adopts abundant vision datasets for pre-training and various detection and grounding datasets for fine-tuning. We give a comprehensive analysis of each reported result and detailed settings for reproduction. The extensive experiments on the benchmarks mentioned demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny baseline. We release all our models to the research community. Codes and trained models are released at https://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino.","sentences":["Grounding-DINO is a state-of-the-art open-set detection model that tackles multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase Grounding (PG), and Referring Expression Comprehension (REC).","Its effectiveness has led to its widespread adoption as a mainstream architecture for various downstream applications.","However, despite its significance, the original Grounding-DINO model lacks comprehensive public technical details due to the unavailability of its training code.","To bridge this gap, we present MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline, which is built with the MMDetection toolbox.","It adopts abundant vision datasets for pre-training and various detection and grounding datasets for fine-tuning.","We give a comprehensive analysis of each reported result and detailed settings for reproduction.","The extensive experiments on the benchmarks mentioned demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny baseline.","We release all our models to the research community.","Codes and trained models are released at https://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino."],"url":"http://arxiv.org/abs/2401.02361v1"}
{"created":"2024-01-04 16:57:56","title":"Fit-NGP: Fitting Object Models to Neural Graphics Primitives","abstract":"Accurate 3D object pose estimation is key to enabling many robotic applications that involve challenging object interactions. In this work, we show that the density field created by a state-of-the-art efficient radiance field reconstruction method is suitable for highly accurate and robust pose estimation for objects with known 3D models, even when they are very small and with challenging reflective surfaces. We present a fully automatic object pose estimation system based on a robot arm with a single wrist-mounted camera, which can scan a scene from scratch, detect and estimate the 6-Degrees of Freedom (DoF) poses of multiple objects within a couple of minutes of operation. Small objects such as bolts and nuts are estimated with accuracy on order of 1mm.","sentences":["Accurate 3D object pose estimation is key to enabling many robotic applications that involve challenging object interactions.","In this work, we show that the density field created by a state-of-the-art efficient radiance field reconstruction method is suitable for highly accurate and robust pose estimation for objects with known 3D models, even when they are very small and with challenging reflective surfaces.","We present a fully automatic object pose estimation system based on a robot arm with a single wrist-mounted camera, which can scan a scene from scratch, detect and estimate the 6-Degrees of Freedom (DoF) poses of multiple objects within a couple of minutes of operation.","Small objects such as bolts and nuts are estimated with accuracy on order of 1mm."],"url":"http://arxiv.org/abs/2401.02357v1"}
{"created":"2024-01-04 16:45:01","title":"A Survey Analyzing Generalization in Deep Reinforcement Learning","abstract":"Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces. While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to self driving vehicles, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies. In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and generalization capabilities. Furthermore, we will formalize and unify the diverse solution approaches to increase generalization, and overcome overfitting in state-action value functions. We believe our study can provide a compact systematic unified analysis for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with improved generalization abilities.","sentences":["Reinforcement learning research obtained significant success and attention with the utilization of deep neural networks to solve problems in high dimensional state or action spaces.","While deep reinforcement learning policies are currently being deployed in many different fields from medical applications to self driving vehicles, there are still ongoing questions the field is trying to answer on the generalization capabilities of deep reinforcement learning policies.","In this paper, we will outline the fundamental reasons why deep reinforcement learning policies encounter overfitting problems that limit their robustness and generalization capabilities.","Furthermore, we will formalize and unify the diverse solution approaches to increase generalization, and overcome overfitting in state-action value functions.","We believe our study can provide a compact systematic unified analysis for the current advancements in deep reinforcement learning, and help to construct robust deep neural policies with improved generalization abilities."],"url":"http://arxiv.org/abs/2401.02349v1"}
{"created":"2024-01-04 16:43:46","title":"Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via Text-Only Training","abstract":"Image captioning aims at generating descriptive and meaningful textual descriptions of images, enabling a broad range of vision-language applications. Prior works have demonstrated that harnessing the power of Contrastive Image Language Pre-training (CLIP) offers a promising approach to achieving zero-shot captioning, eliminating the need for expensive caption annotations. However, the widely observed modality gap in the latent space of CLIP harms the performance of zero-shot captioning by breaking the alignment between paired image-text features. To address this issue, we conduct an analysis on the CLIP latent space which leads to two findings. Firstly, we observe that the CLIP's visual feature of image subregions can achieve closer proximity to the paired caption due to the inherent information loss in text descriptions. In addition, we show that the modality gap between a paired image-text can be empirically modeled as a zero-mean Gaussian distribution. Motivated by the findings, we propose a novel zero-shot image captioning framework with text-only training to reduce the modality gap. In particular, we introduce a subregion feature aggregation to leverage local region information, which produces a compact visual representation for matching text representation. Moreover, we incorporate a noise injection and CLIP reranking strategy to boost captioning performance. We also extend our framework to build a zero-shot VQA pipeline, demonstrating its generality. Through extensive experiments on common captioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that our method achieves remarkable performance improvements. Code is available at https://github.com/Artanic30/MacCap.","sentences":["Image captioning aims at generating descriptive and meaningful textual descriptions of images, enabling a broad range of vision-language applications.","Prior works have demonstrated that harnessing the power of Contrastive Image Language Pre-training (CLIP) offers a promising approach to achieving zero-shot captioning, eliminating the need for expensive caption annotations.","However, the widely observed modality gap in the latent space of CLIP harms the performance of zero-shot captioning by breaking the alignment between paired image-text features.","To address this issue, we conduct an analysis on the CLIP latent space which leads to two findings.","Firstly, we observe that the CLIP's visual feature of image subregions can achieve closer proximity to the paired caption due to the inherent information loss in text descriptions.","In addition, we show that the modality gap between a paired image-text can be empirically modeled as a zero-mean Gaussian distribution.","Motivated by the findings, we propose a novel zero-shot image captioning framework with text-only training to reduce the modality gap.","In particular, we introduce a subregion feature aggregation to leverage local region information, which produces a compact visual representation for matching text representation.","Moreover, we incorporate a noise injection and CLIP reranking strategy to boost captioning performance.","We also extend our framework to build a zero-shot VQA pipeline, demonstrating its generality.","Through extensive experiments on common captioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that our method achieves remarkable performance improvements.","Code is available at https://github.com/Artanic30/MacCap."],"url":"http://arxiv.org/abs/2401.02347v1"}
{"created":"2024-01-04 16:38:47","title":"Multi-Source Domain Adaptation with Transformer-based Feature Generation for Subject-Independent EEG-based Emotion Recognition","abstract":"Although deep learning-based algorithms have demonstrated excellent performance in automated emotion recognition via electroencephalogram (EEG) signals, variations across brain signal patterns of individuals can diminish the model's effectiveness when applied across different subjects. While transfer learning techniques have exhibited promising outcomes, they still encounter challenges related to inadequate feature representations and may overlook the fact that source subjects themselves can possess distinct characteristics. In this work, we propose a multi-source domain adaptation approach with a transformer-based feature generator (MSDA-TF) designed to leverage information from multiple sources. The proposed feature generator retains convolutional layers to capture shallow spatial, temporal, and spectral EEG data representations, while self-attention mechanisms extract global dependencies within these features. During the adaptation process, we group the source subjects based on correlation values and aim to align the moments of the target subject with each source as well as within the sources. MSDA-TF is validated on the SEED dataset and is shown to yield promising results.","sentences":["Although deep learning-based algorithms have demonstrated excellent performance in automated emotion recognition via electroencephalogram (EEG) signals, variations across brain signal patterns of individuals can diminish the model's effectiveness when applied across different subjects.","While transfer learning techniques have exhibited promising outcomes, they still encounter challenges related to inadequate feature representations and may overlook the fact that source subjects themselves can possess distinct characteristics.","In this work, we propose a multi-source domain adaptation approach with a transformer-based feature generator (MSDA-TF) designed to leverage information from multiple sources.","The proposed feature generator retains convolutional layers to capture shallow spatial, temporal, and spectral EEG data representations, while self-attention mechanisms extract global dependencies within these features.","During the adaptation process, we group the source subjects based on correlation values and aim to align the moments of the target subject with each source as well as within the sources.","MSDA-TF is validated on the SEED dataset and is shown to yield promising results."],"url":"http://arxiv.org/abs/2401.02344v1"}
{"created":"2024-01-04 16:34:27","title":"AERIAL-CORE: AI-Powered Aerial Robots for Inspection and Maintenance of Electrical Power Infrastructures","abstract":"Large-scale infrastructures are prone to deterioration due to age, environmental influences, and heavy usage. Ensuring their safety through regular inspections and maintenance is crucial to prevent incidents that can significantly affect public safety and the environment. This is especially pertinent in the context of electrical power networks, which, while essential for energy provision, can also be sources of forest fires. Intelligent drones have the potential to revolutionize inspection and maintenance, eliminating the risks for human operators, increasing productivity, reducing inspection time, and improving data collection quality. However, most of the current methods and technologies in aerial robotics have been trialed primarily in indoor testbeds or outdoor settings under strictly controlled conditions, always within the line of sight of human operators. Additionally, these methods and technologies have typically been evaluated in isolation, lacking comprehensive integration. This paper introduces the first autonomous system that combines various innovative aerial robots. This system is designed for extended-range inspections beyond the visual line of sight, features aerial manipulators for maintenance tasks, and includes support mechanisms for human operators working at elevated heights. The paper further discusses the successful validation of this system on numerous electrical power lines, with aerial robots executing flights over 10 kilometers away from their ground control stations.","sentences":["Large-scale infrastructures are prone to deterioration due to age, environmental influences, and heavy usage.","Ensuring their safety through regular inspections and maintenance is crucial to prevent incidents that can significantly affect public safety and the environment.","This is especially pertinent in the context of electrical power networks, which, while essential for energy provision, can also be sources of forest fires.","Intelligent drones have the potential to revolutionize inspection and maintenance, eliminating the risks for human operators, increasing productivity, reducing inspection time, and improving data collection quality.","However, most of the current methods and technologies in aerial robotics have been trialed primarily in indoor testbeds or outdoor settings under strictly controlled conditions, always within the line of sight of human operators.","Additionally, these methods and technologies have typically been evaluated in isolation, lacking comprehensive integration.","This paper introduces the first autonomous system that combines various innovative aerial robots.","This system is designed for extended-range inspections beyond the visual line of sight, features aerial manipulators for maintenance tasks, and includes support mechanisms for human operators working at elevated heights.","The paper further discusses the successful validation of this system on numerous electrical power lines, with aerial robots executing flights over 10 kilometers away from their ground control stations."],"url":"http://arxiv.org/abs/2401.02343v1"}
{"created":"2024-01-04 16:28:15","title":"Evasive Hardware Trojan through Adversarial Power Trace","abstract":"The globalization of the Integrated Circuit (IC) supply chain, driven by time-to-market and cost considerations, has made ICs vulnerable to hardware Trojans (HTs). Against this threat, a promising approach is to use Machine Learning (ML)-based side-channel analysis, which has the advantage of being a non-intrusive method, along with efficiently detecting HTs under golden chip-free settings. In this paper, we question the trustworthiness of ML-based HT detection via side-channel analysis. We introduce a HT obfuscation (HTO) approach to allow HTs to bypass this detection method. Rather than theoretically misleading the model by simulated adversarial traces, a key aspect of our approach is the design and implementation of adversarial noise as part of the circuitry, alongside the HT. We detail HTO methodologies for ASICs and FPGAs, and evaluate our approach using TrustHub benchmark. Interestingly, we found that HTO can be implemented with only a single transistor for ASIC designs to generate adversarial power traces that can fool the defense with 100% efficiency. We also efficiently implemented our approach on a Spartan 6 Xilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii) ring-oscillator-based design. Additionally, we assess the efficiency of countermeasures like spectral domain analysis, and we show that an adaptive attacker can still design evasive HTOs by constraining the design with a spectral noise budget. In addition, while adversarial training (AT) offers higher protection against evasive HTs, AT models suffer from a considerable utility loss, potentially rendering them unsuitable for such security application. We believe this research represents a significant step in understanding and exploiting ML vulnerabilities in a hardware security context, and we make all resources and designs openly available online: https://dev.d18uu4lqwhbmka.amplifyapp.com","sentences":["The globalization of the Integrated Circuit (IC) supply chain, driven by time-to-market and cost considerations, has made ICs vulnerable to hardware Trojans (HTs).","Against this threat, a promising approach is to use Machine Learning (ML)-based side-channel analysis, which has the advantage of being a non-intrusive method, along with efficiently detecting HTs under golden chip-free settings.","In this paper, we question the trustworthiness of ML-based HT detection via side-channel analysis.","We introduce a HT obfuscation (HTO) approach to allow HTs to bypass this detection method.","Rather than theoretically misleading the model by simulated adversarial traces, a key aspect of our approach is the design and implementation of adversarial noise as part of the circuitry, alongside the HT.","We detail HTO methodologies for ASICs and FPGAs, and evaluate our approach using TrustHub benchmark.","Interestingly, we found that HTO can be implemented with only a single transistor for ASIC designs to generate adversarial power traces that can fool the defense with 100% efficiency.","We also efficiently implemented our approach on a Spartan 6 Xilinx FPGA using 2 different variants: (i) DSP slices-based, and (ii) ring-oscillator-based design.","Additionally, we assess the efficiency of countermeasures like spectral domain analysis, and we show that an adaptive attacker can still design evasive HTOs by constraining the design with a spectral noise budget.","In addition, while adversarial training (AT) offers higher protection against evasive HTs, AT models suffer from a considerable utility loss, potentially rendering them unsuitable for such security application.","We believe this research represents a significant step in understanding and exploiting ML vulnerabilities in a hardware security context, and we make all resources and designs openly available online: https://dev.d18uu4lqwhbmka.amplifyapp.com"],"url":"http://arxiv.org/abs/2401.02342v1"}
{"created":"2024-01-04 16:26:21","title":"How Do Pedestrians' Perception Change toward Autonomous Vehicles during Unmarked Midblock Multilane Crossings: Role of AV Operation and Signal Indication","abstract":"One of the primary impediments hindering the widespread acceptance of autonomous vehicles (AVs) among pedestrians is their limited comprehension of AVs. This study employs virtual reality (VR) to provide pedestrians with an immersive environment for engaging with and comprehending AVs during unmarked midblock multilane crossings. Diverse AV driving behaviors were modeled to exhibit negotiation behavior with a yellow signal indication or non-yielding behavior with a blue signal indication. This paper aims to investigate the impact of various factors, such as AV behavior and signaling, pedestrian past behavior, etc., on pedestrians' perception change of AVs. Before and after the VR experiment, participants completed surveys assessing their perception of AVs, focusing on two main aspects: \"Attitude\" and \"System Effectiveness.\" The Wilcoxon signed-rank test results demonstrated that both pedestrians' overall attitude score toward AVs and trust in the effectiveness of AV systems significantly increased following the VR experiment. Notably, individuals who exhibited a greater trust in the yellow signals were more inclined to display a higher attitude score toward AVs and to augment their trust in the effectiveness of AV systems. This indicates that the design of the yellow signal instills pedestrians with greater confidence in their interactions with AVs. Further, pedestrians who exhibit more aggressive crossing behavior are less likely to change their perception towards AVs as compared to those pedestrians with more positive crossing behaviors. It is concluded that integrating this paper's devised AV behavior and signaling within an immersive VR setting facilitated pedestrian engagement with AVs, thereby changing their perception of AVs.","sentences":["One of the primary impediments hindering the widespread acceptance of autonomous vehicles (AVs) among pedestrians is their limited comprehension of AVs.","This study employs virtual reality (VR) to provide pedestrians with an immersive environment for engaging with and comprehending AVs during unmarked midblock multilane crossings.","Diverse AV driving behaviors were modeled to exhibit negotiation behavior with a yellow signal indication or non-yielding behavior with a blue signal indication.","This paper aims to investigate the impact of various factors, such as AV behavior and signaling, pedestrian past behavior, etc., on pedestrians' perception change of AVs.","Before and after the VR experiment, participants completed surveys assessing their perception of AVs, focusing on two main aspects: \"Attitude\" and \"System Effectiveness.\"","The Wilcoxon signed-rank test results demonstrated that both pedestrians' overall attitude score toward AVs and trust in the effectiveness of AV systems significantly increased following the VR experiment.","Notably, individuals who exhibited a greater trust in the yellow signals were more inclined to display a higher attitude score toward AVs and to augment their trust in the effectiveness of AV systems.","This indicates that the design of the yellow signal instills pedestrians with greater confidence in their interactions with AVs.","Further, pedestrians who exhibit more aggressive crossing behavior are less likely to change their perception towards AVs as compared to those pedestrians with more positive crossing behaviors.","It is concluded that integrating this paper's devised AV behavior and signaling within an immersive VR setting facilitated pedestrian engagement with AVs, thereby changing their perception of AVs."],"url":"http://arxiv.org/abs/2401.02339v1"}
{"created":"2024-01-04 16:19:52","title":"Linguistic Profiling of Deepfakes: An Open Database for Next-Generation Deepfake Detection","abstract":"The emergence of text-to-image generative models has revolutionized the field of deepfakes, enabling the creation of realistic and convincing visual content directly from textual descriptions. However, this advancement presents considerably greater challenges in detecting the authenticity of such content. Existing deepfake detection datasets and methods often fall short in effectively capturing the extensive range of emerging deepfakes and offering satisfactory explanatory information for detection. To address the significant issue, this paper introduces a deepfake database (DFLIP-3K) for the development of convincing and explainable deepfake detection. It encompasses about 300K diverse deepfake samples from approximately 3K generative models, which boasts the largest number of deepfake models in the literature. Moreover, it collects around 190K linguistic footprints of these deepfakes. The two distinguished features enable DFLIP-3K to develop a benchmark that promotes progress in linguistic profiling of deepfakes, which includes three sub-tasks namely deepfake detection, model identification, and prompt prediction. The deepfake model and prompt are two essential components of each deepfake, and thus dissecting them linguistically allows for an invaluable exploration of trustworthy and interpretable evidence in deepfake detection, which we believe is the key for the next-generation deepfake detection. Furthermore, DFLIP-3K is envisioned as an open database that fosters transparency and encourages collaborative efforts to further enhance its growth. Our extensive experiments on the developed benchmark verify that our DFLIP-3K database is capable of serving as a standardized resource for evaluating and comparing linguistic-based deepfake detection, identification, and prompt prediction techniques.","sentences":["The emergence of text-to-image generative models has revolutionized the field of deepfakes, enabling the creation of realistic and convincing visual content directly from textual descriptions.","However, this advancement presents considerably greater challenges in detecting the authenticity of such content.","Existing deepfake detection datasets and methods often fall short in effectively capturing the extensive range of emerging deepfakes and offering satisfactory explanatory information for detection.","To address the significant issue, this paper introduces a deepfake database (DFLIP-3K) for the development of convincing and explainable deepfake detection.","It encompasses about 300K diverse deepfake samples from approximately 3K generative models, which boasts the largest number of deepfake models in the literature.","Moreover, it collects around 190K linguistic footprints of these deepfakes.","The two distinguished features enable DFLIP-3K to develop a benchmark that promotes progress in linguistic profiling of deepfakes, which includes three sub-tasks namely deepfake detection, model identification, and prompt prediction.","The deepfake model and prompt are two essential components of each deepfake, and thus dissecting them linguistically allows for an invaluable exploration of trustworthy and interpretable evidence in deepfake detection, which we believe is the key for the next-generation deepfake detection.","Furthermore, DFLIP-3K is envisioned as an open database that fosters transparency and encourages collaborative efforts to further enhance its growth.","Our extensive experiments on the developed benchmark verify that our DFLIP-3K database is capable of serving as a standardized resource for evaluating and comparing linguistic-based deepfake detection, identification, and prompt prediction techniques."],"url":"http://arxiv.org/abs/2401.02335v1"}
{"created":"2024-01-04 16:16:14","title":"Beyond Extraction: Contextualising Tabular Data for Efficient Summarisation by Language Models","abstract":"The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents. However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems. Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately. The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values. To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture. Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt. This enriched data is then fed into the retrieval database alongside other PDFs. Our approach aims to significantly improve the precision of complex table queries, offering a promising solution to a longstanding challenge in information retrieval.","sentences":["The conventional use of the Retrieval-Augmented Generation (RAG) architecture has proven effective for retrieving information from diverse documents.","However, challenges arise in handling complex table queries, especially within PDF documents containing intricate tabular structures.","This research introduces an innovative approach to enhance the accuracy of complex table queries in RAG-based systems.","Our methodology involves storing PDFs in the retrieval database and extracting tabular content separately.","The extracted tables undergo a process of context enrichment, concatenating headers with corresponding values.","To ensure a comprehensive understanding of the enriched data, we employ a fine-tuned version of the Llama-2-chat language model for summarisation within the RAG architecture.","Furthermore, we augment the tabular data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.","This enriched data is then fed into the retrieval database alongside other PDFs.","Our approach aims to significantly improve the precision of complex table queries, offering a promising solution to a longstanding challenge in information retrieval."],"url":"http://arxiv.org/abs/2401.02333v1"}
{"created":"2024-01-04 16:07:43","title":"LLaVA-$\u03c6$: Efficient Multi-Modal Assistant with Small Language Model","abstract":"In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models. It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora. Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception. Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents. It highlights the potential of smaller language models to achieve sophisticated levels of understanding and interaction, while maintaining greater resource efficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.","sentences":["In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient multi-modal assistant that harnesses the power of the recently advanced small language model, Phi-2, to facilitate multi-modal dialogues.","LLaVA-Phi marks a notable advancement in the realm of compact multi-modal models.","It demonstrates that even smaller language models, with as few as 2.7B parameters, can effectively engage in intricate dialogues that integrate both textual and visual elements, provided they are trained with high-quality corpora.","Our model delivers commendable performance on publicly available benchmarks that encompass visual comprehension, reasoning, and knowledge-based perception.","Beyond its remarkable performance in multi-modal dialogue tasks, our model opens new avenues for applications in time-sensitive environments and systems that require real-time interaction, such as embodied agents.","It highlights the potential of smaller language models to achieve sophisticated levels of understanding and interaction, while maintaining greater resource efficiency.","The project is available at {https://github.com/zhuyiche/llava-phi}."],"url":"http://arxiv.org/abs/2401.02330v1"}
{"created":"2024-01-04 16:06:31","title":"Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning","abstract":"Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning. Substantial efforts have been devoted to addressing the heterogeneity in local label distribution. As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training. Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized. This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously. Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model. Moreover, logit suppression directly penalizes network logits for non-label classes, effectively addressing misclassifications in minority classes that may be biased toward majority classes. Extensive experiments validate the efficacy of FedED, surpassing previous state-of-the-art methods across diverse datasets with varying degrees of label distribution shift.","sentences":["Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning.","Substantial efforts have been devoted to addressing the heterogeneity in local label distribution.","As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training.","Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized.","This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously.","Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model.","Moreover, logit suppression directly penalizes network logits for non-label classes, effectively addressing misclassifications in minority classes that may be biased toward majority classes.","Extensive experiments validate the efficacy of FedED, surpassing previous state-of-the-art methods across diverse datasets with varying degrees of label distribution shift."],"url":"http://arxiv.org/abs/2401.02329v1"}
{"created":"2024-01-04 15:54:45","title":"ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment Anything to SAR Domain for Semantic Segmentation","abstract":"In the realm of artificial intelligence, the emergence of foundation models, backed by high computing capabilities and extensive data, has been revolutionary. Segment Anything Model (SAM), built on the Vision Transformer (ViT) model with millions of parameters and vast training dataset SA-1B, excels in various segmentation scenarios relying on its significance of semantic information and generalization ability. Such achievement of visual foundation model stimulates continuous researches on specific downstream tasks in computer vision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the high-performing SAM for landcover classification on space-borne Synthetic Aperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's parameters and incorporates lightweight adapters for parameter efficient fine-tuning, and a classwise mask decoder is designed to achieve semantic segmentation task. This adapt-tuning method allows for efficient landcover classification of SAR images, balancing the accuracy with computational demand. In addition, the task specific input module injects low frequency information of SAR images by MLP-based layers to improve the model performance. Compared to conventional state-of-the-art semantic segmentation algorithms by extensive experiments, CWSAM showcases enhanced performance with fewer computing resources, highlighting the potential of leveraging foundational models like SAM for specific downstream tasks in the SAR domain. The source code is available at: https://github.com/xypu98/CWSAM.","sentences":["In the realm of artificial intelligence, the emergence of foundation models, backed by high computing capabilities and extensive data, has been revolutionary.","Segment Anything Model (SAM), built on the Vision Transformer (ViT) model with millions of parameters and vast training dataset SA-1B, excels in various segmentation scenarios relying on its significance of semantic information and generalization ability.","Such achievement of visual foundation model stimulates continuous researches on specific downstream tasks in computer vision.","The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the high-performing SAM for landcover classification on space-borne Synthetic Aperture Radar (SAR) images.","The proposed CWSAM freezes most of SAM's parameters and incorporates lightweight adapters for parameter efficient fine-tuning, and a classwise mask decoder is designed to achieve semantic segmentation task.","This adapt-tuning method allows for efficient landcover classification of SAR images, balancing the accuracy with computational demand.","In addition, the task specific input module injects low frequency information of SAR images by MLP-based layers to improve the model performance.","Compared to conventional state-of-the-art semantic segmentation algorithms by extensive experiments, CWSAM showcases enhanced performance with fewer computing resources, highlighting the potential of leveraging foundational models like SAM for specific downstream tasks in the SAR domain.","The source code is available at: https://github.com/xypu98/CWSAM."],"url":"http://arxiv.org/abs/2401.02326v1"}
{"created":"2024-01-04 15:51:49","title":"A Robust Quantile Huber Loss With Interpretable Parameter Adjustment In Distributional Reinforcement Learning","abstract":"Distributional Reinforcement Learning (RL) estimates return distribution mainly by learning quantile values via minimizing the quantile Huber loss function, entailing a threshold parameter often selected heuristically or via hyperparameter search, which may not generalize well and can be suboptimal. This paper introduces a generalized quantile Huber loss function derived from Wasserstein distance (WD) calculation between Gaussian distributions, capturing noise in predicted (current) and target (Bellman-updated) quantile values. Compared to the classical quantile Huber loss, this innovative loss function enhances robustness against outliers. Notably, the classical Huber loss function can be seen as an approximation of our proposed loss, enabling parameter adjustment by approximating the amount of noise in the data during the learning process. Empirical tests on Atari games, a common application in distributional RL, and a recent hedging strategy using distributional RL, validate the effectiveness of our proposed loss function and its potential for parameter adjustments in distributional RL.","sentences":["Distributional Reinforcement Learning (RL) estimates return distribution mainly by learning quantile values via minimizing the quantile Huber loss function, entailing a threshold parameter often selected heuristically or via hyperparameter search, which may not generalize well and can be suboptimal.","This paper introduces a generalized quantile Huber loss function derived from Wasserstein distance (WD) calculation between Gaussian distributions, capturing noise in predicted (current) and target (Bellman-updated) quantile values.","Compared to the classical quantile Huber loss, this innovative loss function enhances robustness against outliers.","Notably, the classical Huber loss function can be seen as an approximation of our proposed loss, enabling parameter adjustment by approximating the amount of noise in the data during the learning process.","Empirical tests on Atari games, a common application in distributional RL, and a recent hedging strategy using distributional RL, validate the effectiveness of our proposed loss function and its potential for parameter adjustments in distributional RL."],"url":"http://arxiv.org/abs/2401.02325v1"}
{"created":"2024-01-04 15:34:44","title":"BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model","abstract":"In this paper, we address the challenge of image resolution variation for the Segment Anything Model (SAM). SAM, known for its zero-shot generalizability, exhibits a performance degradation when faced with datasets with varying image sizes. Previous approaches tend to resize the image to a fixed size or adopt structure modifications, hindering the preservation of SAM's rich prior knowledge. Besides, such task-specific tuning necessitates a complete retraining of the model, which is cost-expensive and unacceptable for deployment in the downstream tasks. In this paper, we reformulate this issue as a length extrapolation problem, where token sequence length varies while maintaining a consistent patch size for images of different sizes. To this end, we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's adaptability to varying image resolutions while eliminating the need for structure modifications. Firstly, we introduce a new scaling factor to ensure consistent magnitude in the attention layer's dot product values when the token sequence length changes. Secondly, we present a bias-mode attention mask that allows each token to prioritize neighboring information, mitigating the impact of untrained distant information. Our BA-SAM demonstrates efficacy in two scenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets, including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to significantly mitigate performance degradation in the zero-shot setting and achieve state-of-the-art performance with minimal fine-tuning. Furthermore, we propose a generalized model and benchmark, showcasing BA-SAM's generalizability across all four datasets simultaneously.","sentences":["In this paper, we address the challenge of image resolution variation for the Segment Anything Model (SAM).","SAM, known for its zero-shot generalizability, exhibits a performance degradation when faced with datasets with varying image sizes.","Previous approaches tend to resize the image to a fixed size or adopt structure modifications, hindering the preservation of SAM's rich prior knowledge.","Besides, such task-specific tuning necessitates a complete retraining of the model, which is cost-expensive and unacceptable for deployment in the downstream tasks.","In this paper, we reformulate this issue as a length extrapolation problem, where token sequence length varies while maintaining a consistent patch size for images of different sizes.","To this end, we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's adaptability to varying image resolutions while eliminating the need for structure modifications.","Firstly, we introduce a new scaling factor to ensure consistent magnitude in the attention layer's dot product values when the token sequence length changes.","Secondly, we present a bias-mode attention mask that allows each token to prioritize neighboring information, mitigating the impact of untrained distant information.","Our BA-SAM demonstrates efficacy in two scenarios: zero-shot and fine-tuning.","Extensive evaluation on diverse datasets, including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to significantly mitigate performance degradation in the zero-shot setting and achieve state-of-the-art performance with minimal fine-tuning.","Furthermore, we propose a generalized model and benchmark, showcasing BA-SAM's generalizability across all four datasets simultaneously."],"url":"http://arxiv.org/abs/2401.02317v1"}
{"created":"2024-01-04 15:21:53","title":"SuperEdge: Towards a Generalization Model for Self-Supervised Edge Detection","abstract":"Edge detection is a fundamental technique in various computer vision tasks. Edges are indeed effectively delineated by pixel discontinuity and can offer reliable structural information even in textureless areas. State-of-the-art heavily relies on pixel-wise annotations, which are labor-intensive and subject to inconsistencies when acquired manually. In this work, we propose a novel self-supervised approach for edge detection that employs a multi-level, multi-homography technique to transfer annotations from synthetic to real-world datasets. To fully leverage the generated edge annotations, we developed SuperEdge, a streamlined yet efficient model capable of concurrently extracting edges at pixel-level and object-level granularity. Thanks to self-supervised training, our method eliminates the dependency on manual annotated edge labels, thereby enhancing its generalizability across diverse datasets. Comparative evaluations reveal that SuperEdge advances edge detection, demonstrating improvements of 4.9% in ODS and 3.3% in OIS over the existing STEdge method on BIPEDv2.","sentences":["Edge detection is a fundamental technique in various computer vision tasks.","Edges are indeed effectively delineated by pixel discontinuity and can offer reliable structural information even in textureless areas.","State-of-the-art heavily relies on pixel-wise annotations, which are labor-intensive and subject to inconsistencies when acquired manually.","In this work, we propose a novel self-supervised approach for edge detection that employs a multi-level, multi-homography technique to transfer annotations from synthetic to real-world datasets.","To fully leverage the generated edge annotations, we developed SuperEdge, a streamlined yet efficient model capable of concurrently extracting edges at pixel-level and object-level granularity.","Thanks to self-supervised training, our method eliminates the dependency on manual annotated edge labels, thereby enhancing its generalizability across diverse datasets.","Comparative evaluations reveal that SuperEdge advances edge detection, demonstrating improvements of 4.9% in ODS and 3.3% in OIS over the existing STEdge method on BIPEDv2."],"url":"http://arxiv.org/abs/2401.02313v1"}
{"created":"2024-01-04 14:55:57","title":"TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection","abstract":"Video moment retrieval (MR) and highlight detection (HD) based on natural language queries are two highly related tasks, which aim to obtain relevant moments within videos and highlight scores of each video clip. Recently, several methods have been devoted to building DETR-based networks to solve both MR and HD jointly. These methods simply add two separate task heads after multi-modal feature extraction and feature interaction, achieving good performance. Nevertheless, these approaches underutilize the reciprocal relationship between two tasks. In this paper, we propose a task-reciprocal transformer based on DETR (TR-DETR) that focuses on exploring the inherent reciprocity between MR and HD. Specifically, a local-global multi-modal alignment module is first built to align features from diverse modalities into a shared latent space. Subsequently, a visual feature refinement is designed to eliminate query-irrelevant information from visual features for modal interaction. Finally, a task cooperation module is constructed to refine the retrieval pipeline and the highlight score prediction process by utilizing the reciprocity between MR and HD. Comprehensive experiments on QVHighlights, Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing state-of-the-art methods. Codes are available at \\url{https://github.com/mingyao1120/TR-DETR}.","sentences":["Video moment retrieval (MR) and highlight detection (HD) based on natural language queries are two highly related tasks, which aim to obtain relevant moments within videos and highlight scores of each video clip.","Recently, several methods have been devoted to building DETR-based networks to solve both MR and HD jointly.","These methods simply add two separate task heads after multi-modal feature extraction and feature interaction, achieving good performance.","Nevertheless, these approaches underutilize the reciprocal relationship between two tasks.","In this paper, we propose a task-reciprocal transformer based on DETR (TR-DETR) that focuses on exploring the inherent reciprocity between MR and HD.","Specifically, a local-global multi-modal alignment module is first built to align features from diverse modalities into a shared latent space.","Subsequently, a visual feature refinement is designed to eliminate query-irrelevant information from visual features for modal interaction.","Finally, a task cooperation module is constructed to refine the retrieval pipeline and the highlight score prediction process by utilizing the reciprocity between MR and HD.","Comprehensive experiments on QVHighlights, Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing state-of-the-art methods.","Codes are available at \\url{https://github.com/mingyao1120/TR-DETR}."],"url":"http://arxiv.org/abs/2401.02309v1"}
{"created":"2024-01-04 14:42:29","title":"Robust Physics Informed Neural Networks","abstract":"We introduce a Robust version of the Physics-Informed Neural Networks (RPINNs) to approximate the Partial Differential Equations (PDEs) solution. Standard Physics Informed Neural Networks (PINN) takes into account the governing physical laws described by PDE during the learning process. The network is trained on a data set that consists of randomly selected points in the physical domain and its boundary. PINNs have been successfully applied to solve various problems described by PDEs with boundary conditions. The loss function in traditional PINNs is based on the strong residuals of the PDEs. This loss function in PINNs is generally not robust with respect to the true error. The loss function in PINNs can be far from the true error, which makes the training process more difficult. In particular, we do not know if the training process has already converged to the solution with the required accuracy. This is especially true if we do not know the exact solution, so we cannot estimate the true error during the training. This paper introduces a different way of defining the loss function. It incorporates the residual and the inverse of the Gram matrix, computed using the energy norm. We test our RPINN algorithm on two Laplace problems and one advection-diffusion problem in two spatial dimensions. We conclude that RPINN is a robust method. The proposed loss coincides well with the true error of the solution, as measured in the energy norm. Thus, we know if our training process goes well, and we know when to stop the training to obtain the neural network approximation of the solution of the PDE with the true error of required accuracy.","sentences":["We introduce a Robust version of the Physics-Informed Neural Networks (RPINNs) to approximate the Partial Differential Equations (PDEs) solution.","Standard Physics Informed Neural Networks (PINN) takes into account the governing physical laws described by PDE during the learning process.","The network is trained on a data set that consists of randomly selected points in the physical domain and its boundary.","PINNs have been successfully applied to solve various problems described by PDEs with boundary conditions.","The loss function in traditional PINNs is based on the strong residuals of the PDEs.","This loss function in PINNs is generally not robust with respect to the true error.","The loss function in PINNs can be far from the true error, which makes the training process more difficult.","In particular, we do not know if the training process has already converged to the solution with the required accuracy.","This is especially true if we do not know the exact solution, so we cannot estimate the true error during the training.","This paper introduces a different way of defining the loss function.","It incorporates the residual and the inverse of the Gram matrix, computed using the energy norm.","We test our RPINN algorithm on two Laplace problems and one advection-diffusion problem in two spatial dimensions.","We conclude that RPINN is a robust method.","The proposed loss coincides well with the true error of the solution, as measured in the energy norm.","Thus, we know if our training process goes well, and we know when to stop the training to obtain the neural network approximation of the solution of the PDE with the true error of required accuracy."],"url":"http://arxiv.org/abs/2401.02300v1"}
{"created":"2024-01-04 14:36:38","title":"Are LLMs Robust for Spoken Dialogues?","abstract":"Large Pre-Trained Language Models have demonstrated state-of-the-art performance in different downstream tasks, including dialogue state tracking and end-to-end response generation. Nevertheless, most of the publicly available datasets and benchmarks on task-oriented dialogues focus on written conversations. Consequently, the robustness of the developed models to spoken interactions is unknown. In this work, we have evaluated the performance of LLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to the lack of proper spoken dialogue datasets, we have automatically transcribed a development set of spoken dialogues with a state-of-the-art ASR engine. We have characterized the ASR-error types and their distributions and simulated these errors in a large dataset of dialogues. We report the intrinsic (perplexity) and extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models in two subtasks of response generation and dialogue state tracking, respectively. The results show that LLMs are not robust to spoken noise by default, however, fine-tuning/training such models on a proper dataset of spoken TODs can result in a more robust performance.","sentences":["Large Pre-Trained Language Models have demonstrated state-of-the-art performance in different downstream tasks, including dialogue state tracking and end-to-end response generation.","Nevertheless, most of the publicly available datasets and benchmarks on task-oriented dialogues focus on written conversations.","Consequently, the robustness of the developed models to spoken interactions is unknown.","In this work, we have evaluated the performance of LLMs for spoken task-oriented dialogues on the DSTC11 test sets.","Due to the lack of proper spoken dialogue datasets, we have automatically transcribed a development set of spoken dialogues with a state-of-the-art ASR engine.","We have characterized the ASR-error types and their distributions and simulated these errors in a large dataset of dialogues.","We report the intrinsic (perplexity) and extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models in two subtasks of response generation and dialogue state tracking, respectively.","The results show that LLMs are not robust to spoken noise by default, however, fine-tuning/training such models on a proper dataset of spoken TODs can result in a more robust performance."],"url":"http://arxiv.org/abs/2401.02297v1"}
{"created":"2024-01-04 14:34:58","title":"Training Single-Layer Morphological Perceptron Using Convex-Concave Programming","abstract":"This paper concerns the training of a single-layer morphological perceptron using disciplined convex-concave programming (DCCP). We introduce an algorithm referred to as K-DDCCP, which combines the existing single-layer morphological perceptron (SLMP) model proposed by Ritter and Urcid with the weighted disciplined convex-concave programming (WDCCP) algorithm by Charisopoulos and Maragos. The proposed training algorithm leverages the disciplined convex-concave procedure (DCCP) and formulates a non-convex optimization problem for binary classification. To tackle this problem, the constraints are expressed as differences of convex functions, enabling the application of the DCCP package. The experimental results confirm the effectiveness of the K-DDCCP algorithm in solving binary classification problems. Overall, this work contributes to the field of morphological neural networks by proposing an algorithm that extends the capabilities of the SLMP model.","sentences":["This paper concerns the training of a single-layer morphological perceptron using disciplined convex-concave programming (DCCP).","We introduce an algorithm referred to as K-DDCCP, which combines the existing single-layer morphological perceptron (SLMP) model proposed by Ritter and Urcid with the weighted disciplined convex-concave programming (WDCCP) algorithm by Charisopoulos and Maragos.","The proposed training algorithm leverages the disciplined convex-concave procedure (DCCP) and formulates a non-convex optimization problem for binary classification.","To tackle this problem, the constraints are expressed as differences of convex functions, enabling the application of the DCCP package.","The experimental results confirm the effectiveness of the K-DDCCP algorithm in solving binary classification problems.","Overall, this work contributes to the field of morphological neural networks by proposing an algorithm that extends the capabilities of the SLMP model."],"url":"http://arxiv.org/abs/2401.02296v1"}
{"created":"2024-01-04 14:31:56","title":"GridFormer: Point-Grid Transformer for Surface Reconstruction","abstract":"Implicit neural networks have emerged as a crucial technology in 3D surface reconstruction. To reconstruct continuous surfaces from discrete point clouds, encoding the input points into regular grid features (plane or volume) has been commonly employed in existing approaches. However, these methods typically use the grid as an index for uniformly scattering point features. Compared with the irregular point features, the regular grid features may sacrifice some reconstruction details but improve efficiency. To take full advantage of these two types of features, we introduce a novel and high-efficiency attention mechanism between the grid and point features named Point-Grid Transformer (GridFormer). This mechanism treats the grid as a transfer point connecting the space and point cloud. Our method maximizes the spatial expressiveness of grid features and maintains computational efficiency. Furthermore, optimizing predictions over the entire space could potentially result in blurred boundaries. To address this issue, we further propose a boundary optimization strategy incorporating margin binary cross-entropy loss and boundary sampling. This approach enables us to achieve a more precise representation of the object structure. Our experiments validate that our method is effective and outperforms the state-of-the-art approaches under widely used benchmarks by producing more precise geometry reconstructions. The code is available at https://github.com/list17/GridFormer.","sentences":["Implicit neural networks have emerged as a crucial technology in 3D surface reconstruction.","To reconstruct continuous surfaces from discrete point clouds, encoding the input points into regular grid features (plane or volume) has been commonly employed in existing approaches.","However, these methods typically use the grid as an index for uniformly scattering point features.","Compared with the irregular point features, the regular grid features may sacrifice some reconstruction details but improve efficiency.","To take full advantage of these two types of features, we introduce a novel and high-efficiency attention mechanism between the grid and point features named Point-Grid Transformer (GridFormer).","This mechanism treats the grid as a transfer point connecting the space and point cloud.","Our method maximizes the spatial expressiveness of grid features and maintains computational efficiency.","Furthermore, optimizing predictions over the entire space could potentially result in blurred boundaries.","To address this issue, we further propose a boundary optimization strategy incorporating margin binary cross-entropy loss and boundary sampling.","This approach enables us to achieve a more precise representation of the object structure.","Our experiments validate that our method is effective and outperforms the state-of-the-art approaches under widely used benchmarks by producing more precise geometry reconstructions.","The code is available at https://github.com/list17/GridFormer."],"url":"http://arxiv.org/abs/2401.02292v1"}
{"created":"2024-01-04 14:19:37","title":"Path-based Explanation for Knowledge Graph Completion","abstract":"Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years. However, the explanation of the predicted facts has not caught the necessary attention. Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models. Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations. Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored. To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models. We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme. We further introduce three new metrics for quantitative evaluation of the explanations, together with a qualitative human evaluation. Extensive experiments demonstrate that Power-Link outperforms the SOTA baselines in interpretability, efficiency, and scalability.","sentences":["Graph Neural Networks (GNNs) have achieved great success in Knowledge Graph Completion (KGC) by modelling how entities and relations interact in recent years.","However, the explanation of the predicted facts has not caught the necessary attention.","Proper explanations for the results of GNN-based KGC models increase model transparency and help researchers develop more reliable models.","Existing practices for explaining KGC tasks rely on instance/subgraph-based approaches, while in some scenarios, paths can provide more user-friendly and interpretable explanations.","Nonetheless, the methods for generating path-based explanations for KGs have not been well-explored.","To address this gap, we propose Power-Link, the first path-based KGC explainer that explores GNN-based models.","We design a novel simplified graph-powering technique, which enables the generation of path-based explanations with a fully parallelisable and memory-efficient training scheme.","We further introduce three new metrics for quantitative evaluation of the explanations, together with a qualitative human evaluation.","Extensive experiments demonstrate that Power-Link outperforms the SOTA baselines in interpretability, efficiency, and scalability."],"url":"http://arxiv.org/abs/2401.02290v1"}
{"created":"2024-01-04 14:10:38","title":"Distillation-based fabric anomaly detection","abstract":"Unsupervised texture anomaly detection has been a concerning topic in a vast amount of industrial processes. Patterned textures inspection, particularly in the context of fabric defect detection, is indeed a widely encountered use case. This task involves handling a diverse spectrum of colors and textile types, encompassing a wide range of fabrics. Given the extensive variability in colors, textures, and defect types, fabric defect detection poses a complex and challenging problem in the field of patterned textures inspection. In this article, we propose a knowledge distillation-based approach tailored specifically for addressing the challenge of unsupervised anomaly detection in textures resembling fabrics. Our method aims to redefine the recently introduced reverse distillation approach, which advocates for an encoder-decoder design to mitigate classifier bias and to prevent the student from reconstructing anomalies. In this study, we present a new reverse distillation technique for the specific task of fabric defect detection. Our approach involves a meticulous design selection that strategically highlights high-level features. To demonstrate the capabilities of our approach both in terms of performance and inference speed, we conducted a series of experiments on multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside conducting experiments on a dataset acquired from a textile manufacturing facility. The main contributions of this paper are the following: a robust texture anomaly detector utilizing a reverse knowledge-distillation technique suitable for both anomaly detection and domain generalization and a novel dataset encompassing a diverse range of fabrics and defects.","sentences":["Unsupervised texture anomaly detection has been a concerning topic in a vast amount of industrial processes.","Patterned textures inspection, particularly in the context of fabric defect detection, is indeed a widely encountered use case.","This task involves handling a diverse spectrum of colors and textile types, encompassing a wide range of fabrics.","Given the extensive variability in colors, textures, and defect types, fabric defect detection poses a complex and challenging problem in the field of patterned textures inspection.","In this article, we propose a knowledge distillation-based approach tailored specifically for addressing the challenge of unsupervised anomaly detection in textures resembling fabrics.","Our method aims to redefine the recently introduced reverse distillation approach, which advocates for an encoder-decoder design to mitigate classifier bias and to prevent the student from reconstructing anomalies.","In this study, we present a new reverse distillation technique for the specific task of fabric defect detection.","Our approach involves a meticulous design selection that strategically highlights high-level features.","To demonstrate the capabilities of our approach both in terms of performance and inference speed, we conducted a series of experiments on multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside conducting experiments on a dataset acquired from a textile manufacturing facility.","The main contributions of this paper are the following: a robust texture anomaly detector utilizing a reverse knowledge-distillation technique suitable for both anomaly detection and domain generalization and a novel dataset encompassing a diverse range of fabrics and defects."],"url":"http://arxiv.org/abs/2401.02287v1"}
{"created":"2024-01-04 14:01:24","title":"DEM: A Method for Certifying Deep Neural Network Classifier Outputs in Aerospace","abstract":"Software development in the aerospace domain requires adhering to strict, high-quality standards. While there exist regulatory guidelines for commercial software in this domain (e.g., ARP-4754 and DO-178), these do not apply to software with deep neural network (DNN) components. Consequently, it is unclear how to allow aerospace systems to benefit from the deep learning revolution. Our work here seeks to address this challenge with a novel, output-centric approach for DNN certification. Our method employs statistical verification techniques, and has the key advantage of being able to flag specific inputs for which the DNN's output may be unreliable - so that they may be later inspected by a human expert. To achieve this, our method conducts a statistical analysis of the DNN's predictions for other, nearby inputs, in order to detect inconsistencies. This is in contrast to existing techniques, which typically attempt to certify the entire DNN, as opposed to individual outputs. Our method uses the DNN as a black-box, and makes no assumptions about its topology. We hope that this work constitutes another step towards integrating DNNs in safety-critical applications - especially in the aerospace domain, where high standards of quality and reliability are crucial.","sentences":["Software development in the aerospace domain requires adhering to strict, high-quality standards.","While there exist regulatory guidelines for commercial software in this domain (e.g., ARP-4754 and DO-178), these do not apply to software with deep neural network (DNN) components.","Consequently, it is unclear how to allow aerospace systems to benefit from the deep learning revolution.","Our work here seeks to address this challenge with a novel, output-centric approach for DNN certification.","Our method employs statistical verification techniques, and has the key advantage of being able to flag specific inputs for which the DNN's output may be unreliable - so that they may be later inspected by a human expert.","To achieve this, our method conducts a statistical analysis of the DNN's predictions for other, nearby inputs, in order to detect inconsistencies.","This is in contrast to existing techniques, which typically attempt to certify the entire DNN, as opposed to individual outputs.","Our method uses the DNN as a black-box, and makes no assumptions about its topology.","We hope that this work constitutes another step towards integrating DNNs in safety-critical applications - especially in the aerospace domain, where high standards of quality and reliability are crucial."],"url":"http://arxiv.org/abs/2401.02283v1"}
{"created":"2024-01-04 13:58:14","title":"PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for 6DOF Object Pose Dataset Generation","abstract":"We introduce Physically Enhanced Gaussian Splatting Simulation System (PEGASUS) for 6DOF object pose dataset generation, a versatile dataset generator based on 3D Gaussian Splatting. Environment and object representations can be easily obtained using commodity cameras to reconstruct with Gaussian Splatting. PEGASUS allows the composition of new scenes by merging the respective underlying Gaussian Splatting point cloud of an environment with one or multiple objects. Leveraging a physics engine enables the simulation of natural object placement within a scene through interaction between meshes extracted for the objects and the environment. Consequently, an extensive amount of new scenes - static or dynamic - can be created by combining different environments and objects. By rendering scenes from various perspectives, diverse data points such as RGB images, depth maps, semantic masks, and 6DoF object poses can be extracted. Our study demonstrates that training on data generated by PEGASUS enables pose estimation networks to successfully transfer from synthetic data to real-world data. Moreover, we introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This dataset includes spherical scans that captures images from both object hemisphere and the Gaussian Splatting reconstruction, making them compatible with PEGASUS.","sentences":["We introduce Physically Enhanced Gaussian Splatting Simulation System (PEGASUS) for 6DOF object pose dataset generation, a versatile dataset generator based on 3D Gaussian Splatting.","Environment and object representations can be easily obtained using commodity cameras to reconstruct with Gaussian Splatting.","PEGASUS allows the composition of new scenes by merging the respective underlying Gaussian Splatting point cloud of an environment with one or multiple objects.","Leveraging a physics engine enables the simulation of natural object placement within a scene through interaction between meshes extracted for the objects and the environment.","Consequently, an extensive amount of new scenes - static or dynamic - can be created by combining different environments and objects.","By rendering scenes from various perspectives, diverse data points such as RGB images, depth maps, semantic masks, and 6DoF object poses can be extracted.","Our study demonstrates that training on data generated by PEGASUS enables pose estimation networks to successfully transfer from synthetic data to real-world data.","Moreover, we introduce the Ramen dataset, comprising 30 Japanese cup noodle items.","This dataset includes spherical scans that captures images from both object hemisphere and the Gaussian Splatting reconstruction, making them compatible with PEGASUS."],"url":"http://arxiv.org/abs/2401.02281v1"}
{"created":"2024-01-04 13:56:54","title":"Lightweight Fish Classification Model for Sustainable Marine Management: Indonesian Case","abstract":"The enormous demand for seafood products has led to exploitation of marine resources and near-extinction of some species. In particular, overfishing is one the main issues in sustainable marine development. In alignment with the protection of marine resources and sustainable fishing, this study proposes to advance fish classification techniques that support identifying protected fish species using state-of-the-art machine learning. We use a custom modification of the MobileNet model to design a lightweight classifier called M-MobileNet that is capable of running on limited hardware. As part of the study, we compiled a labeled dataset of 37,462 images of fish found in the waters of the Indonesian archipelago. The proposed model is trained on the dataset to classify images of the captured fish into their species and give recommendations on whether they are consumable or not. Our modified MobileNet model uses only 50\\% of the top layer parameters with about 42% GTX 860M utility and achieves up to 97% accuracy in fish classification and determining its consumability. Given the limited computing capacity available on many fishing vessels, the proposed model provides a practical solution to on-site fish classification. In addition, synchronized implementation of the proposed model on multiple vessels can supply valuable information about the movement and location of different species of fish.","sentences":["The enormous demand for seafood products has led to exploitation of marine resources and near-extinction of some species.","In particular, overfishing is one the main issues in sustainable marine development.","In alignment with the protection of marine resources and sustainable fishing, this study proposes to advance fish classification techniques that support identifying protected fish species using state-of-the-art machine learning.","We use a custom modification of the MobileNet model to design a lightweight classifier called M-MobileNet that is capable of running on limited hardware.","As part of the study, we compiled a labeled dataset of 37,462 images of fish found in the waters of the Indonesian archipelago.","The proposed model is trained on the dataset to classify images of the captured fish into their species and give recommendations on whether they are consumable or not.","Our modified MobileNet model uses only 50\\% of the top layer parameters with about 42% GTX 860M utility and achieves up to 97% accuracy in fish classification and determining its consumability.","Given the limited computing capacity available on many fishing vessels, the proposed model provides a practical solution to on-site fish classification.","In addition, synchronized implementation of the proposed model on multiple vessels can supply valuable information about the movement and location of different species of fish."],"url":"http://arxiv.org/abs/2401.02278v1"}
{"created":"2024-01-04 13:56:13","title":"Universal Approximation Theorem for Vector- and Hypercomplex-Valued Neural Networks","abstract":"The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision. This theorem supports using neural networks for various applications, including regression and classification tasks. Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks. However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties. This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances. Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras.","sentences":["The universal approximation theorem states that a neural network with one hidden layer can approximate continuous functions on compact sets with any desired precision.","This theorem supports using neural networks for various applications, including regression and classification tasks.","Furthermore, it is valid for real-valued neural networks and some hypercomplex-valued neural networks such as complex-, quaternion-, tessarine-, and Clifford-valued neural networks.","However, hypercomplex-valued neural networks are a type of vector-valued neural network defined on an algebra with additional algebraic or geometric properties.","This paper extends the universal approximation theorem for a wide range of vector-valued neural networks, including hypercomplex-valued models as particular instances.","Precisely, we introduce the concept of non-degenerate algebra and state the universal approximation theorem for neural networks defined on such algebras."],"url":"http://arxiv.org/abs/2401.02277v1"}
{"created":"2024-01-04 13:49:45","title":"ShapeAug: Occlusion Augmentation for Event Camera Data","abstract":"Recently, Dynamic Vision Sensors (DVSs) sparked a lot of interest due to their inherent advantages over conventional RGB cameras. These advantages include a low latency, a high dynamic range and a low energy consumption. Nevertheless, the processing of DVS data using Deep Learning (DL) methods remains a challenge, particularly since the availability of event training data is still limited. This leads to a need for event data augmentation techniques in order to improve accuracy as well as to avoid over-fitting on the training data. Another challenge especially in real world automotive applications is occlusion, meaning one object is hindering the view onto the object behind it. In this paper, we present a novel event data augmentation approach, which addresses this problem by introducing synthetic events for randomly moving objects in a scene. We test our method on multiple DVS classification datasets, resulting in an relative improvement of up to 6.5 % in top1-accuracy. Moreover, we apply our augmentation technique on the real world Gen1 Automotive Event Dataset for object detection, where we especially improve the detection of pedestrians by up to 5 %.","sentences":["Recently, Dynamic Vision Sensors (DVSs) sparked a lot of interest due to their inherent advantages over conventional RGB cameras.","These advantages include a low latency, a high dynamic range and a low energy consumption.","Nevertheless, the processing of DVS data using Deep Learning (DL) methods remains a challenge, particularly since the availability of event training data is still limited.","This leads to a need for event data augmentation techniques in order to improve accuracy as well as to avoid over-fitting on the training data.","Another challenge especially in real world automotive applications is occlusion, meaning one object is hindering the view onto the object behind it.","In this paper, we present a novel event data augmentation approach, which addresses this problem by introducing synthetic events for randomly moving objects in a scene.","We test our method on multiple DVS classification datasets, resulting in an relative improvement of up to 6.5 % in top1-accuracy.","Moreover, we apply our augmentation technique on the real world Gen1 Automotive Event Dataset for object detection, where we especially improve the detection of pedestrians by up to 5 %."],"url":"http://arxiv.org/abs/2401.02274v1"}
{"created":"2024-01-04 13:45:24","title":"Towards Seamless Serverless Computing Across an Edge-Cloud Continuum","abstract":"Serverless computing has emerged as an attractive paradigm due to the efficiency of development and the ease of deployment without managing any underlying infrastructure. Nevertheless, serverless computing approaches face numerous challenges to unlock their full potential in hybrid environments. To gain a deeper understanding and firsthand knowledge of serverless computing in edge-cloud deployments, we review the current state of open-source serverless platforms and compare them based on predefined requirements. We then design and implement a serverless computing platform with a novel edge orchestration technique that seamlessly deploys serverless functions across the edge and cloud environments on top of the Knative serverless platform. Moreover, we propose an offloading strategy for edge environments and four different functions for experimentation and showcase the performance benefits of our solution. Our results demonstrate that such an approach can efficiently utilize both cloud and edge resources by dynamically offloading functions from the edge to the cloud during high activity, while reducing the overall application latency and increasing request throughput compared to an edge-only deployment.","sentences":["Serverless computing has emerged as an attractive paradigm due to the efficiency of development and the ease of deployment without managing any underlying infrastructure.","Nevertheless, serverless computing approaches face numerous challenges to unlock their full potential in hybrid environments.","To gain a deeper understanding and firsthand knowledge of serverless computing in edge-cloud deployments, we review the current state of open-source serverless platforms and compare them based on predefined requirements.","We then design and implement a serverless computing platform with a novel edge orchestration technique that seamlessly deploys serverless functions across the edge and cloud environments on top of the Knative serverless platform.","Moreover, we propose an offloading strategy for edge environments and four different functions for experimentation and showcase the performance benefits of our solution.","Our results demonstrate that such an approach can efficiently utilize both cloud and edge resources by dynamically offloading functions from the edge to the cloud during high activity, while reducing the overall application latency and increasing request throughput compared to an edge-only deployment."],"url":"http://arxiv.org/abs/2401.02271v1"}
{"created":"2024-01-04 13:38:51","title":"Beyond Self-Promotion: How Software Engineering Research Is Discussed on LinkedIn","abstract":"LinkedIn is the largest professional network in the world. As such, it can serve to build bridges between practitioners, whose daily work is software engineering (SE), and researchers, who work to advance the field of software engineering. We know that such a metaphorical bridge exists: SE research findings are sometimes shared on LinkedIn and commented on by software practitioners. Yet, we do not know what state the bridge is in. Therefore, we quantitatively and qualitatively investigate how SE practitioners and researchers approach each other via public LinkedIn discussions and what both sides can contribute to effective science communication. We found that a considerable proportion of LinkedIn posts on SE research are written by people who are not the paper authors (39%). Further, 71% of all comments in our dataset are from people in the industry, but only every second post receives at least one comment at all. Based on our findings, we formulate concrete advice for researchers and practitioners to make sharing new research findings on LinkedIn more fruitful.","sentences":["LinkedIn is the largest professional network in the world.","As such, it can serve to build bridges between practitioners, whose daily work is software engineering (SE), and researchers, who work to advance the field of software engineering.","We know that such a metaphorical bridge exists: SE research findings are sometimes shared on LinkedIn and commented on by software practitioners.","Yet, we do not know what state the bridge is in.","Therefore, we quantitatively and qualitatively investigate how SE practitioners and researchers approach each other via public LinkedIn discussions and what both sides can contribute to effective science communication.","We found that a considerable proportion of LinkedIn posts on SE research are written by people who are not the paper authors (39%).","Further, 71% of all comments in our dataset are from people in the industry, but only every second post receives at least one comment at all.","Based on our findings, we formulate concrete advice for researchers and practitioners to make sharing new research findings on LinkedIn more fruitful."],"url":"http://arxiv.org/abs/2401.02268v1"}
{"created":"2024-01-04 13:30:59","title":"The Effects of Generative AI on Computing Students' Help-Seeking Preferences","abstract":"Help-seeking is a critical way for students to learn new concepts, acquire new skills, and get unstuck when problem-solving in their computing courses. The recent proliferation of generative AI tools, such as ChatGPT, offers students a new source of help that is always available on-demand. However, it is unclear how this new resource compares to existing help-seeking resources along dimensions of perceived quality, latency, and trustworthiness. In this paper, we investigate the help-seeking preferences and experiences of computing students now that generative AI tools are available to them. We collected survey data (n=47) and conducted interviews (n=8) with computing students. Our results suggest that although these models are being rapidly adopted, they have not yet fully eclipsed traditional help resources. The help-seeking resources that students rely on continue to vary depending on the task and other factors. Finally, we observed preliminary evidence about how help-seeking with generative AI is a skill that needs to be developed, with disproportionate benefits for those who are better able to harness the capabilities of LLMs. We discuss potential implications for integrating generative AI into computing classrooms and the future of help-seeking in the era of generative AI.","sentences":["Help-seeking is a critical way for students to learn new concepts, acquire new skills, and get unstuck when problem-solving in their computing courses.","The recent proliferation of generative AI tools, such as ChatGPT, offers students a new source of help that is always available on-demand.","However, it is unclear how this new resource compares to existing help-seeking resources along dimensions of perceived quality, latency, and trustworthiness.","In this paper, we investigate the help-seeking preferences and experiences of computing students now that generative AI tools are available to them.","We collected survey data (n=47) and conducted interviews (n=8) with computing students.","Our results suggest that although these models are being rapidly adopted, they have not yet fully eclipsed traditional help resources.","The help-seeking resources that students rely on continue to vary depending on the task and other factors.","Finally, we observed preliminary evidence about how help-seeking with generative AI is a skill that needs to be developed, with disproportionate benefits for those who are better able to harness the capabilities of LLMs.","We discuss potential implications for integrating generative AI into computing classrooms and the future of help-seeking in the era of generative AI."],"url":"http://arxiv.org/abs/2401.02262v1"}
{"created":"2024-01-04 13:21:11","title":"Uncertainty-Aware Deep Attention Recurrent Neural Network for Heterogeneous Time Series Imputation","abstract":"Missingness is ubiquitous in multivariate time series and poses an obstacle to reliable downstream analysis. Although recurrent network imputation achieved the SOTA, existing models do not scale to deep architectures that can potentially alleviate issues arising in complex data. Moreover, imputation carries the risk of biased estimations of the ground truth. Yet, confidence in the imputed values is always unmeasured or computed post hoc from model output. We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates missing values and their associated uncertainty in heterogeneous multivariate time series. By jointly representing feature-wise correlations and temporal dynamics, we adopt a self attention mechanism, along with an effective residual component, to achieve a deep recurrent neural network with good imputation performance and stable convergence. We also leverage self-supervised metric learning to boost performance by optimizing sample similarity. Finally, we transform DEARI into a Bayesian neural network through a novel Bayesian marginalization strategy to produce stochastic DEARI, which outperforms its deterministic equivalent. Experiments show that DEARI surpasses the SOTA in diverse imputation tasks using real-world datasets, namely air quality control, healthcare and traffic.","sentences":["Missingness is ubiquitous in multivariate time series and poses an obstacle to reliable downstream analysis.","Although recurrent network imputation achieved the SOTA, existing models do not scale to deep architectures that can potentially alleviate issues arising in complex data.","Moreover, imputation carries the risk of biased estimations of the ground truth.","Yet, confidence in the imputed values is always unmeasured or computed post hoc from model output.","We propose DEep Attention Recurrent Imputation (DEARI), which jointly estimates missing values and their associated uncertainty in heterogeneous multivariate time series.","By jointly representing feature-wise correlations and temporal dynamics, we adopt a self attention mechanism, along with an effective residual component, to achieve a deep recurrent neural network with good imputation performance and stable convergence.","We also leverage self-supervised metric learning to boost performance by optimizing sample similarity.","Finally, we transform DEARI into a Bayesian neural network through a novel Bayesian marginalization strategy to produce stochastic DEARI, which outperforms its deterministic equivalent.","Experiments show that DEARI surpasses the SOTA in diverse imputation tasks using real-world datasets, namely air quality control, healthcare and traffic."],"url":"http://arxiv.org/abs/2401.02258v1"}
{"created":"2024-01-04 13:15:41","title":"Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain Dialogue Systems","abstract":"Open-domain dialogue systems have started to engage in continuous conversations with humans. Those dialogue systems are required to be adjusted to the human interlocutor and evaluated in terms of their perspective. However, it is questionable whether the current automatic evaluation methods can approximate the interlocutor's judgments. In this study, we analyzed and examined what features are needed in an automatic response evaluator from the interlocutor's perspective. The first experiment on the Hazumi dataset revealed that interlocutor awareness plays a critical role in making automatic response evaluation correlate with the interlocutor's judgments. The second experiment using massive conversations on X (formerly Twitter) confirmed that dialogue continuity prediction can train an interlocutor-aware response evaluator without human feedback while revealing the difficulty in evaluating generated responses compared to human responses.","sentences":["Open-domain dialogue systems have started to engage in continuous conversations with humans.","Those dialogue systems are required to be adjusted to the human interlocutor and evaluated in terms of their perspective.","However, it is questionable whether the current automatic evaluation methods can approximate the interlocutor's judgments.","In this study, we analyzed and examined what features are needed in an automatic response evaluator from the interlocutor's perspective.","The first experiment on the Hazumi dataset revealed that interlocutor awareness plays a critical role in making automatic response evaluation correlate with the interlocutor's judgments.","The second experiment using massive conversations on X (formerly Twitter) confirmed that dialogue continuity prediction can train an interlocutor-aware response evaluator without human feedback while revealing the difficulty in evaluating generated responses compared to human responses."],"url":"http://arxiv.org/abs/2401.02256v1"}
{"created":"2024-01-04 13:11:43","title":"Balancing Continual Learning and Fine-tuning for Human Activity Recognition","abstract":"Wearable-based Human Activity Recognition (HAR) is a key task in human-centric machine learning due to its fundamental understanding of human behaviours. Due to the dynamic nature of human behaviours, continual learning promises HAR systems that are tailored to users' needs. However, because of the difficulty in collecting labelled data with wearable sensors, existing approaches that focus on supervised continual learning have limited applicability, while unsupervised continual learning methods only handle representation learning while delaying classifier training to a later stage. This work explores the adoption and adaptation of CaSSLe, a continual self-supervised learning model, and Kaizen, a semi-supervised continual learning model that balances representation learning and down-stream classification, for the task of wearable-based HAR. These schemes re-purpose contrastive learning for knowledge retention and, Kaizen combines that with self-training in a unified scheme that can leverage unlabelled and labelled data for continual learning. In addition to comparing state-of-the-art self-supervised continual learning schemes, we further investigated the importance of different loss terms and explored the trade-off between knowledge retention and learning from new tasks. In particular, our extensive evaluation demonstrated that the use of a weighting factor that reflects the ratio between learned and new classes achieves the best overall trade-off in continual learning.","sentences":["Wearable-based Human Activity Recognition (HAR) is a key task in human-centric machine learning due to its fundamental understanding of human behaviours.","Due to the dynamic nature of human behaviours, continual learning promises HAR systems that are tailored to users' needs.","However, because of the difficulty in collecting labelled data with wearable sensors, existing approaches that focus on supervised continual learning have limited applicability, while unsupervised continual learning methods only handle representation learning while delaying classifier training to a later stage.","This work explores the adoption and adaptation of CaSSLe, a continual self-supervised learning model, and Kaizen, a semi-supervised continual learning model that balances representation learning and down-stream classification, for the task of wearable-based HAR.","These schemes re-purpose contrastive learning for knowledge retention and, Kaizen combines that with self-training in a unified scheme that can leverage unlabelled and labelled data for continual learning.","In addition to comparing state-of-the-art self-supervised continual learning schemes, we further investigated the importance of different loss terms and explored the trade-off between knowledge retention and learning from new tasks.","In particular, our extensive evaluation demonstrated that the use of a weighting factor that reflects the ratio between learned and new classes achieves the best overall trade-off in continual learning."],"url":"http://arxiv.org/abs/2401.02255v1"}
{"created":"2024-01-04 13:11:17","title":"L3Cube-IndicNews: News-based Short Text and Long Document Classification Datasets in Indic Languages","abstract":"In this work, we introduce L3Cube-IndicNews, a multilingual text classification corpus aimed at curating a high-quality dataset for Indian regional languages, with a specific focus on news headlines and articles. We have centered our work on 10 prominent Indic languages, including Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and Punjabi. Each of these news datasets comprises 10 or more classes of news articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle different document lengths that are classified as: Short Headlines Classification (SHC) dataset containing the news headline and news category, Long Document Classification (LDC) dataset containing the whole news article and the news category, and Long Paragraph Classification (LPC) containing sub-articles of the news and the news category. We maintain consistent labeling across all 3 datasets for in-depth length-based analysis. We evaluate each of these Indic language datasets using 4 different models including monolingual BERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This research contributes significantly to expanding the pool of available text classification datasets and also makes it possible to develop topic classification models for Indian regional languages. This also serves as an excellent resource for cross-lingual analysis owing to the high overlap of labels among languages. The datasets and models are shared publicly at https://github.com/l3cube-pune/indic-nlp","sentences":["In this work, we introduce L3Cube-IndicNews, a multilingual text classification corpus aimed at curating a high-quality dataset for Indian regional languages, with a specific focus on news headlines and articles.","We have centered our work on 10 prominent Indic languages, including Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and Punjabi.","Each of these news datasets comprises 10 or more classes of news articles.","L3Cube-IndicNews offers 3 distinct datasets tailored to handle different document lengths that are classified as: Short Headlines Classification (SHC) dataset containing the news headline and news category, Long Document Classification (LDC) dataset containing the whole news article and the news category, and Long Paragraph Classification (LPC) containing sub-articles of the news and the news category.","We maintain consistent labeling across all 3 datasets for in-depth length-based analysis.","We evaluate each of these Indic language datasets using 4 different models including monolingual BERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT.","This research contributes significantly to expanding the pool of available text classification datasets and also makes it possible to develop topic classification models for Indian regional languages.","This also serves as an excellent resource for cross-lingual analysis owing to the high overlap of labels among languages.","The datasets and models are shared publicly at https://github.com/l3cube-pune/indic-nlp"],"url":"http://arxiv.org/abs/2401.02254v1"}
{"created":"2024-01-04 13:08:38","title":"REDriver: Runtime Enforcement for Autonomous Vehicles","abstract":"Autonomous driving systems (ADSs) integrate sensing, perception, drive control, and several other critical tasks in autonomous vehicles, motivating research into techniques for assessing their safety. While there are several approaches for testing and analysing them in high-fidelity simulators, ADSs may still encounter additional critical scenarios beyond those covered once they are deployed on real roads. An additional level of confidence can be established by monitoring and enforcing critical properties when the ADS is running. Existing work, however, is only able to monitor simple safety properties (e.g., avoidance of collisions) and is limited to blunt enforcement mechanisms such as hitting the emergency brakes. In this work, we propose REDriver, a general and modular approach to runtime enforcement, in which users can specify a broad range of properties (e.g., national traffic laws) in a specification language based on signal temporal logic (STL). REDriver monitors the planned trajectory of the ADS based on a quantitative semantics of STL, and uses a gradient-driven algorithm to repair the trajectory when a violation of the specification is likely. We implemented REDriver for two versions of Apollo (i.e., a popular ADS), and subjected it to a benchmark of violations of Chinese traffic laws. The results show that REDriver significantly improves Apollo's conformance to the specification with minimal overhead.","sentences":["Autonomous driving systems (ADSs) integrate sensing, perception, drive control, and several other critical tasks in autonomous vehicles, motivating research into techniques for assessing their safety.","While there are several approaches for testing and analysing them in high-fidelity simulators, ADSs may still encounter additional critical scenarios beyond those covered once they are deployed on real roads.","An additional level of confidence can be established by monitoring and enforcing critical properties when the ADS is running.","Existing work, however, is only able to monitor simple safety properties (e.g., avoidance of collisions) and is limited to blunt enforcement mechanisms such as hitting the emergency brakes.","In this work, we propose REDriver, a general and modular approach to runtime enforcement, in which users can specify a broad range of properties (e.g., national traffic laws) in a specification language based on signal temporal logic (STL).","REDriver monitors the planned trajectory of the ADS based on a quantitative semantics of STL, and uses a gradient-driven algorithm to repair the trajectory when a violation of the specification is likely.","We implemented REDriver for two versions of Apollo (i.e., a popular ADS), and subjected it to a benchmark of violations of Chinese traffic laws.","The results show that REDriver significantly improves Apollo's conformance to the specification with minimal overhead."],"url":"http://arxiv.org/abs/2401.02253v1"}
{"created":"2024-01-04 12:58:25","title":"On Augmenting Scenario-Based Modeling with Generative AI","abstract":"The manual modeling of complex systems is a daunting task; and although a plethora of methods exist that mitigate this issue, the problem remains very difficult. Recent advances in generative AI have allowed the creation of general-purpose chatbots, capable of assisting software engineers in various modeling tasks. However, these chatbots are often inaccurate, and an unstructured use thereof could result in erroneous system models. In this paper, we outline a method for the safer and more structured use of chatbots as part of the modeling process. To streamline this integration, we propose leveraging scenario-based modeling techniques, which are known to facilitate the automated analysis of models. We argue that through iterative invocations of the chatbot and the manual and automatic inspection of the resulting models, a more accurate system model can eventually be obtained. We describe favorable preliminary results, which highlight the potential of this approach.","sentences":["The manual modeling of complex systems is a daunting task; and although a plethora of methods exist that mitigate this issue, the problem remains very difficult.","Recent advances in generative AI have allowed the creation of general-purpose chatbots, capable of assisting software engineers in various modeling tasks.","However, these chatbots are often inaccurate, and an unstructured use thereof could result in erroneous system models.","In this paper, we outline a method for the safer and more structured use of chatbots as part of the modeling process.","To streamline this integration, we propose leveraging scenario-based modeling techniques, which are known to facilitate the automated analysis of models.","We argue that through iterative invocations of the chatbot and the manual and automatic inspection of the resulting models, a more accurate system model can eventually be obtained.","We describe favorable preliminary results, which highlight the potential of this approach."],"url":"http://arxiv.org/abs/2401.02245v1"}
{"created":"2024-01-04 12:54:10","title":"Policy-regularized Offline Multi-objective Reinforcement Learning","abstract":"In this paper, we aim to utilize only offline trajectory data to train a policy for multi-objective RL. We extend the offline policy-regularized method, a widely-adopted approach for single-objective offline RL problems, into the multi-objective setting in order to achieve the above goal. However, such methods face a new challenge in offline MORL settings, namely the preference-inconsistent demonstration problem. We propose two solutions to this problem: 1) filtering out preference-inconsistent demonstrations via approximating behavior preferences, and 2) adopting regularization techniques with high policy expressiveness. Moreover, we integrate the preference-conditioned scalarized update method into policy-regularized offline RL, in order to simultaneously learn a set of policies using a single policy network, thus reducing the computational cost induced by the training of a large number of individual policies for various preferences. Finally, we introduce Regularization Weight Adaptation to dynamically determine appropriate regularization weights for arbitrary target preferences during deployment. Empirical results on various multi-objective datasets demonstrate the capability of our approach in solving offline MORL problems.","sentences":["In this paper, we aim to utilize only offline trajectory data to train a policy for multi-objective RL.","We extend the offline policy-regularized method, a widely-adopted approach for single-objective offline RL problems, into the multi-objective setting in order to achieve the above goal.","However, such methods face a new challenge in offline MORL settings, namely the preference-inconsistent demonstration problem.","We propose two solutions to this problem: 1) filtering out preference-inconsistent demonstrations via approximating behavior preferences, and 2) adopting regularization techniques with high policy expressiveness.","Moreover, we integrate the preference-conditioned scalarized update method into policy-regularized offline RL, in order to simultaneously learn a set of policies using a single policy network, thus reducing the computational cost induced by the training of a large number of individual policies for various preferences.","Finally, we introduce Regularization Weight Adaptation to dynamically determine appropriate regularization weights for arbitrary target preferences during deployment.","Empirical results on various multi-objective datasets demonstrate the capability of our approach in solving offline MORL problems."],"url":"http://arxiv.org/abs/2401.02244v1"}
{"created":"2024-01-04 12:52:48","title":"Slot-guided Volumetric Object Radiance Fields","abstract":"We present a novel framework for 3D object-centric representation learning. Our approach effectively decomposes complex scenes into individual objects from a single image in an unsupervised fashion. This method, called slot-guided Volumetric Object Radiance Fields (sVORF), composes volumetric object radiance fields with object slots as a guidance to implement unsupervised 3D scene decomposition. Specifically, sVORF obtains object slots from a single image via a transformer module, maps these slots to volumetric object radiance fields with a hypernetwork and composes object radiance fields with the guidance of object slots at a 3D location. Moreover, sVORF significantly reduces memory requirement due to small-sized pixel rendering during training. We demonstrate the effectiveness of our approach by showing top results in scene decomposition and generation tasks of complex synthetic datasets (e.g., Room-Diverse). Furthermore, we also confirm the potential of sVORF to segment objects in real-world scenes (e.g., the LLFF dataset). We hope our approach can provide preliminary understanding of the physical world and help ease future research in 3D object-centric representation learning.","sentences":["We present a novel framework for 3D object-centric representation learning.","Our approach effectively decomposes complex scenes into individual objects from a single image in an unsupervised fashion.","This method, called slot-guided Volumetric Object Radiance Fields (sVORF), composes volumetric object radiance fields with object slots as a guidance to implement unsupervised 3D scene decomposition.","Specifically, sVORF obtains object slots from a single image via a transformer module, maps these slots to volumetric object radiance fields with a hypernetwork and composes object radiance fields with the guidance of object slots at a 3D location.","Moreover, sVORF significantly reduces memory requirement due to small-sized pixel rendering during training.","We demonstrate the effectiveness of our approach by showing top results in scene decomposition and generation tasks of complex synthetic datasets (e.g., Room-Diverse).","Furthermore, we also confirm the potential of sVORF to segment objects in real-world scenes (e.g., the LLFF dataset).","We hope our approach can provide preliminary understanding of the physical world and help ease future research in 3D object-centric representation learning."],"url":"http://arxiv.org/abs/2401.02241v1"}
{"created":"2024-01-04 12:50:40","title":"A Decision Method for Elementary Stream Calculus","abstract":"The main result is a doubly exponential decision procedure for the first-order equality theory of streams with both arithmetic and control-oriented stream operations. This stream logic is expressive for elementary problems of stream calculus.","sentences":["The main result is a doubly exponential decision procedure for the first-order equality theory of streams with both arithmetic and control-oriented stream operations.","This stream logic is expressive for elementary problems of stream calculus."],"url":"http://arxiv.org/abs/2401.02239v1"}
{"created":"2024-01-04 12:41:40","title":"U-Mixer: An Unet-Mixer Architecture with Stationarity Correction for Time Series Forecasting","abstract":"Time series forecasting is a crucial task in various domains. Caused by factors such as trends, seasonality, or irregular fluctuations, time series often exhibits non-stationary. It obstructs stable feature propagation through deep layers, disrupts feature distributions, and complicates learning data distribution changes. As a result, many existing models struggle to capture the underlying patterns, leading to degraded forecasting performance. In this study, we tackle the challenge of non-stationarity in time series forecasting with our proposed framework called U-Mixer. By combining Unet and Mixer, U-Mixer effectively captures local temporal dependencies between different patches and channels separately to avoid the influence of distribution variations among channels, and merge low- and high-levels features to obtain comprehensive data representations. The key contribution is a novel stationarity correction method, explicitly restoring data distribution by constraining the difference in stationarity between the data before and after model processing to restore the non-stationarity information, while ensuring the temporal dependencies are preserved. Through extensive experiments on various real-world time series datasets, U-Mixer demonstrates its effectiveness and robustness, and achieves 14.5\\% and 7.7\\% improvements over state-of-the-art (SOTA) methods.","sentences":["Time series forecasting is a crucial task in various domains.","Caused by factors such as trends, seasonality, or irregular fluctuations, time series often exhibits non-stationary.","It obstructs stable feature propagation through deep layers, disrupts feature distributions, and complicates learning data distribution changes.","As a result, many existing models struggle to capture the underlying patterns, leading to degraded forecasting performance.","In this study, we tackle the challenge of non-stationarity in time series forecasting with our proposed framework called U-Mixer.","By combining Unet and Mixer, U-Mixer effectively captures local temporal dependencies between different patches and channels separately to avoid the influence of distribution variations among channels, and merge low- and high-levels features to obtain comprehensive data representations.","The key contribution is a novel stationarity correction method, explicitly restoring data distribution by constraining the difference in stationarity between the data before and after model processing to restore the non-stationarity information, while ensuring the temporal dependencies are preserved.","Through extensive experiments on various real-world time series datasets, U-Mixer demonstrates its effectiveness and robustness, and achieves 14.5\\% and 7.7\\% improvements over state-of-the-art (SOTA) methods."],"url":"http://arxiv.org/abs/2401.02236v1"}
{"created":"2024-01-04 12:29:26","title":"Automated Test Production -- Complement to \"Ad-hoc\" Testing","abstract":"A view on software testing, taken in a broad sense and considered a important activity is presented. We discuss the methods and techniques for applying tests and the reasons we recognize make it difficult for industry to adopt the advances observed in academia. We discuss some advances in the area and briefly point out the approach we intend to follow in the search for a solution.","sentences":["A view on software testing, taken in a broad sense and considered a important activity is presented.","We discuss the methods and techniques for applying tests and the reasons we recognize make it difficult for industry to adopt the advances observed in academia.","We discuss some advances in the area and briefly point out the approach we intend to follow in the search for a solution."],"url":"http://arxiv.org/abs/2401.02230v1"}
{"created":"2024-01-04 12:25:00","title":"Enabling Digitalization in Modular Robotic Systems Integration","abstract":"Integrating robot systems into manufacturing lines is a time-consuming process. In the era of digitalization, the research and development of new technologies is crucial for improving integration processes. Numerous challenges, including the lack of standardization, as well as intricate stakeholder relationships, complicate the process of robotic systems integration. This process typically consists of acquisition, integration, and deployment of the robot systems. This thesis focuses on three areas that help automate and simplify robotic systems integration. In the first area, related to acquisition, a constraint-based configurator is demonstrated that resolves compatibility challenges between robot devices, and automates the configuration process. This reduces the risk of integrating incompatible devices and decreases the need for experts during the configuration phase. In the second area, related to integration, the interoperable modeling format, Unified Robot Description Format (URDF), is investigated, where a detailed analysis is performed, revealing significant inconsistencies and critical improvements. This format is widely used for kinematic modeling and 3D visualization of robots, and its models can be reused across simulation tools. Improving this format benefits a wide range of users, including robotics engineers, researchers, and students. In the third area, related to deployment, Digital Twins (DTs) for robot systems are explored, as these improve efficiency and reduce downtime. A comprehensive literature review of DTs is conducted, and a case study of modular robot systems is developed. This research can accelerate the adoption of DTs in the robotics industry. These insights and approaches improve the process of robotic systems integration, offering valuable contributions that future research can build upon, ultimately driving efficiency, and reducing costs.","sentences":["Integrating robot systems into manufacturing lines is a time-consuming process.","In the era of digitalization, the research and development of new technologies is crucial for improving integration processes.","Numerous challenges, including the lack of standardization, as well as intricate stakeholder relationships, complicate the process of robotic systems integration.","This process typically consists of acquisition, integration, and deployment of the robot systems.","This thesis focuses on three areas that help automate and simplify robotic systems integration.","In the first area, related to acquisition, a constraint-based configurator is demonstrated that resolves compatibility challenges between robot devices, and automates the configuration process.","This reduces the risk of integrating incompatible devices and decreases the need for experts during the configuration phase.","In the second area, related to integration, the interoperable modeling format, Unified Robot Description Format (URDF), is investigated, where a detailed analysis is performed, revealing significant inconsistencies and critical improvements.","This format is widely used for kinematic modeling and 3D visualization of robots, and its models can be reused across simulation tools.","Improving this format benefits a wide range of users, including robotics engineers, researchers, and students.","In the third area, related to deployment, Digital Twins (DTs) for robot systems are explored, as these improve efficiency and reduce downtime.","A comprehensive literature review of DTs is conducted, and a case study of modular robot systems is developed.","This research can accelerate the adoption of DTs in the robotics industry.","These insights and approaches improve the process of robotic systems integration, offering valuable contributions that future research can build upon, ultimately driving efficiency, and reducing costs."],"url":"http://arxiv.org/abs/2401.02227v1"}
{"created":"2024-01-04 12:21:01","title":"Trajectory-Oriented Policy Optimization with Sparse Rewards","abstract":"Deep reinforcement learning (DRL) remains challenging in tasks with sparse rewards. These sparse rewards often only indicate whether the task is partially or fully completed, meaning that many exploration actions must be performed before the agent obtains useful feedback. Hence, most existing DRL algorithms fail to learn feasible policies within a reasonable time frame. To overcome this problem, we develop an approach that exploits offline demonstration trajectories for faster and more efficient online RL in sparse reward settings. Our key insight is that by regarding offline demonstration trajectories as guidance, instead of imitating them, our method learns a policy whose state-action visitation marginal distribution matches that of offline demonstrations. Specifically, we introduce a novel trajectory distance based on maximum mean discrepancy (MMD) and formulate policy optimization as a distance-constrained optimization problem. Then, we show that this distance-constrained optimization problem can be reduced into a policy-gradient algorithm with shaped rewards learned from offline demonstrations. The proposed algorithm is evaluated on extensive discrete and continuous control tasks with sparse and deceptive rewards. The experimental results indicate that our proposed algorithm is significantly better than the baseline methods regarding diverse exploration and learning the optimal policy.","sentences":["Deep reinforcement learning (DRL) remains challenging in tasks with sparse rewards.","These sparse rewards often only indicate whether the task is partially or fully completed, meaning that many exploration actions must be performed before the agent obtains useful feedback.","Hence, most existing DRL algorithms fail to learn feasible policies within a reasonable time frame.","To overcome this problem, we develop an approach that exploits offline demonstration trajectories for faster and more efficient online RL in sparse reward settings.","Our key insight is that by regarding offline demonstration trajectories as guidance, instead of imitating them, our method learns a policy whose state-action visitation marginal distribution matches that of offline demonstrations.","Specifically, we introduce a novel trajectory distance based on maximum mean discrepancy (MMD) and formulate policy optimization as a distance-constrained optimization problem.","Then, we show that this distance-constrained optimization problem can be reduced into a policy-gradient algorithm with shaped rewards learned from offline demonstrations.","The proposed algorithm is evaluated on extensive discrete and continuous control tasks with sparse and deceptive rewards.","The experimental results indicate that our proposed algorithm is significantly better than the baseline methods regarding diverse exploration and learning the optimal policy."],"url":"http://arxiv.org/abs/2401.02225v1"}
{"created":"2024-01-04 12:15:45","title":"A BDI Agent-Based Task Scheduling Framework for Cloud Computing","abstract":"Cloud computing is an attractive technology for providing computing resources over the Internet. Task scheduling is a critical issue in cloud computing, where an efficient task scheduling method can improve overall cloud performance. Since cloud computing is a large-scale and geographically distributed environment, traditional scheduling methods that allocate resources in a centralized manner are ineffective. Besides, traditional methods are difficult to make rational decisions timely when the external environment changes. This paper proposes a decentralized BDI (belief-desire-intention) agent-based scheduling framework for cloud computing. BDI agents have advantages in modelling dynamic environments because BDI agents can update their beliefs, change desires, and trigger behaviours based on environmental changes. Besides, to avoid communication stuck caused by environmental uncertainties, the asynchronous communication mode with a notify listener is employed. The proposed framework covers both the task scheduling and rescheduling stages with the consideration of uncertain events that can interrupt task executions. Two agent-based algorithms are proposed to implement the task scheduling and rescheduling processes, and a novel recommendation mechanism is presented in the scheduling stage to reduce the impact of information synchronization delays. The proposed framework is implemented by JADEX and tested on CloudSim. The experimental results show that our framework can minimize the task makespan, balance the resource utilization in a large-scale environment, and maximize the task success rate when uncertain events occur.","sentences":["Cloud computing is an attractive technology for providing computing resources over the Internet.","Task scheduling is a critical issue in cloud computing, where an efficient task scheduling method can improve overall cloud performance.","Since cloud computing is a large-scale and geographically distributed environment, traditional scheduling methods that allocate resources in a centralized manner are ineffective.","Besides, traditional methods are difficult to make rational decisions timely when the external environment changes.","This paper proposes a decentralized BDI (belief-desire-intention) agent-based scheduling framework for cloud computing.","BDI agents have advantages in modelling dynamic environments because BDI agents can update their beliefs, change desires, and trigger behaviours based on environmental changes.","Besides, to avoid communication stuck caused by environmental uncertainties, the asynchronous communication mode with a notify listener is employed.","The proposed framework covers both the task scheduling and rescheduling stages with the consideration of uncertain events that can interrupt task executions.","Two agent-based algorithms are proposed to implement the task scheduling and rescheduling processes, and a novel recommendation mechanism is presented in the scheduling stage to reduce the impact of information synchronization delays.","The proposed framework is implemented by JADEX and tested on CloudSim.","The experimental results show that our framework can minimize the task makespan, balance the resource utilization in a large-scale environment, and maximize the task success rate when uncertain events occur."],"url":"http://arxiv.org/abs/2401.02223v1"}
{"created":"2024-01-04 12:04:22","title":"A Decentralized Multiagent-Based Task Scheduling Framework for Handling Uncertain Events in Fog Computing","abstract":"Fog computing has become an attractive research topic in recent years. As an extension of the cloud, fog computing provides computing resources for Internet of Things (IoT) applications through communicative fog nodes located at the network edge. Fog nodes assist cloud services in handling real-time and mobile applications by bringing the processing capability to where the data is generated. However, the introduction of fog nodes can increase scheduling openness and uncertainty. The scheduling issues in fog computing need to consider the geography, load balancing, and network latency between IoT devices, fog nodes, as well as the parent cloud. Besides, the scheduling methods also need to deal with the occurrence of uncertain events in real-time so as to ensure service reliability. This paper proposes an agent-based framework with a decentralized structure to construct the architecture of fog computing, while three agent-based algorithms are proposed to implement the scheduling, load balance, and rescheduling processes. The proposed framework is implemented by JADE and evaluated on the iFogSim toolkit. Experimental results show that the proposed scheduling framework can adaptively schedule tasks and resources for different service requests in fog computing and can also improve the task success rate when uncertain events occur.","sentences":["Fog computing has become an attractive research topic in recent years.","As an extension of the cloud, fog computing provides computing resources for Internet of Things (IoT) applications through communicative fog nodes located at the network edge.","Fog nodes assist cloud services in handling real-time and mobile applications by bringing the processing capability to where the data is generated.","However, the introduction of fog nodes can increase scheduling openness and uncertainty.","The scheduling issues in fog computing need to consider the geography, load balancing, and network latency between IoT devices, fog nodes, as well as the parent cloud.","Besides, the scheduling methods also need to deal with the occurrence of uncertain events in real-time so as to ensure service reliability.","This paper proposes an agent-based framework with a decentralized structure to construct the architecture of fog computing, while three agent-based algorithms are proposed to implement the scheduling, load balance, and rescheduling processes.","The proposed framework is implemented by JADE and evaluated on the iFogSim toolkit.","Experimental results show that the proposed scheduling framework can adaptively schedule tasks and resources for different service requests in fog computing and can also improve the task success rate when uncertain events occur."],"url":"http://arxiv.org/abs/2401.02219v1"}
{"created":"2024-01-04 12:04:05","title":"Optimizing Information Freshness in Uplink Multiuser MIMO Networks with Partial Observations","abstract":"This paper investigates a multiuser scheduling problem within an uplink multiple-input multi-output (MIMO) status update network, consisting of a multi-antenna base station (BS) and multiple single-antenna devices. The presence of multiple antennas at the BS introduces spatial degrees-of-freedom, enabling concurrent transmission of status updates from multiple devices in each time slot. Our objective is to optimize network-wide information freshness, quantified by the age of information (AoI) metric, by determining how the BS can best schedule device transmissions, while taking into account the random arrival of status updates at the device side.To address this decision-making problem, we model it as a partially observable Markov decision process (POMDP) and establish that the evolution of belief states for different devices is independent.We also prove that feasible belief states can be described by finite-dimensional vectors. Building on these observations, we develop a dynamic scheduling (DS) policy to solve the POMDP, and then derive an upper bound of its AoI performance, which is used to optimize the parameter configuration. To gain more design insights, we investigate a symmetric network, and put forth a fixed scheduling (FS) policy with lower computational complexity. An action space reduction strategy is applied to further reduce the computational complexity of both DS and FS policies. Our numerical results validate our analyses and indicate that the DS policy with the reduced action space performs almost identically to the original DS policy, and both outperform the baseline policies.","sentences":["This paper investigates a multiuser scheduling problem within an uplink multiple-input multi-output (MIMO) status update network, consisting of a multi-antenna base station (BS) and multiple single-antenna devices.","The presence of multiple antennas at the BS introduces spatial degrees-of-freedom, enabling concurrent transmission of status updates from multiple devices in each time slot.","Our objective is to optimize network-wide information freshness, quantified by the age of information (AoI) metric, by determining how the BS can best schedule device transmissions, while taking into account the random arrival of status updates at the device side.","To address this decision-making problem, we model it as a partially observable Markov decision process (POMDP) and establish that the evolution of belief states for different devices is independent.","We also prove that feasible belief states can be described by finite-dimensional vectors.","Building on these observations, we develop a dynamic scheduling (DS) policy to solve the POMDP, and then derive an upper bound of its AoI performance, which is used to optimize the parameter configuration.","To gain more design insights, we investigate a symmetric network, and put forth a fixed scheduling (FS) policy with lower computational complexity.","An action space reduction strategy is applied to further reduce the computational complexity of both DS and FS policies.","Our numerical results validate our analyses and indicate that the DS policy with the reduced action space performs almost identically to the original DS policy, and both outperform the baseline policies."],"url":"http://arxiv.org/abs/2401.02218v1"}
{"created":"2024-01-04 11:34:39","title":"Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph","abstract":"Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by attaching the time scope. Existing temporal knowledge graph question answering (TKGQA) models solely approach simple questions, owing to the prior assumption that each question only contains a single temporal fact with explicit/implicit temporal constraints. Hence, they perform poorly on questions which own multiple temporal facts. In this paper, we propose \\textbf{\\underline{J}}oint \\textbf{\\underline{M}}ulti \\textbf{\\underline{F}}acts \\textbf{\\underline{R}}easoning \\textbf{\\underline{N}}etwork (JMFRN), to jointly reasoning multiple temporal facts for accurately answering \\emph{complex} temporal questions. Specifically, JMFRN first retrieves question-related temporal facts from TKG for each entity of the given complex question. For joint reasoning, we design two different attention (\\ie entity-aware and time-aware) modules, which are suitable for universal settings, to aggregate entities and timestamps information of retrieved facts. Moreover, to filter incorrect type answers, we introduce an additional answer type discrimination task. Extensive experiments demonstrate our proposed method significantly outperforms the state-of-art on the well-known complex temporal question benchmark TimeQuestions.","sentences":["Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by attaching the time scope.","Existing temporal knowledge graph question answering (TKGQA) models solely approach simple questions, owing to the prior assumption that each question only contains a single temporal fact with explicit/implicit temporal constraints.","Hence, they perform poorly on questions which own multiple temporal facts.","In this paper, we propose \\textbf{\\underline{J}}oint \\textbf{\\underline{M}}ulti \\textbf{\\underline{F}}acts \\textbf{\\underline{R}}easoning \\textbf{\\underline{N}}etwork (JMFRN), to jointly reasoning multiple temporal facts for accurately answering \\emph{complex} temporal questions.","Specifically, JMFRN first retrieves question-related temporal facts from TKG for each entity of the given complex question.","For joint reasoning, we design two different attention (\\ie entity-aware and time-aware) modules, which are suitable for universal settings, to aggregate entities and timestamps information of retrieved facts.","Moreover, to filter incorrect type answers, we introduce an additional answer type discrimination task.","Extensive experiments demonstrate our proposed method significantly outperforms the state-of-art on the well-known complex temporal question benchmark TimeQuestions."],"url":"http://arxiv.org/abs/2401.02212v1"}
