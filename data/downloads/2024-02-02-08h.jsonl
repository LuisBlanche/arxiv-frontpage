{"created":"2024-02-01 18:59:56","title":"AToM: Amortized Text-to-Mesh using 2D Diffusion","abstract":"We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh framework optimized across multiple text prompts simultaneously. In contrast to existing text-to-3D methods that often entail time-consuming per-prompt optimization and commonly output representations other than polygonal meshes, AToM directly generates high-quality textured meshes in less than 1 second with around 10 times reduction in the training cost, and generalizes to unseen prompts. Our key idea is a novel triplane-based text-to-mesh architecture with a two-stage amortized optimization strategy that ensures stable training and enables scalability. Through extensive experiments on various prompt benchmarks, AToM significantly outperforms state-of-the-art amortized approaches with over 4 times higher accuracy (in DF415 dataset) and produces more distinguishable and higher-quality 3D outputs. AToM demonstrates strong generalizability, offering finegrained 3D assets for unseen interpolated prompts without further optimization during inference, unlike per-prompt solutions.","sentences":["We introduce Amortized Text-to-Mesh (AToM), a feed-forward text-to-mesh framework optimized across multiple text prompts simultaneously.","In contrast to existing text-to-3D methods that often entail time-consuming per-prompt optimization and commonly output representations other than polygonal meshes, AToM directly generates high-quality textured meshes in less than 1 second with around 10 times reduction in the training cost, and generalizes to unseen prompts.","Our key idea is a novel triplane-based text-to-mesh architecture with a two-stage amortized optimization strategy that ensures stable training and enables scalability.","Through extensive experiments on various prompt benchmarks, AToM significantly outperforms state-of-the-art amortized approaches with over 4 times higher accuracy (in DF415 dataset) and produces more distinguishable and higher-quality 3D outputs.","AToM demonstrates strong generalizability, offering finegrained 3D assets for unseen interpolated prompts without further optimization during inference, unlike per-prompt solutions."],"url":"http://arxiv.org/abs/2402.00867v1"}
{"created":"2024-02-01 18:59:56","title":"We're Not Using Videos Effectively: An Updated Domain Adaptive Video Segmentation Baseline","abstract":"There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain. While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames. However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking. In this work, we address this gap. Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets. To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark. Code available at https://github.com/SimarKareer/UnifiedVideoDA","sentences":["There has been abundant work in unsupervised domain adaptation for semantic segmentation (DAS) seeking to adapt a model trained on images from a labeled source domain to an unlabeled target domain.","While the vast majority of prior work has studied this as a frame-level Image-DAS problem, a few Video-DAS works have sought to additionally leverage the temporal signal present in adjacent frames.","However, Video-DAS works have historically studied a distinct set of benchmarks from Image-DAS, with minimal cross-benchmarking.","In this work, we address this gap.","Surprisingly, we find that (1) even after carefully controlling for data and model architecture, state-of-the-art Image-DAS methods (HRDA and HRDA+MIC)} outperform Video-DAS methods on established Video-DAS benchmarks (+14.5 mIoU on Viper$\\rightarrow$CityscapesSeq, +19.0 mIoU on Synthia$\\rightarrow$CityscapesSeq), and (2) naive combinations of Image-DAS and Video-DAS techniques only lead to marginal improvements across datasets.","To avoid siloed progress between Image-DAS and Video-DAS, we open-source our codebase with support for a comprehensive set of Video-DAS and Image-DAS methods on a common benchmark.","Code available at https://github.com/SimarKareer/UnifiedVideoDA"],"url":"http://arxiv.org/abs/2402.00868v1"}
{"created":"2024-02-01 18:59:22","title":"Towards Optimal Feature-Shaping Methods for Out-of-Distribution Detection","abstract":"Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection. These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples. However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability. To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods. We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem. Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data. Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures.","sentences":["Feature shaping refers to a family of methods that exhibit state-of-the-art performance for out-of-distribution (OOD) detection.","These approaches manipulate the feature representation, typically from the penultimate layer of a pre-trained deep learning model, so as to better differentiate between in-distribution (ID) and OOD samples.","However, existing feature-shaping methods usually employ rules manually designed for specific model architectures and OOD datasets, which consequently limit their generalization ability.","To address this gap, we first formulate an abstract optimization framework for studying feature-shaping methods.","We then propose a concrete reduction of the framework with a simple piecewise constant shaping function and show that existing feature-shaping methods approximate the optimal solution to the concrete optimization problem.","Further, assuming that OOD data is inaccessible, we propose a formulation that yields a closed-form solution for the piecewise constant shaping function, utilizing solely the ID data.","Through extensive experiments, we show that the feature-shaping function optimized by our method improves the generalization ability of OOD detection across a large variety of datasets and model architectures."],"url":"http://arxiv.org/abs/2402.00865v1"}
{"created":"2024-02-01 18:59:09","title":"ViCA-NeRF: View-Consistency-Aware 3D Editing of Neural Radiance Fields","abstract":"We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions. In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency. For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views. For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene. Incorporating these two strategies, our ViCA-NeRF operates in two stages. In the initial stage, we blend edits from different views to create a preliminary 3D edit. This is followed by a second stage of NeRF training, dedicated to further refining the scene's appearance. Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art. Our code is publicly available.","sentences":["We introduce ViCA-NeRF, the first view-consistency-aware method for 3D editing with text instructions.","In addition to the implicit neural radiance field (NeRF) modeling, our key insight is to exploit two sources of regularization that explicitly propagate the editing information across different views, thus ensuring multi-view consistency.","For geometric regularization, we leverage the depth information derived from NeRF to establish image correspondences between different views.","For learned regularization, we align the latent codes in the 2D diffusion model between edited and unedited images, enabling us to edit key views and propagate the update throughout the entire scene.","Incorporating these two strategies, our ViCA-NeRF operates in two stages.","In the initial stage, we blend edits from different views to create a preliminary 3D edit.","This is followed by a second stage of NeRF training, dedicated to further refining the scene's appearance.","Experimental results demonstrate that ViCA-NeRF provides more flexible, efficient (3 times faster) editing with higher levels of consistency and details, compared with the state of the art.","Our code is publicly available."],"url":"http://arxiv.org/abs/2402.00864v1"}
{"created":"2024-02-01 18:58:44","title":"Geometry Transfer for Stylizing Radiance Fields","abstract":"Shape and geometric patterns are essential in defining stylistic identity. However, current 3D style transfer methods predominantly focus on transferring colors and textures, often overlooking geometric aspects. In this paper, we introduce Geometry Transfer, a novel method that leverages geometric deformation for 3D style transfer. This technique employs depth maps to extract a style guide, subsequently applied to stylize the geometry of radiance fields. Moreover, we propose new techniques that utilize geometric cues from the 3D scene, thereby enhancing aesthetic expressiveness and more accurately reflecting intended styles. Our extensive experiments show that Geometry Transfer enables a broader and more expressive range of stylizations, thereby significantly expanding the scope of 3D style transfer.","sentences":["Shape and geometric patterns are essential in defining stylistic identity.","However, current 3D style transfer methods predominantly focus on transferring colors and textures, often overlooking geometric aspects.","In this paper, we introduce Geometry Transfer, a novel method that leverages geometric deformation for 3D style transfer.","This technique employs depth maps to extract a style guide, subsequently applied to stylize the geometry of radiance fields.","Moreover, we propose new techniques that utilize geometric cues from the 3D scene, thereby enhancing aesthetic expressiveness and more accurately reflecting intended styles.","Our extensive experiments show that Geometry Transfer enables a broader and more expressive range of stylizations, thereby significantly expanding the scope of 3D style transfer."],"url":"http://arxiv.org/abs/2402.00863v1"}
{"created":"2024-02-01 18:56:18","title":"Evaluating Large Language Models for Generalization and Robustness via Data Compression","abstract":"Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation. To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff. Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff. We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness. Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data. We find that the compression rate of many models reduces significantly after their cutoff date, but models such as Mistral and Llama-2 demonstrate a good balance between performance and robustness. Results also suggest that models struggle to generalize on news and code data, but work especially well on arXiv papers. We also find the context size and tokenization implementation have a big impact of on the overall compression performance.","sentences":["Existing methods for evaluating large language models face challenges such as data contamination, sensitivity to prompts, and the high cost of benchmark creation.","To address this, we propose a lossless data compression based evaluation approach that tests how models' predictive abilities generalize after their training cutoff.","Specifically, we collect comprehensive test data spanning 83 months from 2017 to 2023 and split the data into training and testing periods according to models' training data cutoff.","We measure: 1) the compression performance on the testing period as a measure of generalization on unseen data; and 2) the performance gap between the training and testing period as a measure of robustness.","Our experiments test 14 representative large language models with various sizes on sources including Wikipedia, news articles, code, arXiv papers, and multi-modal data.","We find that the compression rate of many models reduces significantly after their cutoff date, but models such as Mistral and Llama-2 demonstrate a good balance between performance and robustness.","Results also suggest that models struggle to generalize on news and code data, but work especially well on arXiv papers.","We also find the context size and tokenization implementation have a big impact of on the overall compression performance."],"url":"http://arxiv.org/abs/2402.00861v1"}
{"created":"2024-02-01 18:55:29","title":"Can Large Language Models Understand Context?","abstract":"Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent. However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features. This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models. This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context. First, we evaluate the performance of LLMs under the in-context learning pretraining scenario. Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models. Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings. We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark. We conduct an extensive analysis of these scenarios to substantiate our experimental results.","sentences":["Understanding context is key to understanding human language, an ability which Large Language Models (LLMs) have been increasingly seen to demonstrate to an impressive extent.","However, though the evaluation of LLMs encompasses various domains within the realm of Natural Language Processing, limited attention has been paid to probing their linguistic capability of understanding contextual features.","This paper introduces a context understanding benchmark by adapting existing datasets to suit the evaluation of generative models.","This benchmark comprises of four distinct tasks and nine datasets, all featuring prompts designed to assess the models' ability to understand context.","First, we evaluate the performance of LLMs under the in-context learning pretraining scenario.","Experimental results indicate that pre-trained dense models struggle with understanding more nuanced contextual features when compared to state-of-the-art fine-tuned models.","Second, as LLM compression holds growing significance in both research and real-world applications, we assess the context understanding of quantized models under in-context-learning settings.","We find that 3-bit post-training quantization leads to varying degrees of performance reduction on our benchmark.","We conduct an extensive analysis of these scenarios to substantiate our experimental results."],"url":"http://arxiv.org/abs/2402.00858v1"}
{"created":"2024-02-01 18:54:34","title":"Early Time Classification with Accumulated Accuracy Gap Control","abstract":"Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input. In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule. This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification. We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances. As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times. Numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method. We show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control.","sentences":["Early time classification algorithms aim to label a stream of features without processing the full input stream, while maintaining accuracy comparable to that achieved by applying the classifier to the entire input.","In this paper, we introduce a statistical framework that can be applied to any sequential classifier, formulating a calibrated stopping rule.","This data-driven rule attains finite-sample, distribution-free control of the accuracy gap between full and early-time classification.","We start by presenting a novel method that builds on the Learn-then-Test calibration framework to control this gap marginally, on average over i.i.d. instances.","As this algorithm tends to yield an excessively high accuracy gap for early halt times, our main contribution is the proposal of a framework that controls a stronger notion of error, where the accuracy gap is controlled conditionally on the accumulated halt times.","Numerical experiments demonstrate the effectiveness, applicability, and usefulness of our method.","We show that our proposed early stopping mechanism reduces up to 94% of timesteps used for classification while achieving rigorous accuracy gap control."],"url":"http://arxiv.org/abs/2402.00857v1"}
{"created":"2024-02-01 18:51:54","title":"Towards Efficient and Exact Optimization of Language Model Alignment","abstract":"The alignment of language models with human preferences is vital for their application in real-world tasks. The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy. While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement. Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data. Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   In this paper, we propose efficient exact optimization (EXO) of the alignment objective. We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms. We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data.","sentences":["The alignment of language models with human preferences is vital for their application in real-world tasks.","The problem is formulated as optimizing the model's policy to maximize the expected reward that reflects human preferences with minimal deviation from the initial policy.","While considered as a straightforward solution, reinforcement learning (RL) suffers from high variance in policy updates, which impedes efficient policy improvement.","Recently, direct preference optimization (DPO) was proposed to directly optimize the policy from preference data.","Though simple to implement, DPO is derived based on the optimal policy that is not assured to be achieved in practice, which undermines its convergence to the intended solution.   ","In this paper, we propose efficient exact optimization (EXO) of the alignment objective.","We prove that EXO is guaranteed to optimize in the same direction as the RL algorithms asymptotically for arbitary parametrization of the policy, while enables efficient optimization by circumventing the complexities associated with RL algorithms.","We compare our method to DPO with both theoretical and empirical analyses, and further demonstrate the advantages of our method over existing approaches on realistic human preference data."],"url":"http://arxiv.org/abs/2402.00856v1"}
{"created":"2024-02-01 18:50:50","title":"SymbolicAI: A framework for logic-based approaches combining generative models and solvers","abstract":"We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes. SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI. We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths. The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives. As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems. In turn, the framework facilitates the creation and evaluation of explainable computational graphs. We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows. We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short. The framework codebase and benchmark are linked below.","sentences":["We introduce SymbolicAI, a versatile and modular framework employing a logic-based approach to concept learning and flow management in generative processes.","SymbolicAI enables the seamless integration of generative models with a diverse range of solvers by treating large language models (LLMs) as semantic parsers that execute tasks based on both natural and formal language instructions, thus bridging the gap between symbolic reasoning and generative AI.","We leverage probabilistic programming principles to tackle complex tasks, and utilize differentiable and classical programming paradigms with their respective strengths.","The framework introduces a set of polymorphic, compositional, and self-referential operations for data stream manipulation, aligning LLM outputs with user objectives.","As a result, we can transition between the capabilities of various foundation models endowed with zero- and few-shot learning capabilities and specialized, fine-tuned models or solvers proficient in addressing specific problems.","In turn, the framework facilitates the creation and evaluation of explainable computational graphs.","We conclude by introducing a quality measure and its empirical score for evaluating these computational graphs, and propose a benchmark that compares various state-of-the-art LLMs across a set of complex workflows.","We refer to the empirical score as the \"Vector Embedding for Relational Trajectory Evaluation through Cross-similarity\", or VERTEX score for short.","The framework codebase and benchmark are linked below."],"url":"http://arxiv.org/abs/2402.00854v1"}
{"created":"2024-02-01 18:50:42","title":"LTAU-FF: Loss Trajectory Analysis for Uncertainty in Atomistic Force Fields","abstract":"Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields. Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference. In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space. Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference. As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test errors on multiple datasets from the literature. We then illustrate the utility of LTAU-FF in two practical applications: 1) tuning the training-validation gap for an example dataset, and 2) predicting errors in relaxation trajectories on the OC20 IS2RS task. Though in this work we focus on the use of LTAU with deep learning atomistic force fields, we emphasize that it can be readily applied to any regression task, or any ensemble-generation technique, to provide a reliable and easy-to-implement UQ metric.","sentences":["Model ensembles are simple and effective tools for estimating the prediction uncertainty of deep learning atomistic force fields.","Despite this, widespread adoption of ensemble-based uncertainty quantification (UQ) techniques is limited by the high computational costs incurred by ensembles during both training and inference.","In this work we leverage the cumulative distribution functions (CDFs) of per-sample errors obtained over the course of training to efficiently represent the model ensemble, and couple them with a distance-based similarity search in the model latent space.","Using these tools, we develop a simple UQ metric (which we call LTAU) that leverages the strengths of ensemble-based techniques without requiring the evaluation of multiple models during either training or inference.","As an initial test, we apply our method towards estimating the epistemic uncertainty in atomistic force fields (LTAU-FF) and demonstrate that it can be easily calibrated to accurately predict test errors on multiple datasets from the literature.","We then illustrate the utility of LTAU-FF in two practical applications: 1) tuning the training-validation gap for an example dataset, and 2) predicting errors in relaxation trajectories on the OC20 IS2RS task.","Though in this work we focus on the use of LTAU with deep learning atomistic force fields, we emphasize that it can be readily applied to any regression task, or any ensemble-generation technique, to provide a reliable and easy-to-implement UQ metric."],"url":"http://arxiv.org/abs/2402.00853v1"}
{"created":"2024-02-01 18:46:28","title":"Data Augmentation Scheme for Raman Spectra with Highly Correlated Annotations","abstract":"In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations. As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum. Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra. However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative. They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances. However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables. In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on such data exhibits low correlations between the model predictions. We show that training a CNN on these generated data points improves the performance on datasets where the annotations do not bear the same correlation as the dataset that was used for model training. This data augmentation technique enables us to reuse spectra as training data for new contexts that exhibit different correlations. The additional data allows for building a better and more robust model. This is of interest in scenarios where large amounts of historical data are available but are currently not used for model training. We demonstrate the capabilities of the proposed method using synthetic spectra of Ralstonia eutropha batch cultivations to monitor substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations during of the experiments.","sentences":["In biotechnology Raman Spectroscopy is rapidly gaining popularity as a process analytical technology (PAT) that measures cell densities, substrate- and product concentrations.","As it records vibrational modes of molecules it provides that information non-invasively in a single spectrum.","Typically, partial least squares (PLS) is the model of choice to infer information about variables of interest from the spectra.","However, biological processes are known for their complexity where convolutional neural networks (CNN) present a powerful alternative.","They can handle non-Gaussian noise and account for beam misalignment, pixel malfunctions or the presence of additional substances.","However, they require a lot of data during model training, and they pick up non-linear dependencies in the process variables.","In this work, we exploit the additive nature of spectra in order to generate additional data points from a given dataset that have statistically independent labels so that a network trained on such data exhibits low correlations between the model predictions.","We show that training a CNN on these generated data points improves the performance on datasets where the annotations do not bear the same correlation as the dataset that was used for model training.","This data augmentation technique enables us to reuse spectra as training data for new contexts that exhibit different correlations.","The additional data allows for building a better and more robust model.","This is of interest in scenarios where large amounts of historical data are available but are currently not used for model training.","We demonstrate the capabilities of the proposed method using synthetic spectra of Ralstonia eutropha batch cultivations to monitor substrate, biomass and polyhydroxyalkanoate (PHA) biopolymer concentrations during of the experiments."],"url":"http://arxiv.org/abs/2402.00851v1"}
{"created":"2024-02-01 18:40:08","title":"Constant Degree Direct Product Testers with Small Soundness","abstract":"Let $X$ be a $d$-dimensional simplicial complex. A function $F\\colon X(k)\\to \\{0,1\\}^k$ is said to be a direct product function if there exists a function $f\\colon X(1)\\to \\{0,1\\}$ such that $F(\\sigma) = (f(\\sigma_1), \\ldots, f(\\sigma_k))$ for each $k$-face $\\sigma$. In an effort to simplify components of the PCP theorem, Goldreich and Safra introduced the problem of direct product testing, which asks whether one can test if $F\\colon X(k)\\to \\{0,1\\}^k$ is correlated with a direct product function by querying $F$ on only $2$ inputs. Dinur and Kaufman conjectured that there exist bounded degree complexes with a direct product test in the small soundness regime. We resolve their conjecture by showing that for all $\\delta>0$, there exists a family of high-dimensional expanders with degree $O_{\\delta}(1)$ and a $2$-query direct product tester with soundness $\\delta$.   We use the characterization given by a subset of the authors and independently by Dikstein and Dinur, who showed that some form of non-Abelian coboundary expansion (which they called \"Unique-Games coboundary expansion\") is a necessary and sufficient condition for a complex to admit such direct product testers. Our main technical contribution is a general technique for showing coboundary expansion of complexes with coefficients in a non-Abelian group. This allows us to prove that the high dimensional expanders constructed by Chapman and Lubotzky satisfies the necessary conditions, thus admitting a 2-query direct product tester with small soundness.","sentences":["Let $X$ be a $d$-dimensional simplicial complex.","A function $F\\colon X(k)\\to \\{0,1\\}^k$ is said to be a direct product function if there exists a function $f\\colon X(1)\\to \\{0,1\\}$ such that $F(\\sigma) = (f(\\sigma_1), \\ldots, f(\\sigma_k))$ for each $k$-face $\\sigma$. In an effort to simplify components of the PCP theorem, Goldreich and Safra introduced the problem of direct product testing, which asks whether one can test if $F\\colon X(k)\\to \\{0,1\\}^k$ is correlated with a direct product function by querying $F$ on only $2$ inputs.","Dinur and Kaufman conjectured that there exist bounded degree complexes with a direct product test in the small soundness regime.","We resolve their conjecture by showing that for all $\\delta>0$, there exists a family of high-dimensional expanders with degree $O_{\\delta}(1)$ and a $2$-query direct product tester with soundness $\\delta$.   We use the characterization given by a subset of the authors and independently by Dikstein and Dinur, who showed that some form of non-Abelian coboundary expansion (which they called \"Unique-Games coboundary expansion\") is a necessary and sufficient condition for a complex to admit such direct product testers.","Our main technical contribution is a general technique for showing coboundary expansion of complexes with coefficients in a non-Abelian group.","This allows us to prove that the high dimensional expanders constructed by Chapman and Lubotzky satisfies the necessary conditions, thus admitting a 2-query direct product tester with small soundness."],"url":"http://arxiv.org/abs/2402.00850v1"}
{"created":"2024-02-01 18:40:03","title":"Score-based Causal Representation Learning: Linear and General Transformations","abstract":"This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \\emph{identifiability} and \\emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \\emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \\emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \\emph{linear} transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability. It also provides partial identifiability guarantees for soft interventions, including identifiability up to ancestors for general causal models and perfect latent graph recovery for sufficiently non-linear causal models. Secondly, it focuses on \\emph{general} transformations and shows that two stochastic hard interventions per node suffice for identifiability. Notably, one does \\emph{not} need to know which pair of interventional environments have the same node intervened.","sentences":["This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables.","Linear and general transformations are investigated.","The paper addresses both the \\emph{identifiability} and \\emph{achievability} aspects.","Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them.","Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees.","By drawing novel connections between \\emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \\emph{score-based class of algorithms} that ensures both identifiability and achievability.","First, the paper focuses on \\emph{linear} transformations and shows that one stochastic hard intervention per node suffices to guarantee identifiability.","It also provides partial identifiability guarantees for soft interventions, including identifiability up to ancestors for general causal models and perfect latent graph recovery for sufficiently non-linear causal models.","Secondly, it focuses on \\emph{general} transformations and shows that two stochastic hard interventions per node suffice for identifiability.","Notably, one does \\emph{not} need to know which pair of interventional environments have the same node intervened."],"url":"http://arxiv.org/abs/2402.00849v1"}
{"created":"2024-02-01 18:38:55","title":"BootsTAP: Bootstrapped Training for Tracking-Any-Point","abstract":"To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%.","sentences":["To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes.","This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to be able to track any point corresponding to a solid surface in a video, potentially densely in space and time.","Large-scale ground-truth training data for TAP is only available in simulation, which currently has limited variety of objects and motion.","In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup.","We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 66.4%, and TAP-Vid-Kinetics from 57.2% to 61.5%."],"url":"http://arxiv.org/abs/2402.00847v1"}
{"created":"2024-02-01 18:35:50","title":"When to Preempt in a Status Update System?","abstract":"We consider a time slotted status update system with an error-free preemptive queue. The goal of the sampler-scheduler pair is to minimize the age of information at the monitor by sampling and transmitting the freshly sampled update packets to the monitor. The sampler-scheduler pair also has a choice to preempt an old update packet from the server and transmit a new update packet to the server. We formulate this problem as a Markov decision process and find the optimal sampling policy. We show that it is optimal for the sampler-scheduler pair to sample a new packet immediately upon the reception of an update packet at the monitor. We also show that the optimal choice for the scheduler is to preempt an update packet in the server, if the age of that packet crosses a fixed threshold. Finally, we find the optimal preemption threshold when the range of the service time of the server is finite, otherwise we find the $\\epsilon$-optimal preemption threshold.","sentences":["We consider a time slotted status update system with an error-free preemptive queue.","The goal of the sampler-scheduler pair is to minimize the age of information at the monitor by sampling and transmitting the freshly sampled update packets to the monitor.","The sampler-scheduler pair also has a choice to preempt an old update packet from the server and transmit a new update packet to the server.","We formulate this problem as a Markov decision process and find the optimal sampling policy.","We show that it is optimal for the sampler-scheduler pair to sample a new packet immediately upon the reception of an update packet at the monitor.","We also show that the optimal choice for the scheduler is to preempt an update packet in the server, if the age of that packet crosses a fixed threshold.","Finally, we find the optimal preemption threshold when the range of the service time of the server is finite, otherwise we find the $\\epsilon$-optimal preemption threshold."],"url":"http://arxiv.org/abs/2402.00845v1"}
{"created":"2024-02-01 18:31:34","title":"Tiny Titans: Can Smaller Large Language Models Punch Above Their Weight in the Real World for Meeting Summarization?","abstract":"Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets. However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources. In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world. In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2). We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets. However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller. This makes compact LLMs like FLAN-T5 a suitable cost-efficient solution for real-world industrial deployment.","sentences":["Large Language Models (LLMs) have demonstrated impressive capabilities to solve a wide range of tasks without being explicitly fine-tuned on task-specific datasets.","However, deploying LLMs in the real world is not trivial, as it requires substantial computing resources.","In this paper, we investigate whether smaller, compact LLMs are a good alternative to the comparatively Larger LLMs2 to address significant costs associated with utilizing LLMs in the real world.","In this regard, we study the meeting summarization task in a real-world industrial environment and conduct extensive experiments by comparing the performance of fine-tuned compact LLMs (e.g., FLAN-T5, TinyLLaMA, LiteLLaMA) with zero-shot larger LLMs (e.g., LLaMA-2, GPT-3.5, PaLM-2).","We observe that most smaller LLMs, even after fine-tuning, fail to outperform larger zero-shot LLMs in meeting summarization datasets.","However, a notable exception is FLAN-T5 (780M parameters), which performs on par or even better than many zero-shot Larger LLMs (from 7B to above 70B parameters), while being significantly smaller.","This makes compact LLMs like FLAN-T5 a suitable cost-efficient solution for real-world industrial deployment."],"url":"http://arxiv.org/abs/2402.00841v1"}
{"created":"2024-02-01 18:29:16","title":"X-CBA: Explainability Aided CatBoosted Anomal-E for Intrusion Detection System","abstract":"The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex. Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks. However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making. This transparency gap in IDS research is significant, affecting confidence and accountability. To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology. Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to improve detection capabilities and adapt to novel threats. Through empirical testing, we establish that our approach not only achieves high accuracy with 99.47% in threat detection but also advances the field by providing clear, actionable explanations of its analytical outcomes. This research also aims to bridge the current gap and facilitate the broader integration of ML/DL technologies in cybersecurity defenses by offering a local and global explainability solution that is both precise and interpretable.","sentences":["The effectiveness of Intrusion Detection Systems (IDS) is critical in an era where cyber threats are becoming increasingly complex.","Machine learning (ML) and deep learning (DL) models provide an efficient and accurate solution for identifying attacks and anomalies in computer networks.","However, using ML and DL models in IDS has led to a trust deficit due to their non-transparent decision-making.","This transparency gap in IDS research is significant, affecting confidence and accountability.","To address, this paper introduces a novel Explainable IDS approach, called X-CBA, that leverages the structural advantages of Graph Neural Networks (GNNs) to effectively process network traffic data, while also adapting a new Explainable AI (XAI) methodology.","Unlike most GNN-based IDS that depend on labeled network traffic and node features, thereby overlooking critical packet-level information, our approach leverages a broader range of traffic data through network flows, including edge attributes, to improve detection capabilities and adapt to novel threats.","Through empirical testing, we establish that our approach not only achieves high accuracy with 99.47% in threat detection but also advances the field by providing clear, actionable explanations of its analytical outcomes.","This research also aims to bridge the current gap and facilitate the broader integration of ML/DL technologies in cybersecurity defenses by offering a local and global explainability solution that is both precise and interpretable."],"url":"http://arxiv.org/abs/2402.00839v1"}
{"created":"2024-02-01 18:28:55","title":"OLMo: Accelerating the Science of Language Models","abstract":"Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.","sentences":["Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings.","As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed.","Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs.","To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling.","Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code.","We hope this release will empower and strengthen the open research community and inspire a new wave of innovation."],"url":"http://arxiv.org/abs/2402.00838v1"}
{"created":"2024-02-01 18:22:32","title":"ALISON: Fast and Effective Stylometric Authorship Obfuscation","abstract":"Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.","sentences":["Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research.","Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier.","AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship.","To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours.","To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation.","We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics.","To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON."],"url":"http://arxiv.org/abs/2402.00835v1"}
{"created":"2024-02-01 18:20:55","title":"Approximating maximum-size properly colored forests","abstract":"In the Properly Colored Spanning Tree problem, we are given an edge-colored undirected graph and the goal is to find a properly colored spanning tree, i.e., a spanning tree in which any two adjacent edges have distinct colors. The problem is interesting not only from a graph coloring point of view, but is also closely related to the Degree Bounded Spanning Tree and (1,2)-Traveling Salesman problems, two classical questions that have attracted considerable interest in combinatorial optimization and approximation theory. Previous work on properly colored spanning trees has mainly focused on determining the existence of such a tree and hence has not considered the question from an algorithmic perspective. We propose an optimization version called Maximum-size Properly Colored Forest problem, which aims to find a properly colored forest with as many edges as possible. We consider the problem in different graph classes and for different numbers of colors, and present polynomial-time approximation algorithms as well as inapproximability results for these settings. Our proof technique relies on the sum of matching matroids defined by the color classes, a connection that might be of independent combinatorial interest.   We also consider the Maximum-size Properly Colored Tree problem, which asks for the maximum size of a properly colored tree not necessarily spanning all the vertices. We show that the optimum is significantly more difficult to approximate than in the forest case, and provide an approximation algorithm for complete multigraphs.","sentences":["In the Properly Colored Spanning Tree problem, we are given an edge-colored undirected graph and the goal is to find a properly colored spanning tree, i.e., a spanning tree in which any two adjacent edges have distinct colors.","The problem is interesting not only from a graph coloring point of view, but is also closely related to the Degree Bounded Spanning Tree and (1,2)-Traveling Salesman problems, two classical questions that have attracted considerable interest in combinatorial optimization and approximation theory.","Previous work on properly colored spanning trees has mainly focused on determining the existence of such a tree and hence has not considered the question from an algorithmic perspective.","We propose an optimization version called Maximum-size Properly Colored Forest problem, which aims to find a properly colored forest with as many edges as possible.","We consider the problem in different graph classes and for different numbers of colors, and present polynomial-time approximation algorithms as well as inapproximability results for these settings.","Our proof technique relies on the sum of matching matroids defined by the color classes, a connection that might be of independent combinatorial interest.   ","We also consider the Maximum-size Properly Colored Tree problem, which asks for the maximum size of a properly colored tree not necessarily spanning all the vertices.","We show that the optimum is significantly more difficult to approximate than in the forest case, and provide an approximation algorithm for complete multigraphs."],"url":"http://arxiv.org/abs/2402.00834v1"}
{"created":"2024-02-01 18:17:37","title":"A YANG-aided Unified Strategy for Black Hole Detection for Backbone Networks","abstract":"Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking. This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks. Furthermore, detecting Black Hole failures in backbone networks is particularly challenging. It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward. Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis. This paper details our method of selecting and analyzing four YANG models relevant to Black Hole detection in ISP networks, focusing on routing protocols and ISP-specific configurations. Our BHMM approach derived from these models demonstrates a 10% improvement in detection accuracy and a 13% increase in packet delivery rate, highlighting the efficiency of our approach. Additionally, we evaluate the Machine Learning approach leveraged with BHMM analysis in two different network settings, a commercial ISP network, and a scientific research-only network topology. This evaluation also demonstrates the practical applicability of our method, yielding significantly improved prediction outcomes in both environments.","sentences":["Despite the crucial importance of addressing Black Hole failures in Internet backbone networks, effective detection strategies in backbone networks are lacking.","This is largely because previous research has been centered on Mobile Ad-hoc Networks (MANETs), which operate under entirely different dynamics, protocols, and topologies, making their findings not directly transferable to backbone networks.","Furthermore, detecting Black Hole failures in backbone networks is particularly challenging.","It requires a comprehensive range of network data due to the wide variety of conditions that need to be considered, making data collection and analysis far from straightforward.","Addressing this gap, our study introduces a novel approach for Black Hole detection in backbone networks using specialized","Yet Another Next Generation (YANG) data models with Black Hole-sensitive Metric Matrix (BHMM) analysis.","This paper details our method of selecting and analyzing four YANG models relevant to Black Hole detection in ISP networks, focusing on routing protocols and ISP-specific configurations.","Our BHMM approach derived from these models demonstrates a 10% improvement in detection accuracy and a 13% increase in packet delivery rate, highlighting the efficiency of our approach.","Additionally, we evaluate the Machine Learning approach leveraged with BHMM analysis in two different network settings, a commercial ISP network, and a scientific research-only network topology.","This evaluation also demonstrates the practical applicability of our method, yielding significantly improved prediction outcomes in both environments."],"url":"http://arxiv.org/abs/2402.00831v1"}
{"created":"2024-02-01 18:17:29","title":"Common errors in Generative AI systems used for knowledge extraction in the climate action domain","abstract":"Large Language Models (LLMs) and, more specifically, the Generative Pre-Trained Transformers (GPT) can help stakeholders in climate action explore digital knowledge bases and extract and utilize climate action knowledge in a sustainable manner. However, LLMs are \"probabilistic models of knowledge bases\" that excel at generating convincing texts but cannot be entirely relied upon due to the probabilistic nature of the information produced. This brief report illustrates the problem space with examples of LLM responses to some of the questions of relevance to climate action.","sentences":["Large Language Models (LLMs) and, more specifically, the Generative Pre-Trained Transformers (GPT) can help stakeholders in climate action explore digital knowledge bases and extract and utilize climate action knowledge in a sustainable manner.","However, LLMs are \"probabilistic models of knowledge bases\" that excel at generating convincing texts but cannot be entirely relied upon due to the probabilistic nature of the information produced.","This brief report illustrates the problem space with examples of LLM responses to some of the questions of relevance to climate action."],"url":"http://arxiv.org/abs/2402.00830v1"}
{"created":"2024-02-01 18:16:53","title":"The En Route Truck-Drone Delivery Problem","abstract":"We study the truck-drone cooperative delivery problem in a setting where a single truck carrying a drone travels at constant speed on a straight-line trajectory/street. Delivery to clients located in the plane and not on the truck's trajectory is performed by the drone, which has limited carrying capacity and flying range, and whose battery can be recharged when on the truck. We show that the problem of maximizing the number of deliveries is strongly NP-hard even in this simple setting. We present a 2-approximation algorithm for the problem, and an optimal algorithm for a non-trivial family of instances.","sentences":["We study the truck-drone cooperative delivery problem in a setting where a single truck carrying a drone travels at constant speed on a straight-line trajectory/street.","Delivery to clients located in the plane and not on the truck's trajectory is performed by the drone, which has limited carrying capacity and flying range, and whose battery can be recharged when on the truck.","We show that the problem of maximizing the number of deliveries is strongly NP-hard even in this simple setting.","We present a 2-approximation algorithm for the problem, and an optimal algorithm for a non-trivial family of instances."],"url":"http://arxiv.org/abs/2402.00829v1"}
{"created":"2024-02-01 18:14:42","title":"Emo-Avatar: Efficient Monocular Video Style Avatar through Texture Rendering","abstract":"Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision. While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating and editing drivable portraits, these approaches face several challenges. They often rely heavily on large datasets, require extensive customization processes, and frequently result in reduced image quality. To address the above problems, we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's capacity for producing dynamic, drivable portrait videos. We proposed a two-stage deferred neural rendering pipeline. In the first stage, we utilize few-shot PTI initialization to initialize the StyleGAN generator through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the target portrait. In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance StyleGAN's ability to generate complete and upper body for portrait video rendering. Emo-Avatar reduces style customization time from hours to merely 5 minutes compared with existing methods. In addition, Emo-Avatar requires only a single reference image for editing and employs region-aware contrastive learning with semantic invariant CLIP guidance, ensuring consistent high-resolution output and identity preservation. Through both quantitative and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of training efficiency, rendering quality and editability in self- and cross-reenactment.","sentences":["Artistic video portrait generation is a significant and sought-after task in the fields of computer graphics and vision.","While various methods have been developed that integrate NeRFs or StyleGANs with instructional editing models for creating and editing drivable portraits, these approaches face several challenges.","They often rely heavily on large datasets, require extensive customization processes, and frequently result in reduced image quality.","To address the above problems, we propose the Efficient Monotonic Video Style Avatar (Emo-Avatar) through deferred neural rendering that enhances StyleGAN's capacity for producing dynamic, drivable portrait videos.","We proposed a two-stage deferred neural rendering pipeline.","In the first stage, we utilize few-shot PTI initialization to initialize the StyleGAN generator through several extreme poses sampled from the video to capture the consistent representation of aligned faces from the target portrait.","In the second stage, we propose a Laplacian pyramid for high-frequency texture sampling from UV maps deformed by dynamic flow of expression for motion-aware texture prior integration to provide torso features to enhance StyleGAN's ability to generate complete and upper body for portrait video rendering.","Emo-Avatar reduces style customization time from hours to merely 5 minutes compared with existing methods.","In addition, Emo-Avatar requires only a single reference image for editing and employs region-aware contrastive learning with semantic invariant CLIP guidance, ensuring consistent high-resolution output and identity preservation.","Through both quantitative and qualitative assessments, Emo-Avatar demonstrates superior performance over existing methods in terms of training efficiency, rendering quality and editability in self- and cross-reenactment."],"url":"http://arxiv.org/abs/2402.00827v1"}
{"created":"2024-02-01 18:07:33","title":"SLIM: Skill Learning with Multiple Critics","abstract":"Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, surpassing the state-of-the-art approaches for skill discovery by a large margin.","sentences":["Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment.","Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation.","As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors.","To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation.","Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills.","Furthermore, in the context of tabletop manipulation, we demonstrate the applicability of our novel skill discovery approach to acquire safe and efficient motor primitives in a hierarchical reinforcement learning fashion and leverage them through planning, surpassing the state-of-the-art approaches for skill discovery by a large margin."],"url":"http://arxiv.org/abs/2402.00823v1"}
{"created":"2024-02-01 18:05:38","title":"WiOpen: A Robust Wi-Fi-based Open-set Gesture Recognition Framework","abstract":"Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition. However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training. This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing. To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework. Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing. This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data. Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise. To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries. Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the CSI ratio. Next, it designs the OSGR network based on an uncertainty quantification method. Throughout the learning process, this network effectively mitigates uncertainty stemming from domains. Ultimately, the network leverages relationships among samples' neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR. Comprehensive experiments on publicly accessible datasets confirm WiOpen's effectiveness. Notably, WiOpen also demonstrates superiority in cross-domain tasks when compared to state-of-the-art approaches.","sentences":["Recent years have witnessed a growing interest in Wi-Fi-based gesture recognition.","However, existing works have predominantly focused on closed-set paradigms, where all testing gestures are predefined during training.","This poses a significant challenge in real-world applications, as unseen gestures might be misclassified as known classes during testing.","To address this issue, we propose WiOpen, a robust Wi-Fi-based Open-Set Gesture Recognition (OSGR) framework.","Implementing OSGR requires addressing challenges caused by the unique uncertainty in Wi-Fi sensing.","This uncertainty, resulting from noise and domains, leads to widely scattered and irregular data distributions in collected Wi-Fi sensing data.","Consequently, data ambiguity between classes and challenges in defining appropriate decision boundaries to identify unknowns arise.","To tackle these challenges, WiOpen adopts a two-fold approach to eliminate uncertainty and define precise decision boundaries.","Initially, it addresses uncertainty induced by noise during data preprocessing by utilizing the CSI ratio.","Next, it designs the OSGR network based on an uncertainty quantification method.","Throughout the learning process, this network effectively mitigates uncertainty stemming from domains.","Ultimately, the network leverages relationships among samples' neighbors to dynamically define open-set decision boundaries, successfully realizing OSGR.","Comprehensive experiments on publicly accessible datasets confirm WiOpen's effectiveness.","Notably, WiOpen also demonstrates superiority in cross-domain tasks when compared to state-of-the-art approaches."],"url":"http://arxiv.org/abs/2402.00822v1"}
{"created":"2024-02-01 17:45:26","title":"Position Paper: Bayesian Deep Learning in the Age of Large-Scale AI","abstract":"In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets. However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention. Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings. This paper posits that BDL can elevate the capabilities of deep learning. It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles. Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential.","sentences":["In the current landscape of deep learning research, there is a predominant emphasis on achieving high predictive accuracy in supervised tasks involving large image and language datasets.","However, a broader perspective reveals a multitude of overlooked metrics, tasks, and data types, such as uncertainty, active and continual learning, and scientific data, that demand attention.","Bayesian deep learning (BDL) constitutes a promising avenue, offering advantages across these diverse settings.","This paper posits that BDL can elevate the capabilities of deep learning.","It revisits the strengths of BDL, acknowledges existing challenges, and highlights some exciting research avenues aimed at addressing these obstacles.","Looking ahead, the discussion focuses on possible ways to combine large-scale foundation models with BDL to unlock their full potential."],"url":"http://arxiv.org/abs/2402.00809v1"}
{"created":"2024-02-01 17:44:46","title":"Exploring the Dynamics between Cobot's Production Rhythm, Locus of Control and Emotional State in a Collaborative Assembly Scenario","abstract":"In industrial scenarios, there is widespread use of collaborative robots (cobots), and growing interest is directed at evaluating and measuring the impact of some characteristics of the cobot on the human factor. In the present pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 - Adapted to the participant's pace) of a cobot has on the Experiential Locus of Control (ELoC) and the emotional state of 31 participants has been examined. The operators' performance, the degree of basic internal Locus of Control, and the attitude towards the robots were also considered. No difference was found regarding the emotional state and the ELoC in the three conditions, but considering the other psychological variables, a more complex situation emerges. Overall, results seem to indicate a need to consider the person's psychological characteristics to offer a differentiated and optimal interaction experience.","sentences":["In industrial scenarios, there is widespread use of collaborative robots (cobots), and growing interest is directed at evaluating and measuring the impact of some characteristics of the cobot on the human factor.","In the present pilot study, the effect that the production rhythm (C1 - Slow, C2 - Fast, C3 - Adapted to the participant's pace) of a cobot has on the Experiential Locus of Control (ELoC) and the emotional state of 31 participants has been examined.","The operators' performance, the degree of basic internal Locus of Control, and the attitude towards the robots were also considered.","No difference was found regarding the emotional state and the ELoC in the three conditions, but considering the other psychological variables, a more complex situation emerges.","Overall, results seem to indicate a need to consider the person's psychological characteristics to offer a differentiated and optimal interaction experience."],"url":"http://arxiv.org/abs/2402.00808v1"}
{"created":"2024-02-01 17:44:11","title":"Distilling Conditional Diffusion Models for Offline Reinforcement Learning through Trajectory Stitching","abstract":"Deep generative models have recently emerged as an effective approach to offline reinforcement learning. However, their large model size poses challenges in computation. We address this issue by proposing a knowledge distillation method based on data augmentation. In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator. Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks.","sentences":["Deep generative models have recently emerged as an effective approach to offline reinforcement learning.","However, their large model size poses challenges in computation.","We address this issue by proposing a knowledge distillation method based on data augmentation.","In particular, high-return trajectories are generated from a conditional diffusion model, and they are blended with the original trajectories through a novel stitching algorithm that leverages a new reward generator.","Applying the resulting dataset to behavioral cloning, the learned shallow policy whose size is much smaller outperforms or nearly matches deep generative planners on several D4RL benchmarks."],"url":"http://arxiv.org/abs/2402.00807v1"}
{"created":"2024-02-01 17:40:10","title":"Signal Quality Auditing for Time-series Data","abstract":"Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts. SQA is vital for addressing \"silent failures\" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences. We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data. We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment. We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data. To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility. The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses.","sentences":["Signal quality assessment (SQA) is required for monitoring the reliability of data acquisition systems, especially in AI-driven Predictive Maintenance (PMx) application contexts.","SQA is vital for addressing \"silent failures\" of data acquisition hardware and software, which when unnoticed, misinform the users of data, creating the risk for incorrect decisions with unintended or even catastrophic consequences.","We have developed an open-source software implementation of signal quality indices (SQIs) for the analysis of time-series data.","We codify a range of SQIs, demonstrate them using established benchmark data, and show that they can be effective for signal quality assessment.","We also study alternative approaches to denoising time-series data in an attempt to improve the quality of the already degraded signal, and evaluate them empirically on relevant real-world data.","To our knowledge, our software toolkit is the first to provide an open source implementation of a broad range of signal quality assessment and improvement techniques validated on publicly available benchmark data for ease of reproducibility.","The generality of our framework can be easily extended to assessing reliability of arbitrary time-series measurements in complex systems, especially when morphological patterns of the waveform shapes and signal periodicity are of key interest in downstream analyses."],"url":"http://arxiv.org/abs/2402.00803v1"}
{"created":"2024-02-01 17:30:50","title":"Formal-LLM: Integrating Formal Language and Natural Language for Controllable LLM-based Agents","abstract":"Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks. However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents. In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language. Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton. A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable. We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans. Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential. The work is open-sourced at https://github.com/agiresearch/Formal-LLM.","sentences":["Recent advancements on Large Language Models (LLMs) enable AI Agents to automatically generate and execute multi-step plans to solve complex tasks.","However, since LLM's content generation process is hardly controllable, current LLM-based agents frequently generate invalid or non-executable plans, which jeopardizes the performance of the generated plans and corrupts users' trust in LLM-based agents.","In response, this paper proposes a novel ``Formal-LLM'' framework for LLM-based agents by integrating the expressiveness of natural language and the precision of formal language.","Specifically, the framework allows human users to express their requirements or constraints for the planning process as an automaton.","A stack-based LLM plan generation process is then conducted under the supervision of the automaton to ensure that the generated plan satisfies the constraints, making the planning process controllable.","We conduct experiments on both benchmark tasks and practical real-life tasks, and our framework achieves over 50% overall performance increase, which validates the feasibility and effectiveness of employing Formal-LLM to guide the plan generation of agents, preventing the agents from generating invalid and unsuccessful plans.","Further, more controllable LLM-based agents can facilitate the broader utilization of LLM in application scenarios where high validity of planning is essential.","The work is open-sourced at https://github.com/agiresearch/Formal-LLM."],"url":"http://arxiv.org/abs/2402.00798v1"}
{"created":"2024-02-01 17:28:10","title":"LLMs learn governing principles of dynamical systems, revealing an in-context neural scaling law","abstract":"Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting. However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models. In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest. Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering. Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law. Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs.","sentences":["Pretrained large language models (LLMs) are surprisingly effective at performing zero-shot tasks, including time-series forecasting.","However, understanding the mechanisms behind such capabilities remains highly challenging due to the complexity of the models.","In this paper, we study LLMs' ability to extrapolate the behavior of dynamical systems whose evolution is governed by principles of physical interest.","Our results show that LLaMA 2, a language model trained primarily on texts, achieves accurate predictions of dynamical system time series without fine-tuning or prompt engineering.","Moreover, the accuracy of the learned physical rules increases with the length of the input context window, revealing an in-context version of neural scaling law.","Along the way, we present a flexible and efficient algorithm for extracting probability density functions of multi-digit numbers directly from LLMs."],"url":"http://arxiv.org/abs/2402.00795v1"}
{"created":"2024-02-01 17:25:51","title":"ReAGent: Towards A Model-agnostic Feature Attribution Method for Generative Language Models","abstract":"Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions. Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks. However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively. Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks. This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute. To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent). Our method updates the token importance distribution in a recursive manner. For each update, we compute the difference in the probability distribution over the vocabulary for predicting the next token between using the original input and using a modified version where a part of the input is replaced with RoBERTa predictions. Our intuition is that replacing an important token in the context should have resulted in a larger change in the model's confidence in predicting the token than replacing an unimportant token. Our method can be universally applied to any generative LM without accessing internal model weights or additional training and fine-tuning, as most other FAs require. We extensively compare the faithfulness of ReAGent with seven popular FAs across six decoder-only LMs of various sizes. The results show that our method consistently provides more faithful token importance distributions.","sentences":["Feature attribution methods (FAs), such as gradients and attention, are widely employed approaches to derive the importance of all input features to the model predictions.","Existing work in natural language processing has mostly focused on developing and testing FAs for encoder-only language models (LMs) in classification tasks.","However, it is unknown if it is faithful to use these FAs for decoder-only models on text generation, due to the inherent differences between model architectures and task settings respectively.","Moreover, previous work has demonstrated that there is no `one-wins-all' FA across models and tasks.","This makes the selection of a FA computationally expensive for large LMs since input importance derivation often requires multiple forward and backward passes including gradient computations that might be prohibitive even with access to large compute.","To address these issues, we present a model-agnostic FA for generative LMs called Recursive Attribution Generator (ReAGent).","Our method updates the token importance distribution in a recursive manner.","For each update, we compute the difference in the probability distribution over the vocabulary for predicting the next token between using the original input and using a modified version where a part of the input is replaced with RoBERTa predictions.","Our intuition is that replacing an important token in the context should have resulted in a larger change in the model's confidence in predicting the token than replacing an unimportant token.","Our method can be universally applied to any generative LM without accessing internal model weights or additional training and fine-tuning, as most other FAs require.","We extensively compare the faithfulness of ReAGent with seven popular FAs across six decoder-only LMs of various sizes.","The results show that our method consistently provides more faithful token importance distributions."],"url":"http://arxiv.org/abs/2402.00794v1"}
{"created":"2024-02-01 17:23:54","title":"Distinguishing the Indistinguishable: Human Expertise in Algorithmic Prediction","abstract":"We introduce a novel framework for incorporating human expertise into algorithmic predictions. Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm. We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data. We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor. We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante). In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population. Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration.","sentences":["We introduce a novel framework for incorporating human expertise into algorithmic predictions.","Our approach focuses on the use of human judgment to distinguish inputs which `look the same' to any feasible predictive algorithm.","We argue that this framing clarifies the problem of human/AI collaboration in prediction tasks, as experts often have access to information -- particularly subjective information -- which is not encoded in the algorithm's training data.","We use this insight to develop a set of principled algorithms for selectively incorporating human feedback only when it improves the performance of any feasible predictor.","We find empirically that although algorithms often outperform their human counterparts on average, human judgment can significantly improve algorithmic predictions on specific instances (which can be identified ex-ante).","In an X-ray classification task, we find that this subset constitutes nearly 30% of the patient population.","Our approach provides a natural way of uncovering this heterogeneity and thus enabling effective human-AI collaboration."],"url":"http://arxiv.org/abs/2402.00793v1"}
{"created":"2024-02-01 17:23:21","title":"Hausdorff Reductions and the Exponential Hierarchies","abstract":"The Strong Exponential Hierarchy $SEH$ was shown to collapse to $P^{NExp}$ by Hemachandra by proving $P^{NExp} = NP^{NExp}$ via a census argument. Nonetheless, Hemachandra also asked for certificate-based and alternating Turing machine characterizations of the $SEH$ levels, in the hope that these might have revealed deeper structural reasons behind the collapse. These open questions have thus far remained unanswered.   To close them, by building upon the notion of Hausdorff reductions, we investigate a natural normal form for the intermediate levels of the (generalized) exponential hierarchies, i.e., the single-, the double-Exponential Hierarchy, and so on. Although the two characterizations asked for derive from our Hausdorff characterization, it is nevertheless from the latter that a surprising structural reason behind the collapse of $SEH$ is uncovered as a consequence of a very general result: the intermediate levels of the exponential hierarchies are precisely characterized by specific \"Hausdorff classes\", which define these levels without resorting to oracle machines. By this, contrarily to oracle classes, which may have different shapes for a same class (e.g., $P^{NP}_{||} = P^{NP[Log]} = LogSpace^{NP}$), hierarchy intermediate levels are univocally identified by Hausdorff classes (under the hypothesis of no hierarchy collapse). In fact, we show that the rather simple reason behind many equivalences of oracle classes is that they just refer to different ways of deciding the languages of a same Hausdorff class, and this happens also for $P^{NExp}$ and $NP^{NExp}$.   In addition, via Hausdorff classes, we define complete problems for various intermediate levels of the exponential hierarchies. Through these, we obtain matching lower-bounds for problems known to be in $P^{NExp[Log]}$, but whose hardness was left open due to the lack of known $P^{NExp[Log]}$-complete problems.","sentences":["The Strong Exponential Hierarchy $SEH$ was shown to collapse to $P^{NExp}$ by Hemachandra by proving $P^{NExp} = NP^{NExp}$ via a census argument.","Nonetheless, Hemachandra also asked for certificate-based and alternating Turing machine characterizations of the $SEH$ levels, in the hope that these might have revealed deeper structural reasons behind the collapse.","These open questions have thus far remained unanswered.   ","To close them, by building upon the notion of Hausdorff reductions, we investigate a natural normal form for the intermediate levels of the (generalized) exponential hierarchies, i.e., the single-, the double-Exponential Hierarchy, and so on.","Although the two characterizations asked for derive from our Hausdorff characterization, it is nevertheless from the latter that a surprising structural reason behind the collapse of $SEH$ is uncovered as a consequence of a very general result: the intermediate levels of the exponential hierarchies are precisely characterized by specific \"Hausdorff classes\", which define these levels without resorting to oracle machines.","By this, contrarily to oracle classes, which may have different shapes for a same class (e.g., $P^{NP}_{||} = P^{NP[Log]} = LogSpace^{NP}$), hierarchy intermediate levels are univocally identified by Hausdorff classes (under the hypothesis of no hierarchy collapse).","In fact, we show that the rather simple reason behind many equivalences of oracle classes is that they just refer to different ways of deciding the languages of a same Hausdorff class, and this happens also for $P^{NExp}$ and $NP^{NExp}$.   In addition, via Hausdorff classes, we define complete problems for various intermediate levels of the exponential hierarchies.","Through these, we obtain matching lower-bounds for problems known to be in $P^{NExp[Log]}$, but whose hardness was left open due to the lack of known $P^{NExp[Log]}$-complete problems."],"url":"http://arxiv.org/abs/2402.00791v1"}
{"created":"2024-02-01 17:22:48","title":"From Pre-Quantum to Post-Quantum IoT Security: A Survey on Quantum-Resistant Cryptosystems for the Internet of Things","abstract":"This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated. Thus, this paper is aimed at providing a wide view of post-quantum IoT security and give useful guidelines to the future post-quantum IoT developers.","sentences":["This article provides a survey on what can be called post-quantum IoT systems (IoT systems protected from the currently known quantum computing attacks): the main post-quantum cryptosystems and initiatives are reviewed, the most relevant IoT architectures and challenges are analyzed, and the expected future trends are indicated.","Thus, this paper is aimed at providing a wide view of post-quantum IoT security and give useful guidelines to the future post-quantum IoT developers."],"url":"http://arxiv.org/abs/2402.00790v1"}
{"created":"2024-02-01 17:21:53","title":"Graph-Mamba: Towards Long-Range Graph Sequence Modeling with Selective State Spaces","abstract":"Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers. Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs. Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning. State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data. However, adapting SSMs to non-sequential graph data presents a notable challenge. In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism. Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance. Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption. The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba.","sentences":["Attention mechanisms have been widely used to capture long-range dependencies among nodes in Graph Transformers.","Bottlenecked by the quadratic computational cost, attention mechanisms fail to scale in large graphs.","Recent improvements in computational efficiency are mainly achieved by attention sparsification with random or heuristic-based graph subsampling, which falls short in data-dependent context reasoning.","State space models (SSMs), such as Mamba, have gained prominence for their effectiveness and efficiency in modeling long-range dependencies in sequential data.","However, adapting SSMs to non-sequential graph data presents a notable challenge.","In this work, we introduce Graph-Mamba, the first attempt to enhance long-range context modeling in graph networks by integrating a Mamba block with the input-dependent node selection mechanism.","Specifically, we formulate graph-centric node prioritization and permutation strategies to enhance context-aware reasoning, leading to a substantial improvement in predictive performance.","Extensive experiments on ten benchmark datasets demonstrate that Graph-Mamba outperforms state-of-the-art methods in long-range graph prediction tasks, with a fraction of the computational cost in both FLOPs and GPU memory consumption.","The code and models are publicly available at https://github.com/bowang-lab/Graph-Mamba."],"url":"http://arxiv.org/abs/2402.00789v1"}
{"created":"2024-02-01 17:21:45","title":"Learning and Calibrating Heterogeneous Bounded Rational Market Behaviour with Multi-Agent Reinforcement Learning","abstract":"Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis. However, a critical concern is the manual definition of behavioural rules in ABMs. Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification. This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents. However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled. To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework. The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation. Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods. To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels. Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours. We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability.","sentences":["Agent-based models (ABMs) have shown promise for modelling various real world phenomena incompatible with traditional equilibrium analysis.","However, a critical concern is the manual definition of behavioural rules in ABMs.","Recent developments in multi-agent reinforcement learning (MARL) offer a way to address this issue from an optimisation perspective, where agents strive to maximise their utility, eliminating the need for manual rule specification.","This learning-focused approach aligns with established economic and financial models through the use of rational utility-maximising agents.","However, this representation departs from the fundamental motivation for ABMs: that realistic dynamics emerging from bounded rationality and agent heterogeneity can be modelled.","To resolve this apparent disparity between the two approaches, we propose a novel technique for representing heterogeneous processing-constrained agents within a MARL framework.","The proposed approach treats agents as constrained optimisers with varying degrees of strategic skills, permitting departure from strict utility maximisation.","Behaviour is learnt through repeated simulations with policy gradients to adjust action likelihoods.","To allow efficient computation, we use parameterised shared policy learning with distributions of agent skill levels.","Shared policy learning avoids the need for agents to learn individual policies yet still enables a spectrum of bounded rational behaviours.","We validate our model's effectiveness using real-world data on a range of canonical $n$-agent settings, demonstrating significantly improved predictive capability."],"url":"http://arxiv.org/abs/2402.00787v1"}
{"created":"2024-02-01 17:17:55","title":"CroissantLLM: A Truly Bilingual French-English Language Model","abstract":"We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models. We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives. This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models.","sentences":["We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware.","To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets.","We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources.","To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language.","Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models.","We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives.","This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models."],"url":"http://arxiv.org/abs/2402.00786v1"}
{"created":"2024-02-01 17:10:35","title":"Dense Reward for Free in Reinforcement Learning from Human Feedback","abstract":"Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance. Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion. As an auto-regressive process, the LLM has to take many \"actions\" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning. In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture. We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling. We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged. Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has been credited as the key advance that has allowed Large Language Models (LLMs) to effectively follow instructions and produce useful assistance.","Classically, this involves generating completions from the LLM in response to a query before using a separate reward model to assign a score to the full completion.","As an auto-regressive process, the LLM has to take many \"actions\" (selecting individual tokens) and only receives a single, sparse reward at the end of an episode, a setup that is known to be difficult to optimise in traditional reinforcement learning.","In this work we leverage the fact that the reward model contains more information than just its scalar output, in particular, it calculates an attention map over tokens as part of the transformer architecture.","We use these attention weights to redistribute the reward along the whole completion, effectively densifying the signal and highlighting the most important tokens, all without incurring extra computational cost or requiring any additional modelling.","We demonstrate that, theoretically, this approach is equivalent to potential-based reward shaping, ensuring that the optimal policy remains unchanged.","Empirically, we show that it stabilises training, accelerates the rate of learning, and, in practical cases, may lead to better local optima."],"url":"http://arxiv.org/abs/2402.00782v1"}
{"created":"2024-02-01 17:04:41","title":"Adaptive Control for Triadic Human-Robot-FES Collaboration in Gait Rehabilitation: A Pilot Study","abstract":"The hybridisation of robot-assisted gait training and functional electrical stimulation (FES) can provide numerous physiological benefits to neurological patients. However, the design of an effective hybrid controller poses significant challenges. In this over-actuated system, it is extremely difficult to find the right balance between robotic assistance and FES that will provide personalised assistance, prevent muscle fatigue and encourage the patient's active participation in order to accelerate recovery. In this paper, we present an adaptive hybrid robot-FES controller to do this and enable the triadic collaboration between the patient, the robot and FES. A patient-driven controller is designed where the voluntary movement of the patient is prioritised and assistance is provided using FES and the robot in a hierarchical order depending on the patient's performance and their muscles' fitness. The performance of this hybrid adaptive controller is tested in simulation and on one healthy subject. Our results indicate an increase in tracking performance with lower overall assistance, and less muscle fatigue when the hybrid adaptive controller is used, compared to its non adaptive equivalent. This suggests that our hybrid adaptive controller may be able to adapt to the behaviour of the user to provide assistance as needed and prevent the early termination of physical therapy due to muscle fatigue.","sentences":["The hybridisation of robot-assisted gait training and functional electrical stimulation (FES) can provide numerous physiological benefits to neurological patients.","However, the design of an effective hybrid controller poses significant challenges.","In this over-actuated system, it is extremely difficult to find the right balance between robotic assistance and FES that will provide personalised assistance, prevent muscle fatigue and encourage the patient's active participation in order to accelerate recovery.","In this paper, we present an adaptive hybrid robot-FES controller to do this and enable the triadic collaboration between the patient, the robot and FES.","A patient-driven controller is designed where the voluntary movement of the patient is prioritised and assistance is provided using FES and the robot in a hierarchical order depending on the patient's performance and their muscles' fitness.","The performance of this hybrid adaptive controller is tested in simulation and on one healthy subject.","Our results indicate an increase in tracking performance with lower overall assistance, and less muscle fatigue when the hybrid adaptive controller is used, compared to its non adaptive equivalent.","This suggests that our hybrid adaptive controller may be able to adapt to the behaviour of the user to provide assistance as needed and prevent the early termination of physical therapy due to muscle fatigue."],"url":"http://arxiv.org/abs/2402.00775v1"}
{"created":"2024-02-01 16:59:59","title":"Mixed Static and Reconfigurable Metasurface Deployment in Indoor Dense Spaces: How Much Reconfigurability is Needed?","abstract":"In this paper, we investigate how metasurfaces can be deployed to deliver high data rates in a millimeter-wave (mmWave) indoor dense space with many blocking objects. These surfaces can either be static metasurfaces (SMSs) that reflect with fixed phase-shifts or reconfigurable intelligent surfaces (RISs) that can reconfigure their phase-shifts to the currently served user. The latter comes with an increased power, cabling, and signaling cost. To see how reconfigurability affects the network performance, we propose an iterative algorithm based on the feasible point pursuit successive convex approximation method. We jointly optimize the types and phase-shifts of the surfaces and the time portion allocated to each user equipment to maximize the minimum data rate achieved by the network. Our numerical results demonstrate that the minimum data rate improves as more RISs are introduced but the gain diminishes after some point. Therefore, introducing more reconfigurability is not always necessary. Another result shows that to reach the same data rate achieved by using 22 SMSs, at least 18 RISs are needed. This suggests that when it is costly to deploy many RISs, as an inexpensive alternative solution, one can reach the same data rate just by densely deploying more SMSs.","sentences":["In this paper, we investigate how metasurfaces can be deployed to deliver high data rates in a millimeter-wave (mmWave) indoor dense space with many blocking objects.","These surfaces can either be static metasurfaces (SMSs) that reflect with fixed phase-shifts or reconfigurable intelligent surfaces (RISs) that can reconfigure their phase-shifts to the currently served user.","The latter comes with an increased power, cabling, and signaling cost.","To see how reconfigurability affects the network performance, we propose an iterative algorithm based on the feasible point pursuit successive convex approximation method.","We jointly optimize the types and phase-shifts of the surfaces and the time portion allocated to each user equipment to maximize the minimum data rate achieved by the network.","Our numerical results demonstrate that the minimum data rate improves as more RISs are introduced but the gain diminishes after some point.","Therefore, introducing more reconfigurability is not always necessary.","Another result shows that to reach the same data rate achieved by using 22 SMSs, at least 18 RISs are needed.","This suggests that when it is costly to deploy many RISs, as an inexpensive alternative solution, one can reach the same data rate just by densely deploying more SMSs."],"url":"http://arxiv.org/abs/2402.00771v1"}
{"created":"2024-02-01 16:58:11","title":"AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning","abstract":"Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity. However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications. Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps. Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality. Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation). we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed. We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results. Experimental results validate the effectiveness of our proposed method. Code and weights will be made public. More details are available at https://github.com/G-U-N/AnimateLCM.","sentences":["Video diffusion models has been gaining increasing attention for its ability to produce videos that are both coherent and of high fidelity.","However, the iterative denoising process makes it computationally intensive and time-consuming, thus limiting its applications.","Inspired by the Consistency Model (CM) that distills pretrained image diffusion models to accelerate the sampling with minimal steps and its successful extension Latent Consistency Model (LCM) on conditional image generation, we propose AnimateLCM, allowing for high-fidelity video generation within minimal steps.","Instead of directly conducting consistency learning on the raw video dataset, we propose a decoupled consistency learning strategy that decouples the distillation of image generation priors and motion generation priors, which improves the training efficiency and enhance the generation visual quality.","Additionally, to enable the combination of plug-and-play adapters in stable diffusion community to achieve various functions (e.g., ControlNet for controllable generation).","we propose an efficient strategy to adapt existing adapters to our distilled text-conditioned video consistency model or train adapters from scratch without harming the sampling speed.","We validate the proposed strategy in image-conditioned video generation and layout-conditioned video generation, all achieving top-performing results.","Experimental results validate the effectiveness of our proposed method.","Code and weights will be made public.","More details are available at https://github.com/G-U-N/AnimateLCM."],"url":"http://arxiv.org/abs/2402.00769v1"}
{"created":"2024-02-01 16:53:15","title":"To Search or To Gen? Exploring the Synergy between Generative AI and Web Search in Programming","abstract":"The convergence of generative AI and web search is reshaping problem-solving for programmers. However, the lack of understanding regarding their interplay in the information-seeking process often leads programmers to perceive them as alternatives rather than complementary tools. To analyze this interaction and explore their synergy, we conducted an interview study with eight experienced programmers. Drawing from the results and literature, we have identified three major challenges and proposed three decision-making stages, each with its own relevant factors. Additionally, we present a comprehensive process model that captures programmers' interaction patterns. This model encompasses decision-making stages, the information-foraging loop, and cognitive activities during system interaction, offering a holistic framework to comprehend and optimize the use of these convergent tools in programming.","sentences":["The convergence of generative AI and web search is reshaping problem-solving for programmers.","However, the lack of understanding regarding their interplay in the information-seeking process often leads programmers to perceive them as alternatives rather than complementary tools.","To analyze this interaction and explore their synergy, we conducted an interview study with eight experienced programmers.","Drawing from the results and literature, we have identified three major challenges and proposed three decision-making stages, each with its own relevant factors.","Additionally, we present a comprehensive process model that captures programmers' interaction patterns.","This model encompasses decision-making stages, the information-foraging loop, and cognitive activities during system interaction, offering a holistic framework to comprehend and optimize the use of these convergent tools in programming."],"url":"http://arxiv.org/abs/2402.00764v1"}
{"created":"2024-02-01 16:52:21","title":"360-GS: Layout-guided Panoramic Gaussian Splatting For Indoor Roaming","abstract":"3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings. This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians. However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians. In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality. In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views. To address these issues, we propose 360-GS, a novel $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs. Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections. This adaptation enables the representation of the projection using Gaussians. We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene. Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios.","sentences":["3D Gaussian Splatting (3D-GS) has recently attracted great attention with real-time and photo-realistic renderings.","This technique typically takes perspective images as input and optimizes a set of 3D elliptical Gaussians by splatting them onto the image planes, resulting in 2D Gaussians.","However, applying 3D-GS to panoramic inputs presents challenges in effectively modeling the projection onto the spherical surface of ${360^\\circ}$ images using 2D Gaussians.","In practical applications, input panoramas are often sparse, leading to unreliable initialization of 3D Gaussians and subsequent degradation of 3D-GS quality.","In addition, due to the under-constrained geometry of texture-less planes (e.g., walls and floors), 3D-GS struggles to model these flat regions with elliptical Gaussians, resulting in significant floaters in novel views.","To address these issues, we propose 360-GS, a novel $360^{\\circ}$ Gaussian splatting for a limited set of panoramic inputs.","Instead of splatting 3D Gaussians directly onto the spherical surface, 360-GS projects them onto the tangent plane of the unit sphere and then maps them to the spherical projections.","This adaptation enables the representation of the projection using Gaussians.","We guide the optimization of 360-GS by exploiting layout priors within panoramas, which are simple to obtain and contain strong structural information about the indoor scene.","Our experimental results demonstrate that 360-GS allows panoramic rendering and outperforms state-of-the-art methods with fewer artifacts in novel view synthesis, thus providing immersive roaming in indoor scenarios."],"url":"http://arxiv.org/abs/2402.00763v1"}
{"created":"2024-02-01 16:51:11","title":"Control-Theoretic Techniques for Online Adaptation of Deep Neural Networks in Dynamical Systems","abstract":"Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science. In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference. However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls. Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution. To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online. We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel last-layer update laws that guarantee desirable error convergence under various conditions on the time derivative of the DNN input vector. We further show that training the DNN under spectral normalization controls the upper bound of the error trajectories of the online DNN predictions, which is desirable when numerically differentiated quantities or noisy state measurements are input to the DNN. The proposed online DNN adaptation laws are validated in simulation to learn the dynamics of the Van der Pol system under domain shift, where parameters are varied in inference from the training dataset. The simulations demonstrate the effectiveness of using control-theoretic techniques to derive performance improvements and guarantees in DNN-based learning systems.","sentences":["Deep neural networks (DNNs), trained with gradient-based optimization and backpropagation, are currently the primary tool in modern artificial intelligence, machine learning, and data science.","In many applications, DNNs are trained offline, through supervised learning or reinforcement learning, and deployed online for inference.","However, training DNNs with standard backpropagation and gradient-based optimization gives no intrinsic performance guarantees or bounds on the DNN, which is essential for applications such as controls.","Additionally, many offline-training and online-inference problems, such as sim2real transfer of reinforcement learning policies, experience domain shift from the training distribution to the real-world distribution.","To address these stability and transfer learning issues, we propose using techniques from control theory to update DNN parameters online.","We formulate the fully-connected feedforward DNN as a continuous-time dynamical system, and we propose novel last-layer update laws that guarantee desirable error convergence under various conditions on the time derivative of the DNN input vector.","We further show that training the DNN under spectral normalization controls the upper bound of the error trajectories of the online DNN predictions, which is desirable when numerically differentiated quantities or noisy state measurements are input to the DNN.","The proposed online DNN adaptation laws are validated in simulation to learn the dynamics of the Van der Pol system under domain shift, where parameters are varied in inference from the training dataset.","The simulations demonstrate the effectiveness of using control-theoretic techniques to derive performance improvements and guarantees in DNN-based learning systems."],"url":"http://arxiv.org/abs/2402.00761v1"}
{"created":"2024-02-01 16:49:27","title":"Building Expressive and Tractable Probabilistic Generative Models: A Review","abstract":"We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs). We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field. We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field.","sentences":["We present a comprehensive survey of the advancements and techniques in the field of tractable probabilistic generative modeling, primarily focusing on Probabilistic Circuits (PCs).","We provide a unified perspective on the inherent trade-offs between expressivity and the tractability, highlighting the design principles and algorithmic extensions that have enabled building expressive and efficient PCs, and provide a taxonomy of the field.","We also discuss recent efforts to build deep and hybrid PCs by fusing notions from deep neural models, and outline the challenges and open questions that can guide future research in this evolving field."],"url":"http://arxiv.org/abs/2402.00759v1"}
{"created":"2024-02-01 16:43:58","title":"GS++: Error Analyzing and Optimal Gaussian Splatting","abstract":"3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering. Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance , and robustness in sparse viewpoints , leading to various improvements. However, there has been a notable lack of attention to the projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering. This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function $\\phi$. The analysis establishes a correlation between the error and the Gaussian mean position. Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting. Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering.","sentences":["3D Gaussian Splatting has garnered extensive attention and application in real-time neural rendering.","Concurrently, concerns have been raised about the limitations of this technology in aspects such as point cloud storage, performance , and robustness in sparse viewpoints , leading to various improvements.","However, there has been a notable lack of attention to the projection errors introduced by the local affine approximation inherent in the splatting itself, and the consequential impact of these errors on the quality of photo-realistic rendering.","This paper addresses the projection error function of 3D Gaussian Splatting, commencing with the residual error from the first-order Taylor expansion of the projection function $\\phi$. The analysis establishes a correlation between the error and the Gaussian mean position.","Subsequently, leveraging function optimization theory, this paper analyzes the function's minima to provide an optimal projection strategy for Gaussian Splatting referred to Optimal Gaussian Splatting.","Experimental validation further confirms that this projection methodology reduces artifacts, resulting in a more convincingly realistic rendering."],"url":"http://arxiv.org/abs/2402.00752v1"}
{"created":"2024-02-01 16:43:04","title":"Unlearnable Algorithms for In-context Learning","abstract":"Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests.","sentences":["Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance.","However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining.","In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM).","We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data.","We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets.","We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches.","This leads us to propose a new holistic measure of unlearning cost which accounts for varying inference costs, and conclude that in-context learning can often be more favourable than fine-tuning for deployments involving unlearning requests."],"url":"http://arxiv.org/abs/2402.00751v1"}
{"created":"2024-02-01 16:40:32","title":"Health-LLM: Personalized Retrieval-Augmented Disease Prediction Model","abstract":"Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment. However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges. Hence, a more professional and detailed intelligent healthcare method is needed for development. To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring. Compared to traditional health management methods, our approach has three main advantages. First, our method integrates health reports into a large model to provide detailed task information. Second, professional medical expertise is used to adjust the weighted scores of health characteristics. Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease prediction. We have conducted disease prediction experiments on a large number of health reports to assess the effectiveness of Health-LLM. The results of the experiments indicate that the proposed method surpasses traditional methods and has the potential to revolutionize disease prediction and personalized health management. The code is available at https://github.com/jmyissb/HealthLLM.","sentences":["Artificial intelligence (AI) in healthcare has significantly advanced intelligent medical treatment.","However, traditional intelligent healthcare is limited by static data and unified standards, preventing full integration with individual situations and other challenges.","Hence, a more professional and detailed intelligent healthcare method is needed for development.","To this end, we propose an innovative framework named Heath-LLM, which combines large-scale feature extraction and medical knowledge trade-off scoring.","Compared to traditional health management methods, our approach has three main advantages.","First, our method integrates health reports into a large model to provide detailed task information.","Second, professional medical expertise is used to adjust the weighted scores of health characteristics.","Third, we use a semi-automated feature extraction framework to enhance the analytical power of language models and incorporate expert insights to improve the accuracy of disease prediction.","We have conducted disease prediction experiments on a large number of health reports to assess the effectiveness of Health-LLM.","The results of the experiments indicate that the proposed method surpasses traditional methods and has the potential to revolutionize disease prediction and personalized health management.","The code is available at https://github.com/jmyissb/HealthLLM."],"url":"http://arxiv.org/abs/2402.00746v1"}
{"created":"2024-02-01 16:39:51","title":"Enhancing Ethical Explanations of Large Language Models through Iterative Symbolic Refinement","abstract":"An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities. Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains. In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs. Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy. An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning. As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs.","sentences":["An increasing amount of research in Natural Language Inference (NLI) focuses on the application and evaluation of Large Language Models (LLMs) and their reasoning capabilities.","Despite their success, however, LLMs are still prone to factual errors and inconsistencies in their explanations, offering limited control and interpretability for inference in complex domains.","In this paper, we focus on ethical NLI, investigating how hybrid neuro-symbolic techniques can enhance the logical validity and alignment of ethical explanations produced by LLMs.","Specifically, we present an abductive-deductive framework named Logic-Explainer, which integrates LLMs with an external backward-chaining solver to refine step-wise natural language explanations and jointly verify their correctness, reduce incompleteness and minimise redundancy.","An extensive empirical analysis demonstrates that Logic-Explainer can improve explanations generated via in-context learning methods and Chain-of-Thought (CoT) on challenging ethical NLI tasks, while, at the same time, producing formal proofs describing and supporting models' reasoning.","As ethical NLI requires commonsense reasoning to identify underlying moral violations, our results suggest the effectiveness of neuro-symbolic methods for multi-step NLI more broadly, opening new opportunities to enhance the logical consistency, reliability, and alignment of LLMs."],"url":"http://arxiv.org/abs/2402.00745v1"}
{"created":"2024-02-01 16:39:47","title":"BATON: Aligning Text-to-Audio Model with Human Preference Feedback","abstract":"With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention. However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability. To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback. Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback. Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs. Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model. The experiment results demonstrate that our BATON can significantly improve the generation quality of the original text-to-audio models, concerning audio integrity, temporal relationship, and alignment with human preference.","sentences":["With the development of AI-Generated Content (AIGC), text-to-audio models are gaining widespread attention.","However, it is challenging for these models to generate audio aligned with human preference due to the inherent information density of natural language and limited model understanding ability.","To alleviate this issue, we formulate the BATON, a framework designed to enhance the alignment between generated audio and text prompt using human preference feedback.","Our BATON comprises three key stages: Firstly, we curated a dataset containing both prompts and the corresponding generated audio, which was then annotated based on human feedback.","Secondly, we introduced a reward model using the constructed dataset, which can mimic human preference by assigning rewards to input text-audio pairs.","Finally, we employed the reward model to fine-tune an off-the-shelf text-to-audio model.","The experiment results demonstrate that our BATON can significantly improve the generation quality of the original text-to-audio models, concerning audio integrity, temporal relationship, and alignment with human preference."],"url":"http://arxiv.org/abs/2402.00744v1"}
{"created":"2024-02-01 16:39:45","title":"Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data","abstract":"In practice, it is observed that transformer-based models can learn concepts in context in the inference stage. While existing literature, e.g., \\citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data). However, in reality, they are presented in two tokens (i.e., unstructured data \\cite{wibisono2023role}). In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data. We study the exact components in a transformer that facilitate the in-context learning. In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention.","sentences":["In practice, it is observed that transformer-based models can learn concepts in context in the inference stage.","While existing literature, e.g., \\citet{zhang2023trained,huang2023context}, provide theoretical explanations on this in-context learning ability, they assume the input $x_i$ and the output $y_i$ for each sample are embedded in the same token (i.e., structured data).","However, in reality, they are presented in two tokens (i.e., unstructured data \\cite{wibisono2023role}).","In this case, this paper conducts experiments in linear regression tasks to study the benefits of the architecture of transformers and provides some corresponding theoretical intuitions to explain why the transformer can learn from unstructured data.","We study the exact components in a transformer that facilitate the in-context learning.","In particular, we observe that (1) a transformer with two layers of softmax (self-)attentions with look-ahead attention mask can learn from the prompt if $y_i$ is in the token next to $x_i$ for each example; (2) positional encoding can further improve the performance; and (3) multi-head attention with a high input embedding dimension has a better prediction performance than single-head attention."],"url":"http://arxiv.org/abs/2402.00743v1"}
{"created":"2024-02-01 16:39:28","title":"Transforming and Combining Rewards for Aligning Large Language Models","abstract":"A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model. We study two closely related problems that arise in this approach. First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others? Second, we often wish to align language models to multiple properties: how should we combine multiple reward models? Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models. This derived transformation has two important properties. First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well. This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model). Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise. Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach.","sentences":["A common approach for aligning language models to human preferences is to first learn a reward model from preference data, and then use this reward model to update the language model.","We study two closely related problems that arise in this approach.","First, any monotone transformation of the reward model preserves preference ranking; is there a choice that is ``better'' than others?","Second, we often wish to align language models to multiple properties: how should we combine multiple reward models?","Using a probabilistic interpretation of the alignment procedure, we identify a natural choice for transformation for (the common case of) rewards learned from Bradley-Terry preference models.","This derived transformation has two important properties.","First, it emphasizes improving poorly-performing outputs, rather than outputs that already score well.","This mitigates both underfitting (where some prompts are not improved) and reward hacking (where the model learns to exploit misspecification of the reward model).","Second, it enables principled aggregation of rewards by linking summation to logical conjunction: the sum of transformed rewards corresponds to the probability that the output is ``good'' in all measured properties, in a sense we make precise.","Experiments aligning language models to be both helpful and harmless using RLHF show substantial improvements over the baseline (non-transformed) approach."],"url":"http://arxiv.org/abs/2402.00742v1"}
{"created":"2024-02-01 16:38:51","title":"DRSM: efficient neural 4d decomposition for dynamic reconstruction in stationary monocular cameras","abstract":"With the popularity of monocular videos generated by video sharing and live broadcasting applications, reconstructing and editing dynamic scenes in stationary monocular cameras has become a special but anticipated technology. In contrast to scene reconstructions that exploit multi-view observations, the problem of modeling a dynamic scene from a single view is significantly more under-constrained and ill-posed. Inspired by recent progress in neural rendering, we present a novel framework to tackle 4D decomposition problem for dynamic scenes in monocular cameras. Our framework utilizes decomposed static and dynamic feature planes to represent 4D scenes and emphasizes the learning of dynamic regions through dense ray casting. Inadequate 3D clues from a single-view and occlusion are also particular challenges in scene reconstruction. To overcome these difficulties, we propose deep supervised optimization and ray casting strategies. With experiments on various videos, our method generates higher-fidelity results than existing methods for single-view dynamic scene representation.","sentences":["With the popularity of monocular videos generated by video sharing and live broadcasting applications, reconstructing and editing dynamic scenes in stationary monocular cameras has become a special but anticipated technology.","In contrast to scene reconstructions that exploit multi-view observations, the problem of modeling a dynamic scene from a single view is significantly more under-constrained and ill-posed.","Inspired by recent progress in neural rendering, we present a novel framework to tackle 4D decomposition problem for dynamic scenes in monocular cameras.","Our framework utilizes decomposed static and dynamic feature planes to represent 4D scenes and emphasizes the learning of dynamic regions through dense ray casting.","Inadequate 3D clues from a single-view and occlusion are also particular challenges in scene reconstruction.","To overcome these difficulties, we propose deep supervised optimization and ray casting strategies.","With experiments on various videos, our method generates higher-fidelity results than existing methods for single-view dynamic scene representation."],"url":"http://arxiv.org/abs/2402.00740v1"}
{"created":"2024-02-01 16:37:21","title":"FM3Q: Factorized Multi-Agent MiniMax Q-Learning for Two-Team Zero-Sum Markov Game","abstract":"Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team. The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years. However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability. In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs. Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs. Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players. A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs.","sentences":["Many real-world applications involve some agents that fall into two teams, with payoffs that are equal within the same team but of opposite sign across the opponent team.","The so-called two-team zero-sum Markov games (2t0sMGs) can be resolved with reinforcement learning in recent years.","However, existing methods are thus inefficient in light of insufficient consideration of intra-team credit assignment, data utilization and computational intractability.","In this paper, we propose the individual-global-minimax (IGMM) principle to ensure the coherence between two-team minimax behaviors and the individual greedy behaviors through Q functions in 2t0sMGs.","Based on it, we present a novel multi-agent reinforcement learning framework, Factorized Multi-Agent MiniMax Q-Learning (FM3Q), which can factorize the joint minimax Q function into individual ones and iteratively solve for the IGMM-satisfied minimax Q functions for 2t0sMGs.","Moreover, an online learning algorithm with neural networks is proposed to implement FM3Q and obtain the deterministic and decentralized minimax policies for two-team players.","A theoretical analysis is provided to prove the convergence of FM3Q. Empirically, we use three environments to evaluate the learning efficiency and final performance of FM3Q and show its superiority on 2t0sMGs."],"url":"http://arxiv.org/abs/2402.00738v1"}
{"created":"2024-02-01 16:33:06","title":"BIOMERO: BioImage analysis in OMERO","abstract":"In the rapidly evolving field of bioimaging, the integration and orchestration of Findable, Accessible, Interoperable, and Reusable (FAIR) image analysis workflows remains a challenge. We introduce BIOMERO, a bridge connecting OMERO, a renowned bioimaging data management platform, FAIR workflows and high-performance computing (HPC) environments. BIOMERO, featuring our opensource Python library \"OMERO Slurm Client\", facilitates seamless execution of FAIR workflows, particularly for large datasets from High Content or High Throughput Screening. BIOMERO empowers researchers by eliminating the need for specialized knowledge, enabling scalable image processing directly from OMERO. BIOMERO notably supports the sharing and utilization of FAIR workflows between OMERO, Cytomine/BIAFLOWS, and other bioimaging communities. BIOMERO will promote the widespread adoption of FAIR workflows, emphasizing reusability, across the realm of bioimaging research. Its user-friendly interface will empower users, including those without technical expertise, to seamlessly apply these workflows to their datasets, democratizing the utilization of AI by the broader research community.","sentences":["In the rapidly evolving field of bioimaging, the integration and orchestration of Findable, Accessible, Interoperable, and Reusable (FAIR) image analysis workflows remains a challenge.","We introduce BIOMERO, a bridge connecting OMERO, a renowned bioimaging data management platform, FAIR workflows and high-performance computing (HPC) environments.","BIOMERO, featuring our opensource Python library \"OMERO Slurm Client\", facilitates seamless execution of FAIR workflows, particularly for large datasets from High Content or High Throughput Screening.","BIOMERO empowers researchers by eliminating the need for specialized knowledge, enabling scalable image processing directly from OMERO.","BIOMERO notably supports the sharing and utilization of FAIR workflows between OMERO, Cytomine/BIAFLOWS, and other bioimaging communities.","BIOMERO will promote the widespread adoption of FAIR workflows, emphasizing reusability, across the realm of bioimaging research.","Its user-friendly interface will empower users, including those without technical expertise, to seamlessly apply these workflows to their datasets, democratizing the utilization of AI by the broader research community."],"url":"http://arxiv.org/abs/2402.00734v1"}
{"created":"2024-02-01 16:30:00","title":"MobilityDL: A Review of Deep Learning From Trajectory Data","abstract":"Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior. As data availability and computing power have increased, so has the popularity of deep learning from trajectory data. This review paper provides the first comprehensive overview of deep learning approaches for trajectory data. We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used. Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information).","sentences":["Trajectory data combines the complexities of time series, spatial data, and (sometimes irrational) movement behavior.","As data availability and computing power have increased, so has the popularity of deep learning from trajectory data.","This review paper provides the first comprehensive overview of deep learning approaches for trajectory data.","We have identified eight specific mobility use cases which we analyze with regards to the deep learning models and the training data used.","Besides a comprehensive quantitative review of the literature since 2018, the main contribution of our work is the data-centric analysis of recent work in this field, placing it along the mobility data continuum which ranges from detailed dense trajectories of individual movers (quasi-continuous tracking data), to sparse trajectories (such as check-in data), and aggregated trajectories (crowd information)."],"url":"http://arxiv.org/abs/2402.00732v1"}
{"created":"2024-02-01 16:26:30","title":"Profiling and Modeling of Power Characteristics of Leadership-Scale HPC System Workloads","abstract":"In the exascale era in which application behavior has large power & energy footprints, per-application job-level awareness of such impression is crucial in taking steps towards achieving efficiency goals beyond performance, such as energy efficiency, and sustainability.   To achieve these goals, we have developed a novel low-latency job power profiling machine learning pipeline that can group job-level power profiles based on their shapes as they complete. This pipeline leverages a comprehensive feature extraction and clustering pipeline powered by a generative adversarial network (GAN) model to handle the feature-rich time series of job-level power measurements. The output is then used to train a classification model that can predict whether an incoming job power profile is similar to a known group of profiles or is completely new. With extensive evaluations, we demonstrate the effectiveness of each component in our pipeline. Also, we provide a preliminary analysis of the resulting clusters that depict the power profile landscape of the Summit supercomputer from more than 60K jobs sampled from the year 2021.","sentences":["In the exascale era in which application behavior has large power & energy footprints, per-application job-level awareness of such impression is crucial in taking steps towards achieving efficiency goals beyond performance, such as energy efficiency, and sustainability.   ","To achieve these goals, we have developed a novel low-latency job power profiling machine learning pipeline that can group job-level power profiles based on their shapes as they complete.","This pipeline leverages a comprehensive feature extraction and clustering pipeline powered by a generative adversarial network (GAN) model to handle the feature-rich time series of job-level power measurements.","The output is then used to train a classification model that can predict whether an incoming job power profile is similar to a known group of profiles or is completely new.","With extensive evaluations, we demonstrate the effectiveness of each component in our pipeline.","Also, we provide a preliminary analysis of the resulting clusters that depict the power profile landscape of the Summit supercomputer from more than 60K jobs sampled from the year 2021."],"url":"http://arxiv.org/abs/2402.00729v1"}
{"created":"2024-02-01 16:25:00","title":"Dropout-Based Rashomon Set Exploration for Efficient Predictive Multiplicity Estimation","abstract":"Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples. This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications. Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces. To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set. We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation. Numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to $20\\times \\sim 5000\\times$. With efficient Rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection.","sentences":["Predictive multiplicity refers to the phenomenon in which classification tasks may admit multiple competing models that achieve almost-equally-optimal performance, yet generate conflicting outputs for individual samples.","This presents significant concerns, as it can potentially result in systemic exclusion, inexplicable discrimination, and unfairness in practical applications.","Measuring and mitigating predictive multiplicity, however, is computationally challenging due to the need to explore all such almost-equally-optimal models, known as the Rashomon set, in potentially huge hypothesis spaces.","To address this challenge, we propose a novel framework that utilizes dropout techniques for exploring models in the Rashomon set.","We provide rigorous theoretical derivations to connect the dropout parameters to properties of the Rashomon set, and empirically evaluate our framework through extensive experimentation.","Numerical results show that our technique consistently outperforms baselines in terms of the effectiveness of predictive multiplicity metric estimation, with runtime speedup up to $20\\times \\sim 5000\\times$. With efficient Rashomon set exploration and metric estimation, mitigation of predictive multiplicity is then achieved through dropout ensemble and model selection."],"url":"http://arxiv.org/abs/2402.00728v1"}
{"created":"2024-02-01 16:14:54","title":"Automatic Segmentation of the Spinal Cord Nerve Rootlets","abstract":"Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord. The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans. Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets. Each output class corresponds to a spinal level. The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability. The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance. The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 1.30 %) indicating stable predictions across different MRI vendors, sites, and sessions. The proposed methodology is open-source and readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher.","sentences":["Precise identification of spinal nerve rootlets is relevant to delineate spinal levels for the study of functional activity in the spinal cord.","The goal of this study was to develop an automatic method for the semantic segmentation of spinal nerve rootlets from T2-weighted magnetic resonance imaging (MRI) scans.","Images from two open-access MRI datasets were used to train a 3D multi-class convolutional neural network using an active learning approach to segment C2-C8 dorsal nerve rootlets.","Each output class corresponds to a spinal level.","The method was tested on 3T T2-weighted images from datasets unseen during training to assess inter-site, inter-session, and inter-resolution variability.","The test Dice score was 0.67 +- 0.16 (mean +- standard deviation across rootlets levels), suggesting a good performance.","The method also demonstrated low inter-vendor and inter-site variability (coefficient of variation <= 1.41 %), as well as low inter-session variability (coefficient of variation <= 1.30 %) indicating stable predictions across different MRI vendors, sites, and sessions.","The proposed methodology is open-source and readily available in the Spinal Cord Toolbox (SCT) v6.2 and higher."],"url":"http://arxiv.org/abs/2402.00724v1"}
{"created":"2024-02-01 16:14:35","title":"Improving Semantic Control in Discrete Latent Spaces with Transformer Quantized Variational Autoencoders","abstract":"Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon. Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism. To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs. In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities. Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and preservation of semantic information across different tasks such as auto-encoding of sentences and mathematical expressions, text transfer, and inference. Moreover, T5VQVAE exhibits improved inference capabilities, suggesting potential applications for downstream natural language and symbolic reasoning tasks.","sentences":["Achieving precise semantic control over the latent spaces of Variational AutoEncoders (VAEs) holds significant value for downstream tasks in NLP as the underlying generative mechanisms could be better localised, explained and improved upon.","Recent research, however, has struggled to achieve consistent results, primarily due to the inevitable loss of semantic information in the variational bottleneck and limited control over the decoding mechanism.","To overcome these challenges, we investigate discrete latent spaces in Vector Quantized Variational AutoEncoders (VQVAEs) to improve semantic control and generation in Transformer-based VAEs.","In particular, We propose T5VQVAE, a novel model that leverages the controllability of VQVAEs to guide the self-attention mechanism in T5 at the token-level, exploiting its full generalization capabilities.","Experimental results indicate that T5VQVAE outperforms existing state-of-the-art VAE models, including Optimus, in terms of controllability and preservation of semantic information across different tasks such as auto-encoding of sentences and mathematical expressions, text transfer, and inference.","Moreover, T5VQVAE exhibits improved inference capabilities, suggesting potential applications for downstream natural language and symbolic reasoning tasks."],"url":"http://arxiv.org/abs/2402.00723v1"}
{"created":"2024-02-01 16:14:32","title":"Neural Style Transfer with Twin-Delayed DDPG for Shared Control of Robotic Manipulators","abstract":"Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one. Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element. In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way. An autoencoder architecture extracts and defines the Content and the Style of the target robot motions. A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder. The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style. Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments, or online, for adapting at runtime the style of a teleoperated robot. The considered styles can be learned online from human demonstrations. We carried out an evaluation with human subjects enrolling 73 volunteers, asking them to recognize the style behind some representative robotic motions. Results show a good recognition rate, proving that it is possible to convey different styles to a robot using this approach.","sentences":["Neural Style Transfer (NST) refers to a class of algorithms able to manipulate an element, most often images, to adopt the appearance or style of another one.","Each element is defined as a combination of Content and Style: the Content can be conceptually defined as the what and the Style as the how of said element.","In this context, we propose a custom NST framework for transferring a set of styles to the motion of a robotic manipulator, e.g., the same robotic task can be carried out in an angry, happy, calm, or sad way.","An autoencoder architecture extracts and defines the Content and the Style of the target robot motions.","A Twin Delayed Deep Deterministic Policy Gradient (TD3) network generates the robot control policy using the loss defined by the autoencoder.","The proposed Neural Policy Style Transfer TD3 (NPST3) alters the robot motion by introducing the trained style.","Such an approach can be implemented either offline, for carrying out autonomous robot motions in dynamic environments, or online, for adapting at runtime the style of a teleoperated robot.","The considered styles can be learned online from human demonstrations.","We carried out an evaluation with human subjects enrolling 73 volunteers, asking them to recognize the style behind some representative robotic motions.","Results show a good recognition rate, proving that it is possible to convey different styles to a robot using this approach."],"url":"http://arxiv.org/abs/2402.00722v1"}
{"created":"2024-02-01 16:12:15","title":"Orientation-aware Incremental Potential Contact","abstract":"The Incremental Potential Contact (IPC) method enables robust complex simulations of deformable objects with contact and friction. The key to IPC's robustness is its strict adherence to geometric constraints, avoiding intersections, which are a common cause of robustness issues in contact mechanics. A key element of the IPC approach to contact is a geometric barrier function, which is defined directly in the discrete setting. While IPC achieves its main goal of providing guarantees for contact constraints, its parameters need to be chosen carefully to avoid significant simulation artifacts and inaccuracies. We present a systematic derivation of an IPC-like continuum potential defined for smooth and piecewise smooth surfaces, starting from identifying a set of natural requirements for contact potentials, including the barrier property, locality, differentiable dependence of shape, and absence of forces in rest configurations, based on the idea of candidate sets. Our potential is formulated in a way independent of surface discretization.   This new potential is suitable for piecewise-linear surfaces and its efficiency is similar to standard IPC. We demonstrate its behavior and compare it to IPC on a range of challenging contact examples.","sentences":["The Incremental Potential Contact (IPC) method enables robust complex simulations of deformable objects with contact and friction.","The key to IPC's robustness is its strict adherence to geometric constraints, avoiding intersections, which are a common cause of robustness issues in contact mechanics.","A key element of the IPC approach to contact is a geometric barrier function, which is defined directly in the discrete setting.","While IPC achieves its main goal of providing guarantees for contact constraints, its parameters need to be chosen carefully to avoid significant simulation artifacts and inaccuracies.","We present a systematic derivation of an IPC-like continuum potential defined for smooth and piecewise smooth surfaces, starting from identifying a set of natural requirements for contact potentials, including the barrier property, locality, differentiable dependence of shape, and absence of forces in rest configurations, based on the idea of candidate sets.","Our potential is formulated in a way independent of surface discretization.   ","This new potential is suitable for piecewise-linear surfaces and its efficiency is similar to standard IPC.","We demonstrate its behavior and compare it to IPC on a range of challenging contact examples."],"url":"http://arxiv.org/abs/2402.00719v1"}
{"created":"2024-02-01 16:09:19","title":"Intent Assurance using LLMs guided by Intent Drift","abstract":"Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner. However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents. To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states. In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs. To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents.","sentences":["Intent-Based Networking (IBN) presents a paradigm shift for network management, by promising to align intents and business objectives with network operations--in an automated manner.","However, its practical realization is challenging: 1) processing intents, i.e., translate, decompose and identify the logic to fulfill the intent, and 2) intent conformance, that is, considering dynamic networks, the logic should be adequately adapted to assure intents.","To address the latter, intent assurance is tasked with continuous verification and validation, including taking the necessary actions to align the operational and target states.","In this paper, we define an assurance framework that allows us to detect and act when intent drift occurs.","To do so, we leverage AI-driven policies, generated by Large Language Models (LLMs) which can quickly learn the necessary in-context requirements, and assist with the fulfillment and assurance of intents."],"url":"http://arxiv.org/abs/2402.00715v1"}
{"created":"2024-02-01 16:07:12","title":"ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction","abstract":"Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change. Yet, S2S prediction remains challenging due to the chaotic nature of the system. At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability. Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction. ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years. We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model. Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart. We establish two tasks that vary in complexity: full and sparse dynamics prediction. Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task. We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench.","sentences":["Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster readiness, reduced economic risk, and improved policy-making amidst climate change.","Yet, S2S prediction remains challenging due to the chaotic nature of the system.","At present, existing benchmarks for weather and climate applications, tend to (1) have shorter forecasting range of up-to 14 days, (2) do not include a wide range of operational baseline forecasts, and (3) lack physics-based constraints for explainability.","Thus, we propose ChaosBench, a large-scale, multi-channel, physics-based benchmark for S2S prediction.","ChaosBench has over 460K frames of real-world observations and simulations, each with 60 variable-channels and spanning for up-to 45 years.","We also propose several physics-based, in addition to vision-based metrics, that enables for a more physically-consistent model.","Furthermore, we include a diverse set of physics-based forecasts from 4 national weather agencies as baselines to our data-driven counterpart.","We establish two tasks that vary in complexity: full and sparse dynamics prediction.","Our benchmark is one of the first to perform large-scale evaluation on existing models including PanguWeather, FourCastNetV2, GraphCast, and ClimaX, and finds methods originally developed for weather-scale applications fails on S2S task.","We release our benchmark code and datasets at https://leap-stc.github.io/ChaosBench."],"url":"http://arxiv.org/abs/2402.00712v1"}
{"created":"2024-02-01 16:06:35","title":"Explaining Text Classifiers with Counterfactual Representations","abstract":"One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature. Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events. In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation. We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework. To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the representation space. Second, we study a real world scenario where our counterfactuals can be leveraged both for explaining a classifier and for bias mitigation.","sentences":["One well motivated explanation method for classifiers leverages counterfactuals which are hypothetical events identical to real observations in all aspects except for one categorical feature.","Constructing such counterfactual poses specific challenges for texts, however, as some attribute values may not necessarily align with plausible real-world events.","In this paper we propose a simple method for generating counterfactuals by intervening in the space of text representations which bypasses this limitation.","We argue that our interventions are minimally disruptive and that they are theoretically sound as they align with counterfactuals as defined in Pearl's causal inference framework.","To validate our method, we first conduct experiments on a synthetic dataset of counterfactuals, allowing for a direct comparison between classifier predictions based on ground truth counterfactuals (obtained through explicit text interventions) and our counterfactuals, derived through interventions in the representation space.","Second, we study a real world scenario where our counterfactuals can be leveraged both for explaining a classifier and for bias mitigation."],"url":"http://arxiv.org/abs/2402.00711v1"}
{"created":"2024-02-01 16:05:15","title":"Towards an autonomous industry 4.0 warehouse: A UAV and blockchain-based system for inventory and traceability applications in big data-driven supply chain management","abstract":"In this paper we present the design and evaluation of a UAV-based system aimed at automating inventory tasks and keeping the traceability of industrial items attached to Radio-Frequency IDentification (RFID) tags. To confront current shortcomings, such a system is developed under a versatile, modular and scalable architecture aimed to reinforce cyber security and decentralization while fostering external audits and big data analytics. Therefore, the system uses a blockchain and a distributed ledger to store certain inventory data collected by UAVs, validate them, ensure their trustworthiness and make them available to the interested parties. In order to show the performance of the proposed system, different tests were performed in a real industrial warehouse, concluding that the system is able to obtain the inventory data really fast in comparison to traditional manual tasks, while being also able to estimate the position of the items when hovering over them thanks to their tag's signal strength. In addition, the performance of the proposed blockchain-based architecture was evaluated in different scenarios.","sentences":["In this paper we present the design and evaluation of a UAV-based system aimed at automating inventory tasks and keeping the traceability of industrial items attached to Radio-Frequency IDentification (RFID) tags.","To confront current shortcomings, such a system is developed under a versatile, modular and scalable architecture aimed to reinforce cyber security and decentralization while fostering external audits and big data analytics.","Therefore, the system uses a blockchain and a distributed ledger to store certain inventory data collected by UAVs, validate them, ensure their trustworthiness and make them available to the interested parties.","In order to show the performance of the proposed system, different tests were performed in a real industrial warehouse, concluding that the system is able to obtain the inventory data really fast in comparison to traditional manual tasks, while being also able to estimate the position of the items when hovering over them thanks to their tag's signal strength.","In addition, the performance of the proposed blockchain-based architecture was evaluated in different scenarios."],"url":"http://arxiv.org/abs/2402.00709v1"}
{"created":"2024-02-01 16:04:36","title":"Benchmarking human-robot collaborative assembly tasks","abstract":"Manufacturing assembly tasks can vary in complexity and level of automation. Yet, achieving full automation can be challenging and inefficient, particularly due to the complexity of certain assembly operations. Human-robot collaborative work, leveraging the strengths of human labor alongside the capabilities of robots, can be a solution for enhancing efficiency. This paper introduces the CT benchmark, a benchmark and model set designed to facilitate the testing and evaluation of human-robot collaborative assembly scenarios. It was designed to compare manual and automatic processes using metrics such as the assembly time and human workload. The components of the model set can be assembled through the most common assembly tasks, each with varying levels of difficulty. The CT benchmark was designed with a focus on its applicability in human-robot collaborative environments, with the aim of ensuring the reproducibility and replicability of experiments. Experiments were carried out to assess assembly performance in three different setups (manual, automatic and collaborative), measuring metrics related to the assembly time and the workload on human operators. The results suggest that the collaborative approach takes longer than the fully manual assembly, with an increase of 70.8%. However, users reported a lower overall workload, as well as reduced mental demand, physical demand, and effort according to the NASA-TLX questionnaire.","sentences":["Manufacturing assembly tasks can vary in complexity and level of automation.","Yet, achieving full automation can be challenging and inefficient, particularly due to the complexity of certain assembly operations.","Human-robot collaborative work, leveraging the strengths of human labor alongside the capabilities of robots, can be a solution for enhancing efficiency.","This paper introduces the CT benchmark, a benchmark and model set designed to facilitate the testing and evaluation of human-robot collaborative assembly scenarios.","It was designed to compare manual and automatic processes using metrics such as the assembly time and human workload.","The components of the model set can be assembled through the most common assembly tasks, each with varying levels of difficulty.","The CT benchmark was designed with a focus on its applicability in human-robot collaborative environments, with the aim of ensuring the reproducibility and replicability of experiments.","Experiments were carried out to assess assembly performance in three different setups (manual, automatic and collaborative), measuring metrics related to the assembly time and the workload on human operators.","The results suggest that the collaborative approach takes longer than the fully manual assembly, with an increase of 70.8%.","However, users reported a lower overall workload, as well as reduced mental demand, physical demand, and effort according to the NASA-TLX questionnaire."],"url":"http://arxiv.org/abs/2402.00708v1"}
{"created":"2024-02-01 16:04:04","title":"Non-Exchangeable Conformal Language Generation with Nearest Neighbors","abstract":"Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees.","sentences":["Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable.","Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic.","In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage.","The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors.","Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees.","Experiments in machine translation and language modeling show encouraging results in generation quality.","By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees."],"url":"http://arxiv.org/abs/2402.00707v1"}
{"created":"2024-02-01 16:00:21","title":"Combining the Strengths of Dutch Survey and Register Data in a Data Challenge to Predict Fertility (PreFer)","abstract":"The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children. However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data. This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge. In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands. One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values. The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents. We provide information about the datasets and the samples, and describe the fertility outcome of interest. We also introduce the fertility prediction data challenge PreFer which is based on these datasets and will start in Spring 2024. We outline the ways in which measuring the predictability of fertility outcomes using these datasets and combining their strengths in the data challenge can advance our understanding of fertility behaviour and computational social science. We further provide details for participants on how to take part in the data challenge.","sentences":["The social sciences have produced an impressive body of research on determinants of fertility outcomes, or whether and when people have children.","However, the strength of these determinants and underlying theories are rarely evaluated on their predictive ability on new data.","This prevents us from systematically comparing studies, hindering the evaluation and accumulation of knowledge.","In this paper, we present two datasets which can be used to study the predictability of fertility outcomes in the Netherlands.","One dataset is based on the LISS panel, a longitudinal survey which includes thousands of variables on a wide range of topics, including individual preferences and values.","The other is based on the Dutch register data which lacks attitudinal data but includes detailed information about the life courses of millions of Dutch residents.","We provide information about the datasets and the samples, and describe the fertility outcome of interest.","We also introduce the fertility prediction data challenge PreFer which is based on these datasets and will start in Spring 2024.","We outline the ways in which measuring the predictability of fertility outcomes using these datasets and combining their strengths in the data challenge can advance our understanding of fertility behaviour and computational social science.","We further provide details for participants on how to take part in the data challenge."],"url":"http://arxiv.org/abs/2402.00705v1"}
{"created":"2024-02-01 15:59:16","title":"Vehicle Perception from Satellite","abstract":"Satellites are capable of capturing high-resolution videos. It makes vehicle perception from satellite become possible. Compared to street surveillance, drive recorder or other equipments, satellite videos provide a much broader city-scale view, so that the global dynamic scene of the traffic are captured and displayed. Traffic monitoring from satellite is a new task with great potential applications, including traffic jams prediction, path planning, vehicle dispatching, \\emph{etc.}. Practically, limited by the resolution and view, the captured vehicles are very tiny (a few pixels) and move slowly. Worse still, these satellites are in Low Earth Orbit (LEO) to capture such high-resolution videos, so the background is also moving. Under this circumstance, traffic monitoring from the satellite view is an extremely challenging task. To attract more researchers into this field, we build a large-scale benchmark for traffic monitoring from satellite. It supports several tasks, including tiny object detection, counting and density estimation. The dataset is constructed based on 12 satellite videos and 14 synthetic videos recorded from GTA-V. They are separated into 408 video clips, which contain 7,336 real satellite images and 1,960 synthetic images. 128,801 vehicles are annotated totally, and the number of vehicles in each image varies from 0 to 101. Several classic and state-of-the-art approaches in traditional computer vision are evaluated on the datasets, so as to compare the performance of different approaches, analyze the challenges in this task, and discuss the future prospects. The dataset is available at: https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos.","sentences":["Satellites are capable of capturing high-resolution videos.","It makes vehicle perception from satellite become possible.","Compared to street surveillance, drive recorder or other equipments, satellite videos provide a much broader city-scale view, so that the global dynamic scene of the traffic are captured and displayed.","Traffic monitoring from satellite is a new task with great potential applications, including traffic jams prediction, path planning, vehicle dispatching, \\emph{etc.}.","Practically, limited by the resolution and view, the captured vehicles are very tiny (a few pixels) and move slowly.","Worse still, these satellites are in Low Earth Orbit (LEO) to capture such high-resolution videos, so the background is also moving.","Under this circumstance, traffic monitoring from the satellite view is an extremely challenging task.","To attract more researchers into this field, we build a large-scale benchmark for traffic monitoring from satellite.","It supports several tasks, including tiny object detection, counting and density estimation.","The dataset is constructed based on 12 satellite videos and 14 synthetic videos recorded from GTA-V.","They are separated into 408 video clips, which contain 7,336 real satellite images and 1,960 synthetic images.","128,801 vehicles are annotated totally, and the number of vehicles in each image varies from 0 to 101.","Several classic and state-of-the-art approaches in traditional computer vision are evaluated on the datasets, so as to compare the performance of different approaches, analyze the challenges in this task, and discuss the future prospects.","The dataset is available at: https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos."],"url":"http://arxiv.org/abs/2402.00703v1"}
{"created":"2024-02-01 15:57:11","title":"In-Bed Pose Estimation: A Review","abstract":"Human pose estimation, the process of identifying joint positions in a person's body from images or videos, represents a widely utilized technology across diverse fields, including healthcare. One such healthcare application involves in-bed pose estimation, where the body pose of an individual lying under a blanket is analyzed. This task, for instance, can be used to monitor a person's sleep behavior and detect symptoms early for potential disease diagnosis in homes and hospitals. Several studies have utilized unimodal and multimodal methods to estimate in-bed human poses. The unimodal studies generally employ RGB images, whereas the multimodal studies use modalities including RGB, long-wavelength infrared, pressure map, and depth map. Multimodal studies have the advantage of using modalities in addition to RGB that might capture information useful to cope with occlusions. Moreover, some multimodal studies exclude RGB and, this way, better suit privacy preservation. To expedite advancements in this domain, we conduct a review of existing datasets and approaches. Our objectives are to show the limitations of the previous studies, current challenges, and provide insights for future works on the in-bed human pose estimation field.","sentences":["Human pose estimation, the process of identifying joint positions in a person's body from images or videos, represents a widely utilized technology across diverse fields, including healthcare.","One such healthcare application involves in-bed pose estimation, where the body pose of an individual lying under a blanket is analyzed.","This task, for instance, can be used to monitor a person's sleep behavior and detect symptoms early for potential disease diagnosis in homes and hospitals.","Several studies have utilized unimodal and multimodal methods to estimate in-bed human poses.","The unimodal studies generally employ RGB images, whereas the multimodal studies use modalities including RGB, long-wavelength infrared, pressure map, and depth map.","Multimodal studies have the advantage of using modalities in addition to RGB that might capture information useful to cope with occlusions.","Moreover, some multimodal studies exclude RGB and, this way, better suit privacy preservation.","To expedite advancements in this domain, we conduct a review of existing datasets and approaches.","Our objectives are to show the limitations of the previous studies, current challenges, and provide insights for future works on the in-bed human pose estimation field."],"url":"http://arxiv.org/abs/2402.00700v1"}
{"created":"2024-02-01 15:55:50","title":"PeaTMOSS: A Dataset and Initial Analysis of Pre-Trained Models in Open-Source Software","abstract":"The development and training of deep learning models have become increasingly costly and complex. Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications. The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models. Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse. This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models. Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use. To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics. Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation. Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects. PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain. We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions.","sentences":["The development and training of deep learning models have become increasingly costly and complex.","Consequently, software engineers are adopting pre-trained models (PTMs) for their downstream applications.","The dynamics of the PTM supply chain remain largely unexplored, signaling a clear need for structured datasets that document not only the metadata but also the subsequent applications of these models.","Without such data, the MSR community cannot comprehensively understand the impact of PTM adoption and reuse.","This paper presents the PeaTMOSS dataset, which comprises metadata for 281,638 PTMs and detailed snapshots for all PTMs with over 50 monthly downloads (14,296 PTMs), along with 28,575 open-source software repositories from GitHub that utilize these models.","Additionally, the dataset includes 44,337 mappings from 15,129 downstream GitHub repositories to the 2,530 PTMs they use.","To enhance the dataset's comprehensiveness, we developed prompts for a large language model to automatically extract model metadata, including the model's training datasets, parameters, and evaluation metrics.","Our analysis of this dataset provides the first summary statistics for the PTM supply chain, showing the trend of PTM development and common shortcomings of PTM package documentation.","Our example application reveals inconsistencies in software licenses across PTMs and their dependent projects.","PeaTMOSS lays the foundation for future research, offering rich opportunities to investigate the PTM supply chain.","We outline mining opportunities on PTMs, their downstream usage, and cross-cutting questions."],"url":"http://arxiv.org/abs/2402.00699v1"}
{"created":"2024-02-01 15:55:25","title":"Time-Series Analysis Approach for Improving Energy Efficiency of a Fixed-Route Vessel in Short-Sea Shipping","abstract":"Several approaches have been developed for improving the ship energy efficiency, thereby reducing operating costs and ensuring compliance with climate change mitigation regulations. Many of these approaches will heavily depend on measured data from onboard IoT devices, including operational and environmental information, as well as external data sources for additional navigational data. In this paper, we develop a framework that implements time-series analysis techniques to optimize the vessel's speed profile for improving the vessel's energy efficiency. We present a case study involving a real-world data from a passenger vessel that was collected over a span of 15 months in the south of Sweden. The results indicate that the implemented models exhibit a range of outcomes and adaptability across different scenarios. The findings highlight the effectiveness of time-series analysis approach for optimizing vessel voyages within the context of constrained landscapes, as often seen in short-sea shipping.","sentences":["Several approaches have been developed for improving the ship energy efficiency, thereby reducing operating costs and ensuring compliance with climate change mitigation regulations.","Many of these approaches will heavily depend on measured data from onboard IoT devices, including operational and environmental information, as well as external data sources for additional navigational data.","In this paper, we develop a framework that implements time-series analysis techniques to optimize the vessel's speed profile for improving the vessel's energy efficiency.","We present a case study involving a real-world data from a passenger vessel that was collected over a span of 15 months in the south of Sweden.","The results indicate that the implemented models exhibit a range of outcomes and adaptability across different scenarios.","The findings highlight the effectiveness of time-series analysis approach for optimizing vessel voyages within the context of constrained landscapes, as often seen in short-sea shipping."],"url":"http://arxiv.org/abs/2402.00698v1"}
{"created":"2024-02-01 15:51:46","title":"Approximating Optimal Morphing Attacks using Template Inversion","abstract":"Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings. We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images. We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism. We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks. We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally much faster to run. We hope this might facilitate the development of large scale deep morph datasets for training detection models.","sentences":["Recent works have demonstrated the feasibility of inverting face recognition systems, enabling to recover convincing face images using only their embeddings.","We leverage such template inversion models to develop a novel type ofdeep morphing attack based on inverting a theoretical optimal morph embedding, which is obtained as an average of the face embeddings of source images.","We experiment with two variants of this approach: the first one exploits a fully self-contained embedding-to-image inversion model, while the second leverages the synthesis network of a pretrained StyleGAN network for increased morph realism.","We generate morphing attacks from several source datasets and study the effectiveness of those attacks against several face recognition networks.","We showcase that our method can compete with and regularly beat the previous state of the art for deep-learning based morph generation in terms of effectiveness, both in white-box and black-box attack scenarios, and is additionally much faster to run.","We hope this might facilitate the development of large scale deep morph datasets for training detection models."],"url":"http://arxiv.org/abs/2402.00695v1"}
{"created":"2024-02-01 15:50:40","title":"A Framework for Building Point Cloud Cleaning, Plane Detection and Semantic Segmentation","abstract":"This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling. We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure. Following the cleaning process, we perform plane detection using the robust RANSAC paradigm. The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls. The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements. Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects. Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings. The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization.","sentences":["This paper presents a framework to address the challenges involved in building point cloud cleaning, plane detection, and semantic segmentation, with the ultimate goal of enhancing building modeling.","We focus in the cleaning stage on removing outliers from the acquired point cloud data by employing an adaptive threshold technique based on z-score measure.","Following the cleaning process, we perform plane detection using the robust RANSAC paradigm.","The goal is to carry out multiple plane segmentations, and to classify segments into distinct categories, such as floors, ceilings, and walls.","The resulting segments can generate accurate and detailed point clouds representing the building's architectural elements.","Moreover, we address the problem of semantic segmentation, which plays a vital role in the identification and classification of different components within the building, such as walls, windows, doors, roofs, and objects.","Inspired by the PointNet architecture, we propose a deep learning architecture for efficient semantic segmentation in buildings.","The results demonstrate the effectiveness of the proposed framework in handling building modeling tasks, paving the way for improved accuracy and efficiency in the field of building modelization."],"url":"http://arxiv.org/abs/2402.00692v1"}
{"created":"2024-02-01 15:50:37","title":"Comparative Study of Large Language Model Architectures on Frontier","abstract":"Large language models (LLMs) have garnered significant attention in both the AI community and beyond. Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants. However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies. Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world's first Exascale supercomputer. Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance. Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark. Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design. To our knowledge, these pre-trained models represent the largest available for materials science. Our findings provide practical guidance for building LLMs on HPC platforms.","sentences":["Large language models (LLMs) have garnered significant attention in both the AI community and beyond.","Among these, the Generative Pre-trained Transformer (GPT) has emerged as the dominant architecture, spawning numerous variants.","However, these variants have undergone pre-training under diverse conditions, including variations in input data, data preprocessing, and training methodologies, resulting in a lack of controlled comparative studies.","Here we meticulously examine two prominent open-sourced GPT architectures, GPT-NeoX and LLaMA, leveraging the computational power of Frontier, the world's first Exascale supercomputer.","Employing the same materials science text corpus and a comprehensive end-to-end pipeline, we conduct a comparative analysis of their training and downstream performance.","Our efforts culminate in achieving state-of-the-art performance on a challenging materials science benchmark.","Furthermore, we investigate the computation and energy efficiency, and propose a computationally efficient method for architecture design.","To our knowledge, these pre-trained models represent the largest available for materials science.","Our findings provide practical guidance for building LLMs on HPC platforms."],"url":"http://arxiv.org/abs/2402.00691v1"}
{"created":"2024-02-01 15:49:47","title":"Ocassionally Secure: A Comparative Analysis of Code Generation Assistants","abstract":"$ $Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example. While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code. Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code. We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities. We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work. Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our developer persona. In total, we collected 61 code outputs and analyzed them across several aspects: functionality, security, performance, complexity, and reliability. These insights are crucial for understanding the models' capabilities and limitations, guiding future development and practical applications in the field of automated code generation.","sentences":["$ $Large Language Models (LLMs) are being increasingly utilized in various applications, with code generations being a notable example.","While previous research has shown that LLMs have the capability to generate both secure and insecure code, the literature does not take into account what factors help generate secure and effective code.","Therefore in this paper we focus on identifying and understanding the conditions and contexts in which LLMs can be effectively and safely deployed in real-world scenarios to generate quality code.","We conducted a comparative analysis of four advanced LLMs--GPT-3.5 and GPT-4 using ChatGPT and Bard and Gemini from Google--using 9 separate tasks to assess each model's code generation capabilities.","We contextualized our study to represent the typical use cases of a real-life developer employing LLMs for everyday tasks as work.","Additionally, we place an emphasis on security awareness which is represented through the use of two distinct versions of our developer persona.","In total, we collected 61 code outputs and analyzed them across several aspects: functionality, security, performance, complexity, and reliability.","These insights are crucial for understanding the models' capabilities and limitations, guiding future development and practical applications in the field of automated code generation."],"url":"http://arxiv.org/abs/2402.00689v1"}
{"created":"2024-02-01 15:48:45","title":"A Review on the Use of Blockchain for the Internet of Things","abstract":"The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks. Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance. Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin. In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented. After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications. Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application. Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications.","sentences":["The paradigm of Internet of Things (IoT) is paving the way for a world, where many of our daily objects will be interconnected and will interact with their environment in order to collect information and automate certain tasks.","Such a vision requires, among other things, seamless authentication, data privacy, security, robustness against attacks, easy deployment, and self-maintenance.","Such features can be brought by blockchain, a technology born with a cryptocurrency called Bitcoin.","In this paper, a thorough review on how to adapt blockchain to the specific needs of IoT in order to develop Blockchain-based IoT (BIoT) applications is presented.","After describing the basics of blockchain, the most relevant BIoT applications are described with the objective of emphasizing how blockchain can impact traditional cloud-centered IoT applications.","Then, the current challenges and possible optimizations are detailed regarding many aspects that affect the design, development, and deployment of a BIoT application.","Finally, some recommendations are enumerated with the aim of guiding future BIoT researchers and developers on some of the issues that will have to be tackled before deploying the next generation of BIoT applications."],"url":"http://arxiv.org/abs/2402.00687v1"}
{"created":"2024-02-01 15:47:01","title":"An Investigation of Hardware Security Bug Characteristics in Open-Source Projects","abstract":"Hardware security is an important concern of system security as vulnerabilities can arise from design errors introduced throughout the development lifecycle. Recent works have proposed techniques to detect hardware security bugs, such as static analysis, fuzzing, and symbolic execution. However, the fundamental properties of hardware security bugs remain relatively unexplored. To gain a better understanding of hardware security bugs, we perform a deep dive into the popular OpenTitan project, including its bug reports and bug fixes. We manually classify the bugs as relevant to functionality or security and analyze characteristics, such as the impact and location of security bugs, and the size of their bug fixes. We also investigate relationships between security impact and bug management during development. Finally, we propose an abstract syntax tree-based analysis to identify the syntactic characteristics of bug fixes. Our results show that 53% of the bugs in OpenTitan have potential security implications and that 55% of all bug fixes modify only one file. Our findings underscore the importance of security-aware development practices and tools and motivate the development of techniques that leverage the highly localized nature of hardware bugs.","sentences":["Hardware security is an important concern of system security as vulnerabilities can arise from design errors introduced throughout the development lifecycle.","Recent works have proposed techniques to detect hardware security bugs, such as static analysis, fuzzing, and symbolic execution.","However, the fundamental properties of hardware security bugs remain relatively unexplored.","To gain a better understanding of hardware security bugs, we perform a deep dive into the popular OpenTitan project, including its bug reports and bug fixes.","We manually classify the bugs as relevant to functionality or security and analyze characteristics, such as the impact and location of security bugs, and the size of their bug fixes.","We also investigate relationships between security impact and bug management during development.","Finally, we propose an abstract syntax tree-based analysis to identify the syntactic characteristics of bug fixes.","Our results show that 53% of the bugs in OpenTitan have potential security implications and that 55% of all bug fixes modify only one file.","Our findings underscore the importance of security-aware development practices and tools and motivate the development of techniques that leverage the highly localized nature of hardware bugs."],"url":"http://arxiv.org/abs/2402.00684v1"}
{"created":"2024-02-01 15:46:04","title":"WayFASTER: a Self-Supervised Traversability Prediction for Increased Navigation Awareness","abstract":"Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors. Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible, but were visible at a different time. To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations. Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics. Our experiments demonstrate that our method excels at avoiding geometric obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable. By using a sequence of images, WayFASTER significantly enhances the robot's awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible. This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential.","sentences":["Accurate and robust navigation in unstructured environments requires fusing data from multiple sensors.","Such fusion ensures that the robot is better aware of its surroundings, including areas of the environment that are not immediately visible, but were visible at a different time.","To solve this problem, we propose a method for traversability prediction in challenging outdoor environments using a sequence of RGB and depth images fused with pose estimations.","Our method, termed WayFASTER (Waypoints-Free Autonomous System for Traversability with Enhanced Robustness), uses experience data recorded from a receding horizon estimator to train a self-supervised neural network for traversability prediction, eliminating the need for heuristics.","Our experiments demonstrate that our method excels at avoiding geometric obstacles, and correctly detects that traversable terrains, such as tall grass, can be navigable.","By using a sequence of images, WayFASTER significantly enhances the robot's awareness of its surroundings, enabling it to predict the traversability of terrains that are not immediately visible.","This enhanced awareness contributes to better navigation performance in environments where such predictive capabilities are essential."],"url":"http://arxiv.org/abs/2402.00683v1"}
{"created":"2024-02-01 15:38:21","title":"Real Evaluations Tractability using Continuous Goal-Directed Actions in Smart City Applications","abstract":"One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users. Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations. In classical frameworks, actions are modeled using joint or Cartesian space trajectories. Other features, such as visual ones, are not always well represented with these pure geometrical approaches. Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment. As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding. This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot. Current strategies involve performing evaluations in a simulation, transferring the final joint trajectory to the actual robot. Smart City applications involve working in highly dynamic and complex environments, where having a precise model is not always achievable. Our goal is to study the tractability of performing these evaluations directly in a real-world scenario. Two different approaches to reduce the number of evaluations using EA, are proposed and compared. In the first approach, Particle Swarm Optimization (PSO)-based methods have been studied and compared within CGDA: naive PSO, Fitness Inheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO (AFFG-PSO). The second approach studied the introduction of geometrical and velocity constraints within CGDA. The effects of both approaches were analyzed and compared in the wax and paint actions, two CGDA commonly studied use cases. Results from this paper depict an important reduction in the number of evaluations.","sentences":["One of the most important challenges of Smart City Applications is to adapt the system to interact with non-expert users.","Robot imitation frameworks aim to simplify and reduce times of robot programming by allowing users to program directly through demonstrations.","In classical frameworks, actions are modeled using joint or Cartesian space trajectories.","Other features, such as visual ones, are not always well represented with these pure geometrical approaches.","Continuous Goal-Directed Actions (CGDA) is an alternative to these methods, as it encodes actions as changes of any feature that can be extracted from the environment.","As a consequence of this, the robot joint trajectories for execution must be fully computed to comply with this feature-agnostic encoding.","This is achieved using Evolutionary Algorithms (EA), which usually requires too many evaluations to perform this evolution step in the actual robot.","Current strategies involve performing evaluations in a simulation, transferring the final joint trajectory to the actual robot.","Smart City applications involve working in highly dynamic and complex environments, where having a precise model is not always achievable.","Our goal is to study the tractability of performing these evaluations directly in a real-world scenario.","Two different approaches to reduce the number of evaluations using EA, are proposed and compared.","In the first approach, Particle Swarm Optimization (PSO)-based methods have been studied and compared within CGDA: naive PSO, Fitness Inheritance PSO (FI-PSO), and Adaptive Fuzzy Fitness Granulation with PSO (AFFG-PSO).","The second approach studied the introduction of geometrical and velocity constraints within CGDA.","The effects of both approaches were analyzed and compared in the wax and paint actions, two CGDA commonly studied use cases.","Results from this paper depict an important reduction in the number of evaluations."],"url":"http://arxiv.org/abs/2402.00678v1"}
{"created":"2024-02-01 15:37:42","title":"Neural Policy Style Transfer","abstract":"Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories. We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure. Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content. The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style. The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter. Different policies are defined via Deep Q-Network architectures. These models are trained using demonstrations through Inverse Reinforcement Learning. Two different sets of user demonstrations are performed, one for content and other for style. Different styles are encoded as defined by user demonstrations. The generated policy is the result of feeding a content policy and a style policy to the NPST algorithm. Experiments are performed in a catch-ball game inspired by the Deep Reinforcement Learning classical Atari games; and a real-world painting scenario with a full-sized humanoid robot, based on previous works of the authors. The implementation of three different Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode the policies within the NPST framework is proposed and the results obtained in the experiments with each of these architectures compared.","sentences":["Style Transfer has been proposed in a number of fields: fine arts, natural language processing, and fixed trajectories.","We scale this concept up to control policies within a Deep Reinforcement Learning infrastructure.","Each network is trained to maximize the expected reward, which typically encodes the goal of an action, and can be described as the content.","The expressive power of deep neural networks enables encoding a secondary task, which can be described as the style.","The Neural Policy Style Transfer (NPST) algorithm is proposed to transfer the style of one policy to another, while maintaining the content of the latter.","Different policies are defined via Deep Q-Network architectures.","These models are trained using demonstrations through Inverse Reinforcement Learning.","Two different sets of user demonstrations are performed, one for content and other for style.","Different styles are encoded as defined by user demonstrations.","The generated policy is the result of feeding a content policy and a style policy to the NPST algorithm.","Experiments are performed in a catch-ball game inspired by the Deep Reinforcement Learning classical Atari games; and a real-world painting scenario with a full-sized humanoid robot, based on previous works of the authors.","The implementation of three different Q-Network architectures (Shallow, Deep and Deep Recurrent Q-Network) to encode the policies within the NPST framework is proposed and the results obtained in the experiments with each of these architectures compared."],"url":"http://arxiv.org/abs/2402.00677v1"}
{"created":"2024-02-01 15:37:23","title":"Deep Robot Sketching: An application of Deep Q-Learning Networks for human-like sketching","abstract":"The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science. Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments. In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications. Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics. DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments. Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments. In this work, the introduction of DQN within an art painting robot application is proposed. The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application. The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks. Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot. Results are compared in terms of similarity and obtained reward with respect to the reference inputs","sentences":["The current success of Reinforcement Learning algorithms for its performance in complex environments has inspired many recent theoretical approaches to cognitive science.","Artistic environments are studied within the cognitive science community as rich, natural, multi-sensory, multi-cultural environments.","In this work, we propose the introduction of Reinforcement Learning for improving the control of artistic robot applications.","Deep Q-learning Neural Networks (DQN) is one of the most successful algorithms for the implementation of Reinforcement Learning in robotics.","DQN methods generate complex control policies for the execution of complex robot applications in a wide set of environments.","Current art painting robot applications use simple control laws that limits the adaptability of the frameworks to a set of simple environments.","In this work, the introduction of DQN within an art painting robot application is proposed.","The goal is to study how the introduction of a complex control policy impacts the performance of a basic art painting robot application.","The main expected contribution of this work is to serve as a first baseline for future works introducing DQN methods for complex art painting robot frameworks.","Experiments consist of real world executions of human drawn sketches using the DQN generated policy and TEO, the humanoid robot.","Results are compared in terms of similarity and obtained reward with respect to the reference inputs"],"url":"http://arxiv.org/abs/2402.00676v1"}
{"created":"2024-02-01 15:34:59","title":"On Modular Algorithms and Butterfly Operations in Number Theoretic Transform","abstract":"Number theoretic transform (NTT) has been a very useful tool in computations for number theory, algebra and cryptography. Its performance affects some post-quantum cryptosystems. In this paper, we discuss the butterfly operation of NTT. This basic module of NTT requires heavy modular arithmetics. Montgomery reduction is commonly used in this setting. Recently several variants of Montgomery algorithm have been proposed for the purpose of speeding up NTT. We observe that the Chinese remainder theorem (CRT) can be involved in this type of algorithms in nature and transparent ways. In this paper, a framework of using CRT to model Montgomery type algorithms is described. The derivation of these algorithms as well as their correctness are all treated in the CRT framework. Under our approach, some problems of a modular reduction algorithm (published in IACR Transactions on Cryptographic Hardware and Embedded Systems, doi:10.46586/tches.v2022.i4.614-636 ) are identified, and a counterexample is generated to show that the algorithm is incorrect.","sentences":["Number theoretic transform (NTT) has been a very useful tool in computations for number theory, algebra and cryptography.","Its performance affects some post-quantum cryptosystems.","In this paper, we discuss the butterfly operation of NTT.","This basic module of NTT requires heavy modular arithmetics.","Montgomery reduction is commonly used in this setting.","Recently several variants of Montgomery algorithm have been proposed for the purpose of speeding up NTT.","We observe that the Chinese remainder theorem (CRT) can be involved in this type of algorithms in nature and transparent ways.","In this paper, a framework of using CRT to model Montgomery type algorithms is described.","The derivation of these algorithms as well as their correctness are all treated in the CRT framework.","Under our approach, some problems of a modular reduction algorithm (published in IACR Transactions on Cryptographic Hardware and Embedded Systems, doi:10.46586/tches.v2022.i4.614-636 ) are identified, and a counterexample is generated to show that the algorithm is incorrect."],"url":"http://arxiv.org/abs/2402.00675v1"}
{"created":"2024-02-01 15:33:17","title":"Exploring Homogeneous and Heterogeneous Consistent Label Associations for Unsupervised Visible-Infrared Person ReID","abstract":"Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations. While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations. In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations. It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures. Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the impact of noisy pseudo-labels while simultaneously aligning different modalities, coupled with a Modality-Invariant Representation Learning (MIRL) framework. Experiments demonstrate that our proposed method outperforms existing USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods. The code will be available.","sentences":["Unsupervised visible-infrared person re-identification (USL-VI-ReID) aims to retrieve pedestrian images of the same identity from different modalities without annotations.","While prior work focuses on establishing cross-modality pseudo-label associations to bridge the modality-gap, they ignore maintaining the instance-level homogeneous and heterogeneous consistency in pseudo-label space, resulting in coarse associations.","In response, we introduce a Modality-Unified Label Transfer (MULT) module that simultaneously accounts for both homogeneous and heterogeneous fine-grained instance-level structures, yielding high-quality cross-modality label associations.","It models both homogeneous and heterogeneous affinities, leveraging them to define the inconsistency for the pseudo-labels and then minimize it, leading to pseudo-labels that maintain alignment across modalities and consistency within intra-modality structures.","Additionally, a straightforward plug-and-play Online Cross-memory Label Refinement (OCLR) module is proposed to further mitigate the impact of noisy pseudo-labels while simultaneously aligning different modalities, coupled with a Modality-Invariant Representation Learning (MIRL) framework.","Experiments demonstrate that our proposed method outperforms existing USL-VI-ReID methods, highlighting the superiority of our MULT in comparison to other cross-modality association methods.","The code will be available."],"url":"http://arxiv.org/abs/2402.00672v1"}
{"created":"2024-02-01 15:30:19","title":"Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning","abstract":"This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks. The W2SG framework has opened new possibilities for empirical research in this evolving field. Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence. In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students. In the second phase, an automatic alignment evaluator is employed as the weak supervisor. By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.We also provide an initial validation of the proposed approach for the first phase. Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting. Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate. Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning. Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG.","sentences":["This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG).","Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks.","The W2SG framework has opened new possibilities for empirical research in this evolving field.","Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence.","In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students.","In the second phase, an automatic alignment evaluator is employed as the weak supervisor.","By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.","We also provide an initial validation of the proposed approach for the first phase.","Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting.","Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate.","Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning.","Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG."],"url":"http://arxiv.org/abs/2402.00667v1"}
