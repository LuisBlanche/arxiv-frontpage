{"created":"2024-01-11 18:59:53","title":"Distilling Vision-Language Models on Millions of Videos","abstract":"The recent advance in vision-language models is largely attributed to the abundance of image-text data. We aim to replicate this success for video-language models, but there simply is not enough human-curated video-text data available. We thus resort to fine-tuning a video-language model from a strong image-language baseline with synthesized instructional data. The resulting video-language model is then used to auto-label millions of videos to generate high-quality captions. We show the adapted video-language model performs well on a wide range of video-language benchmarks. For instance, it surpasses the best prior result on open-ended NExT-QA by 2.8%. Besides, our model generates detailed descriptions for previously unseen videos, which provide better textual supervision than existing methods. Experiments show that a video-language dual-encoder model contrastively trained on these auto-generated captions is 3.8% better than the strongest baseline that also leverages vision-language models. Our best model outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video retrieval by 6%.","sentences":["The recent advance in vision-language models is largely attributed to the abundance of image-text data.","We aim to replicate this success for video-language models, but there simply is not enough human-curated video-text data available.","We thus resort to fine-tuning a video-language model from a strong image-language baseline with synthesized instructional data.","The resulting video-language model is then used to auto-label millions of videos to generate high-quality captions.","We show the adapted video-language model performs well on a wide range of video-language benchmarks.","For instance, it surpasses the best prior result on open-ended NExT-QA by 2.8%.","Besides, our model generates detailed descriptions for previously unseen videos, which provide better textual supervision than existing methods.","Experiments show that a video-language dual-encoder model contrastively trained on these auto-generated captions is 3.8% better than the strongest baseline that also leverages vision-language models.","Our best model outperforms state-of-the-art methods on MSR-VTT zero-shot text-to-video retrieval by 6%."],"url":"http://arxiv.org/abs/2401.06129v1"}
{"created":"2024-01-11 18:59:14","title":"E$^{2}$GAN: Efficient Training of Efficient GANs for Image-to-Image Translation","abstract":"One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs). This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models. However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts. In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient? To achieve this goal, we propose a series of innovative techniques. First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating the need for training from scratch. Second, we identify crucial layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet effective rank search process, rather than fine-tuning the entire base model. Third, we investigate the minimal amount of data necessary for fine-tuning, further reducing the overall training time. Extensive experiments show that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkable reduced training cost and storage for each concept.","sentences":["One highly promising direction for enabling flexible real-time on-device image editing is utilizing data distillation by leveraging large-scale text-to-image diffusion models, such as Stable Diffusion, to generate paired datasets used for training generative adversarial networks (GANs).","This approach notably alleviates the stringent requirements typically imposed by high-end commercial GPUs for performing image editing with diffusion models.","However, unlike text-to-image diffusion models, each distilled GAN is specialized for a specific image editing task, necessitating costly training efforts to obtain models for various concepts.","In this work, we introduce and address a novel research direction: can the process of distilling GANs from diffusion models be made significantly more efficient?","To achieve this goal, we propose a series of innovative techniques.","First, we construct a base GAN model with generalized features, adaptable to different concepts through fine-tuning, eliminating the need for training from scratch.","Second, we identify crucial layers within the base GAN model and employ Low-Rank Adaptation (LoRA) with a simple yet effective rank search process, rather than fine-tuning the entire base model.","Third, we investigate the minimal amount of data necessary for fine-tuning, further reducing the overall training time.","Extensive experiments show that we can efficiently empower GANs with the ability to perform real-time high-quality image editing on mobile devices with remarkable reduced training cost and storage for each concept."],"url":"http://arxiv.org/abs/2401.06127v1"}
{"created":"2024-01-11 18:59:12","title":"Dubbing for Everyone: Data-Efficient Visual Dubbing using Neural Rendering Priors","abstract":"Visual dubbing is the process of generating lip motions of an actor in a video to synchronise with given audio. Recent advances have made progress towards this goal but have not been able to produce an approach suitable for mass adoption. Existing methods are split into either person-generic or person-specific models. Person-specific models produce results almost indistinguishable from reality but rely on long training times using large single-person datasets. Person-generic works have allowed for the visual dubbing of any video to any audio without further training, but these fail to capture the person-specific nuances and often suffer from visual artefacts. Our method, based on data-efficient neural rendering priors, overcomes the limitations of existing approaches. Our pipeline consists of learning a deferred neural rendering prior network and actor-specific adaptation using neural textures. This method allows for $\\textbf{high-quality visual dubbing with just a few seconds of data}$, that enables video dubbing for any actor - from A-list celebrities to background actors. We show that we achieve state-of-the-art in terms of $\\textbf{visual quality}$ and $\\textbf{recognisability}$ both quantitatively, and qualitatively through two user studies. Our prior learning and adaptation method $\\textbf{generalises to limited data}$ better and is more $\\textbf{scalable}$ than existing person-specific models. Our experiments on real-world, limited data scenarios find that our model is preferred over all others. The project page may be found at https://dubbingforeveryone.github.io/","sentences":["Visual dubbing is the process of generating lip motions of an actor in a video to synchronise with given audio.","Recent advances have made progress towards this goal but have not been able to produce an approach suitable for mass adoption.","Existing methods are split into either person-generic or person-specific models.","Person-specific models produce results almost indistinguishable from reality but rely on long training times using large single-person datasets.","Person-generic works have allowed for the visual dubbing of any video to any audio without further training, but these fail to capture the person-specific nuances and often suffer from visual artefacts.","Our method, based on data-efficient neural rendering priors, overcomes the limitations of existing approaches.","Our pipeline consists of learning a deferred neural rendering prior network and actor-specific adaptation using neural textures.","This method allows for $\\textbf{high-quality visual dubbing with just a few seconds of data}$, that enables video dubbing for any actor - from A-list celebrities to background actors.","We show that we achieve state-of-the-art in terms of $\\textbf{visual quality}$ and $\\textbf{recognisability}$ both quantitatively, and qualitatively through two user studies.","Our prior learning and adaptation method $\\textbf{generalises to limited data}$ better and is more $\\textbf{scalable}$ than existing person-specific models.","Our experiments on real-world, limited data scenarios find that our model is preferred over all others.","The project page may be found at https://dubbingforeveryone.github.io/"],"url":"http://arxiv.org/abs/2401.06126v1"}
{"created":"2024-01-11 18:58:57","title":"Improved Capacity Outer Bound for Private Quadratic Monomial Computation","abstract":"In private computation, a user wishes to retrieve a function evaluation of messages stored on a set of databases without revealing the function's identity to the databases. Obead \\emph{et al.} introduced a capacity outer bound for private nonlinear computation, dependent on the order of the candidate functions. Focusing on private \\emph{quadratic monomial} computation, we propose three methods for ordering candidate functions: a graph edge-coloring method, a graph-distance method, and an entropy-based greedy method. We confirm, via an exhaustive search, that all three methods yield an optimal ordering for $f < 6$ messages. For $6 \\leq f \\leq 12$ messages, we numerically evaluate the performance of the proposed methods compared with a directed random search. For almost all scenarios considered, the entropy-based greedy method gives the smallest gap to the best-found ordering.","sentences":["In private computation, a user wishes to retrieve a function evaluation of messages stored on a set of databases without revealing the function's identity to the databases.","Obead \\emph{et al.} introduced a capacity outer bound for private nonlinear computation, dependent on the order of the candidate functions.","Focusing on private \\emph{quadratic monomial} computation, we propose three methods for ordering candidate functions: a graph edge-coloring method, a graph-distance method, and an entropy-based greedy method.","We confirm, via an exhaustive search, that all three methods yield an optimal ordering for $f < 6$ messages.","For $6 \\leq f \\leq 12$ messages, we numerically evaluate the performance of the proposed methods compared with a directed random search.","For almost all scenarios considered, the entropy-based greedy method gives the smallest gap to the best-found ordering."],"url":"http://arxiv.org/abs/2401.06125v1"}
{"created":"2024-01-11 18:57:17","title":"Manipulating Feature Visualizations with Gradient Slingshots","abstract":"Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which substantiates our findings.","sentences":["Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown.","A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network.","In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process.","We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing.","As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which substantiates our findings."],"url":"http://arxiv.org/abs/2401.06122v1"}
{"created":"2024-01-11 18:57:12","title":"TOFU: A Task of Fictitious Unlearning for LLMs","abstract":"Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns. Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training. Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place. To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning. We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning. We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy. Finally, we provide a set of baseline results from existing unlearning algorithms. Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all.","sentences":["Large language models trained on massive corpora of data from the web can memorize and reproduce sensitive or private data raising both legal and ethical concerns.","Unlearning, or tuning models to forget information present in their training data, provides us with a way to protect private data after training.","Although several methods exist for such unlearning, it is unclear to what extent they result in models equivalent to those where the data to be forgotten was never learned in the first place.","To address this challenge, we present TOFU, a Task of Fictitious Unlearning, as a benchmark aimed at helping deepen our understanding of unlearning.","We offer a dataset of 200 diverse synthetic author profiles, each consisting of 20 question-answer pairs, and a subset of these profiles called the forget set that serves as the target for unlearning.","We compile a suite of metrics that work together to provide a holistic picture of unlearning efficacy.","Finally, we provide a set of baseline results from existing unlearning algorithms.","Importantly, none of the baselines we consider show effective unlearning motivating continued efforts to develop approaches for unlearning that effectively tune models so that they truly behave as if they were never trained on the forget data at all."],"url":"http://arxiv.org/abs/2401.06121v1"}
{"created":"2024-01-11 18:54:44","title":"Extreme Compression of Large Language Models via Additive Quantization","abstract":"The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices. In this paper, we revisit the problem of \"extreme\" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models. The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget. For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 perplexity (a .22 improvement) on WikiText2. We release our implementation of Additive Quantization for Language Models AQLM as a baseline to facilitate future research in LLM quantization.","sentences":["The emergence of accurate open large language models (LLMs) has led to a race towards quantization techniques for such models enabling execution on end-user devices.","In this paper, we revisit the problem of \"extreme\" LLM compression--defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter, from the point of view of classic methods in Multi-Codebook Quantization (MCQ).","Our work builds on top of Additive Quantization, a classic algorithm from the MCQ family, and adapts it to the quantization of language models.","The resulting algorithm advances the state-of-the-art in LLM compression, outperforming all recently-proposed techniques in terms of accuracy at a given compression budget.","For instance, when compressing Llama 2 models to 2 bits per parameter, our algorithm quantizes the 7B model to 6.93 perplexity (a 1.29 improvement relative to the best prior work, and 1.81 points from FP16), the 13B model to 5.70 perplexity (a .36 improvement) and the 70B model to 3.94 perplexity (a .22 improvement) on WikiText2.","We release our implementation of Additive Quantization for Language Models AQLM as a baseline to facilitate future research in LLM quantization."],"url":"http://arxiv.org/abs/2401.06118v1"}
{"created":"2024-01-11 18:50:31","title":"Gaussian Shadow Casting for Neural Characters","abstract":"Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting. It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly. We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula. It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting. Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead. We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows. Our method is able to optimize the light direction without any input from the user. As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability.","sentences":["Neural character models can now reconstruct detailed geometry and texture from video, but they lack explicit shadows and shading, leading to artifacts when generating novel views and poses or during relighting.","It is particularly difficult to include shadows as they are a global effect and the required casting of secondary rays is costly.","We propose a new shadow model using a Gaussian density proxy that replaces sampling with a simple analytic formula.","It supports dynamic motion and is tailored for shadow computation, thereby avoiding the affine projection approximation and sorting required by the closely related Gaussian splatting.","Combined with a deferred neural rendering model, our Gaussian shadows enable Lambertian shading and shadow casting with minimal overhead.","We demonstrate improved reconstructions, with better separation of albedo, shading, and shadows in challenging outdoor scenes with direct sun light and hard shadows.","Our method is able to optimize the light direction without any input from the user.","As a result, novel poses have fewer shadow artifacts and relighting in novel scenes is more realistic compared to the state-of-the-art methods, providing new ways to pose neural characters in novel environments, increasing their applicability."],"url":"http://arxiv.org/abs/2401.06116v1"}
{"created":"2024-01-11 18:46:12","title":"Axis Tour: Word Tour Determines the Order of Axes in ICA-transformed Embeddings","abstract":"Word embedding is one of the most important components in natural language processing, but interpreting high-dimensional embeddings remains a challenging problem. To address this problem, Independent Component Analysis (ICA) is identified as an effective solution. ICA-transformed word embeddings reveal interpretable semantic axes; however, the order of these axes are arbitrary. In this study, we focus on this property and propose a novel method, Axis Tour, which optimizes the order of the axes. Inspired by Word Tour, a one-dimensional word embedding method, we aim to improve the clarity of the word embedding space by maximizing the semantic continuity of the axes. Furthermore, we show through experiments on downstream tasks that Axis Tour constructs better low-dimensional embeddings compared to both PCA and ICA.","sentences":["Word embedding is one of the most important components in natural language processing, but interpreting high-dimensional embeddings remains a challenging problem.","To address this problem, Independent Component Analysis (ICA) is identified as an effective solution.","ICA-transformed word embeddings reveal interpretable semantic axes; however, the order of these axes are arbitrary.","In this study, we focus on this property and propose a novel method, Axis Tour, which optimizes the order of the axes.","Inspired by Word Tour, a one-dimensional word embedding method, we aim to improve the clarity of the word embedding space by maximizing the semantic continuity of the axes.","Furthermore, we show through experiments on downstream tasks that Axis Tour constructs better low-dimensional embeddings compared to both PCA and ICA."],"url":"http://arxiv.org/abs/2401.06112v1"}
{"created":"2024-01-11 18:40:58","title":"Holey graphs: very large Betti numbers are testable","abstract":"We show that the graph property of having a (very) large $k$-th Betti number $\\beta_k$ for constant $k$ is testable with a constant number of queries in the dense graph model. More specifically, we consider a clique complex defined by an underlying graph and prove that for any $\\varepsilon>0$, there exists $\\delta(\\varepsilon,k)>0$ such that testing whether $\\beta_k \\geq (1-\\delta) d_k$ for $\\delta \\leq \\delta(\\varepsilon,k)$ reduces to tolerantly testing $(k+2)$-clique-freeness, which is known to be testable. This complements a result by Elek (2010) showing that Betti numbers are testable in the bounded-degree model. Our result combines the Euler characteristic, matroid theory and the graph removal lemma.","sentences":["We show that the graph property of having a (very) large $k$-th Betti number $\\beta_k$ for constant $k$ is testable with a constant number of queries in the dense graph model.","More specifically, we consider a clique complex defined by an underlying graph and prove that for any $\\varepsilon>0$, there exists $\\delta(\\varepsilon,k)>0$ such that testing whether $\\beta_k \\geq (1-\\delta) d_k$ for $\\delta \\leq \\delta(\\varepsilon,k)$ reduces to tolerantly testing $(k+2)$-clique-freeness, which is known to be testable.","This complements a result by Elek (2010) showing that Betti numbers are testable in the bounded-degree model.","Our result combines the Euler characteristic, matroid theory and the graph removal lemma."],"url":"http://arxiv.org/abs/2401.06109v1"}
{"created":"2024-01-11 18:35:33","title":"PALP: Prompt Aligned Personalization of Text-to-Image Models","abstract":"Content creators often aim to create personalized images using personal subjects that go beyond the capabilities of conventional text-to-image models. Additionally, they may want the resulting image to encompass a specific location, style, ambiance, and more. Existing personalization methods may compromise personalization ability or the alignment to complex textual prompts. This trade-off can impede the fulfillment of user prompts and subject fidelity. We propose a new approach focusing on personalization methods for a \\emph{single} prompt to address this issue. We term our approach prompt-aligned personalization. While this may seem restrictive, our method excels in improving text alignment, enabling the creation of images with complex and intricate prompts, which may pose a challenge for current techniques. In particular, our method keeps the personalized model aligned with a target prompt using an additional score distillation sampling term. We demonstrate the versatility of our method in multi- and single-shot settings and further show that it can compose multiple subjects or use inspiration from reference images, such as artworks. We compare our approach quantitatively and qualitatively with existing baselines and state-of-the-art techniques.","sentences":["Content creators often aim to create personalized images using personal subjects that go beyond the capabilities of conventional text-to-image models.","Additionally, they may want the resulting image to encompass a specific location, style, ambiance, and more.","Existing personalization methods may compromise personalization ability or the alignment to complex textual prompts.","This trade-off can impede the fulfillment of user prompts and subject fidelity.","We propose a new approach focusing on personalization methods for a \\emph{single} prompt to address this issue.","We term our approach prompt-aligned personalization.","While this may seem restrictive, our method excels in improving text alignment, enabling the creation of images with complex and intricate prompts, which may pose a challenge for current techniques.","In particular, our method keeps the personalized model aligned with a target prompt using an additional score distillation sampling term.","We demonstrate the versatility of our method in multi- and single-shot settings and further show that it can compose multiple subjects or use inspiration from reference images, such as artworks.","We compare our approach quantitatively and qualitatively with existing baselines and state-of-the-art techniques."],"url":"http://arxiv.org/abs/2401.06105v1"}
{"created":"2024-01-11 18:35:26","title":"Transformers are Multi-State RNNs","abstract":"Transformers are considered conceptually different compared to the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs). In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as infinite multi-state RNNs - an RNN variant with unlimited hidden state size. We further show that pretrained transformers can be converted into $\\textit{finite}$ multi-state RNNs by fixing the size of their hidden state. We observe that several existing transformers cache compression techniques can be framed as such conversion policies, and introduce a novel policy, TOVA, which is simpler compared to these policies. Our experiments with several long range tasks indicate that TOVA outperforms all other baseline policies, while being nearly on par with the full (infinite) model, and using in some cases only $\\frac{1}{8}$ of the original cache size. Our results indicate that transformer decoder LLMs often behave in practice as RNNs. They also lay out the option of mitigating one of their most painful computational bottlenecks - the size of their cache memory. We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA.","sentences":["Transformers are considered conceptually different compared to the previous generation of state-of-the-art NLP models - recurrent neural networks (RNNs).","In this work, we demonstrate that decoder-only transformers can in fact be conceptualized as infinite multi-state RNNs - an RNN variant with unlimited hidden state size.","We further show that pretrained transformers can be converted into $\\textit{finite}$ multi-state RNNs by fixing the size of their hidden state.","We observe that several existing transformers cache compression techniques can be framed as such conversion policies, and introduce a novel policy, TOVA, which is simpler compared to these policies.","Our experiments with several long range tasks indicate that TOVA outperforms all other baseline policies, while being nearly on par with the full (infinite) model, and using in some cases only $\\frac{1}{8}$ of the original cache size.","Our results indicate that transformer decoder LLMs often behave in practice as RNNs.","They also lay out the option of mitigating one of their most painful computational bottlenecks - the size of their cache memory.","We publicly release our code at https://github.com/schwartz-lab-NLP/TOVA."],"url":"http://arxiv.org/abs/2401.06104v1"}
{"created":"2024-01-11 18:33:48","title":"Patchscope: A Unifying Framework for Inspecting Hidden Representations of Language Models","abstract":"Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values. Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language. We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation. We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework. Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope. Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and unlocks new applications such as self-correction in multi-hop reasoning.","sentences":["Inspecting the information encoded in hidden representations of large language models (LLMs) can explain models' behavior and verify their alignment with human values.","Given the capabilities of LLMs in generating human-understandable text, we propose leveraging the model itself to explain its internal representations in natural language.","We introduce a framework called Patchscopes and show how it can be used to answer a wide range of research questions about an LLM's computation.","We show that prior interpretability methods based on projecting representations into the vocabulary space and intervening on the LLM computation, can be viewed as special instances of this framework.","Moreover, several of their shortcomings such as failure in inspecting early layers or lack of expressivity can be mitigated by a Patchscope.","Beyond unifying prior inspection techniques, Patchscopes also opens up new possibilities such as using a more capable model to explain the representations of a smaller model, and unlocks new applications such as self-correction in multi-hop reasoning."],"url":"http://arxiv.org/abs/2401.06102v1"}
{"created":"2024-01-11 18:11:42","title":"A Closer Look at AUROC and AUPRC under Class Imbalance","abstract":"In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance. This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms. We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. This bias can inadvertently heighten algorithmic disparities. Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv. Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority. The results expose a significant deficit in empirical backing and a trend of misattributions that have fuelled the widespread acceptance of AUPRC's supposed advantages. Our findings represent a dual contribution: a significant technical advancement in understanding metric behaviors and a stark warning about unchecked assumptions in the ML community. All experiments are accessible at https://github.com/mmcdermott/AUC_is_all_you_need.","sentences":["In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance.","This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms.","We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels.","This bias can inadvertently heighten algorithmic disparities.","Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv.","Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority.","The results expose a significant deficit in empirical backing and a trend of misattributions that have fuelled the widespread acceptance of AUPRC's supposed advantages.","Our findings represent a dual contribution: a significant technical advancement in understanding metric behaviors and a stark warning about unchecked assumptions in the ML community.","All experiments are accessible at https://github.com/mmcdermott/AUC_is_all_you_need."],"url":"http://arxiv.org/abs/2401.06091v1"}
{"created":"2024-01-11 18:08:56","title":"PANDORA: A Parallel Dendrogram Construction Algorithm for Single Linkage Clustering on GPU","abstract":"This paper presents \\pandora, a novel parallel algorithm for efficiently constructing dendrograms for single-linkage hierarchical clustering, including \\hdbscan. Traditional dendrogram construction methods from a minimum spanning tree (MST), such as agglomerative or divisive techniques, often fail to efficiently parallelize, especially with skewed dendrograms common in real-world data.   \\pandora addresses these challenges through a unique recursive tree contraction method, which simplifies the tree for initial dendrogram construction and then progressively reconstructs the complete dendrogram. This process makes \\pandora asymptotically work-optimal, independent of dendrogram skewness. All steps in \\pandora are fully parallel and suitable for massively threaded accelerators such as GPUs.   Our implementation is written in Kokkos, providing support for both CPUs and multi-vendor GPUs (e.g., Nvidia, AMD). The multithreaded version of \\pandora is 2.2$\\times$ faster than the current best-multithreaded implementation, while the GPU \\pandora implementation achieved 6-20$\\times$ on \\amdgpu and 10-37$\\times$ on \\nvidiagpu speed-up over multithreaded \\pandora. These advancements lead to up to a 6-fold speedup for \\hdbscan on GPUs over the current best, which only offload MST construction to GPUs and perform multithreaded dendrogram construction.","sentences":["This paper presents \\pandora, a novel parallel algorithm for efficiently constructing dendrograms for single-linkage hierarchical clustering, including \\hdbscan.","Traditional dendrogram construction methods from a minimum spanning tree (MST), such as agglomerative or divisive techniques, often fail to efficiently parallelize, especially with skewed dendrograms common in real-world data.   ","\\pandora addresses these challenges through a unique recursive tree contraction method, which simplifies the tree for initial dendrogram construction and then progressively reconstructs the complete dendrogram.","This process makes \\pandora asymptotically work-optimal, independent of dendrogram skewness.","All steps in \\pandora are fully parallel and suitable for massively threaded accelerators such as GPUs.   ","Our implementation is written in Kokkos, providing support for both CPUs and multi-vendor GPUs (e.g., Nvidia, AMD).","The multithreaded version of \\pandora is 2.2$\\times$ faster than the current best-multithreaded implementation, while the GPU \\pandora implementation achieved 6-20$\\times$ on \\amdgpu and 10-37$\\times$ on \\nvidiagpu speed-up over multithreaded \\pandora.","These advancements lead to up to a 6-fold speedup for \\hdbscan on GPUs over the current best, which only offload MST construction to GPUs and perform multithreaded dendrogram construction."],"url":"http://arxiv.org/abs/2401.06089v1"}
{"created":"2024-01-11 18:06:30","title":"Autocompletion of Chief Complaints in the Electronic Health Records using Large Language Models","abstract":"The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care. It provides critical information for healthcare providers to make informed decisions about patient care. However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments. To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses. In this study, we utilized text generation techniques to develop machine learning models using CC data. In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA. Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT-4. We evaluate the models' performance based on the perplexity score, modified BERTScore, and cosine similarity score. The results show that BioGPT-Large exhibits superior performance compared to the other models. It consistently achieves a remarkably low perplexity score of 1.65 when generating CC, whereas the baseline LSTM model achieves the best perplexity score of 170. Further, we evaluate and assess the proposed models' performance and the outcome of GPT-4.0. Our study demonstrates that utilizing LLMs such as BioGPT, leads to the development of an effective autocompletion tool for generating CC documentation in healthcare settings.","sentences":["The Chief Complaint (CC) is a crucial component of a patient's medical record as it describes the main reason or concern for seeking medical care.","It provides critical information for healthcare providers to make informed decisions about patient care.","However, documenting CCs can be time-consuming for healthcare providers, especially in busy emergency departments.","To address this issue, an autocompletion tool that suggests accurate and well-formatted phrases or sentences for clinical notes can be a valuable resource for triage nurses.","In this study, we utilized text generation techniques to develop machine learning models using CC data.","In our proposed work, we train a Long Short-Term Memory (LSTM) model and fine-tune three different variants of Biomedical Generative Pretrained Transformers (BioGPT), namely microsoft/biogpt, microsoft/BioGPT-Large, and microsoft/BioGPT-Large-PubMedQA.","Additionally, we tune a prompt by incorporating exemplar CC sentences, utilizing the OpenAI API of GPT-4.","We evaluate the models' performance based on the perplexity score, modified BERTScore, and cosine similarity score.","The results show that BioGPT-Large exhibits superior performance compared to the other models.","It consistently achieves a remarkably low perplexity score of 1.65 when generating CC, whereas the baseline LSTM model achieves the best perplexity score of 170.","Further, we evaluate and assess the proposed models' performance and the outcome of GPT-4.0.","Our study demonstrates that utilizing LLMs such as BioGPT, leads to the development of an effective autocompletion tool for generating CC documentation in healthcare settings."],"url":"http://arxiv.org/abs/2401.06088v1"}
{"created":"2024-01-11 18:03:17","title":"XGBoost Learning of Dynamic Wager Placement for In-Play Betting on an Agent-Based Model of a Sports Betting Exchange","abstract":"We present first results from the use of XGBoost, a highly effective machine learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source agent-based model (ABM) designed to simulate a contemporary sports-betting exchange with in-play betting during track-racing events such as horse races. We use the BBE ABM and its array of minimally-simple bettor-agents as a synthetic data generator which feeds into our XGBoost ML system, with the intention that XGBoost discovers profitable dynamic betting strategies by learning from the more profitable bets made by the BBE bettor-agents. After this XGBoost training, which results in one or more decision trees, a bettor-agent with a betting strategy determined by the XGBoost-learned decision tree(s) is added to the BBE ABM and made to bet on a sequence of races under various conditions and betting-market scenarios, with profitability serving as the primary metric of comparison and evaluation. Our initial findings presented here show that XGBoost trained in this way can indeed learn profitable betting strategies, and can generalise to learn strategies that outperform each of the set of strategies used for creation of the training data. To foster further research and enhancements, the complete version of our extended BBE, including the XGBoost integration, has been made freely available as an open-source release on GitHub.","sentences":["We present first results from the use of XGBoost, a highly effective machine learning (ML) method, within the Bristol Betting Exchange (BBE), an open-source agent-based model (ABM) designed to simulate a contemporary sports-betting exchange with in-play betting during track-racing events such as horse races.","We use the BBE ABM and its array of minimally-simple bettor-agents as a synthetic data generator which feeds into our XGBoost ML system, with the intention that XGBoost discovers profitable dynamic betting strategies by learning from the more profitable bets made by the BBE bettor-agents.","After this XGBoost training, which results in one or more decision trees, a bettor-agent with a betting strategy determined by the XGBoost-learned decision tree(s) is added to the BBE ABM and made to bet on a sequence of races under various conditions and betting-market scenarios, with profitability serving as the primary metric of comparison and evaluation.","Our initial findings presented here show that XGBoost trained in this way can indeed learn profitable betting strategies, and can generalise to learn strategies that outperform each of the set of strategies used for creation of the training data.","To foster further research and enhancements, the complete version of our extended BBE, including the XGBoost integration, has been made freely available as an open-source release on GitHub."],"url":"http://arxiv.org/abs/2401.06086v1"}
{"created":"2024-01-11 17:58:41","title":"Improving Large Language Models via Fine-grained Reinforcement Learning with Minimum Editing Constraint","abstract":"Reinforcement learning (RL) has been widely used in training large language models~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness and errors. However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness. To address it, we propose a new RL method named \\textbf{RLMEC} that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training. Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process. And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens. The experiment results on mathematical tasks and question-answering tasks have demonstrated the effectiveness of our approach. Our code and data are available at \\url{https://github.com/RUCAIBox/RLMEC}.","sentences":["Reinforcement learning (RL) has been widely used in training large language models~(LLMs) for preventing unexpected outputs, \\eg reducing harmfulness and errors.","However, existing RL methods mostly adopt the instance-level reward, which is unable to provide fine-grained supervision for complex reasoning tasks, and can not focus on the few key tokens that lead to the incorrectness.","To address it, we propose a new RL method named \\textbf{RLMEC} that incorporates a generative model as the reward model, which is trained by the erroneous solution rewriting task under the minimum editing constraint, and can produce token-level rewards for RL training.","Based on the generative reward model, we design the token-level RL objective for training and an imitation-based regularization for stabilizing RL process.","And the both objectives focus on the learning of the key tokens for the erroneous solution, reducing the effect of other unimportant tokens.","The experiment results on mathematical tasks and question-answering tasks have demonstrated the effectiveness of our approach.","Our code and data are available at \\url{https://github.com/RUCAIBox/RLMEC}."],"url":"http://arxiv.org/abs/2401.06081v1"}
{"created":"2024-01-11 17:56:59","title":"Secrets of RLHF in Large Language Models Part II: Reward Modeling","abstract":"Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses. Reward models are trained as proxies for human preferences to drive reinforcement learning optimization. While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent. (2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.   In this report, we attempt to address these two issues. (1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models. Experimental results confirm that data with varying preference strengths have different impacts on reward model performance. We introduce a series of novel methods to mitigate the influence of incorrect and ambiguous preferences in the dataset and fully leverage high-quality preference data. (2) From an algorithmic standpoint, we introduce contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses, thereby improving model generalization. Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples, and this approach can be utilized for iterative RLHF optimization.","sentences":["Reinforcement Learning from Human Feedback (RLHF) has become a crucial technology for aligning language models with human values and intentions, enabling models to produce more helpful and harmless responses.","Reward models are trained as proxies for human preferences to drive reinforcement learning optimization.","While reward models are often considered central to achieving high performance, they face the following challenges in practical applications: (1) Incorrect and ambiguous preference pairs in the dataset may hinder the reward model from accurately capturing human intent.","(2) Reward models trained on data from a specific distribution often struggle to generalize to examples outside that distribution and are not suitable for iterative RLHF training.   ","In this report, we attempt to address these two issues.","(1) From a data perspective, we propose a method to measure the strength of preferences within the data, based on a voting mechanism of multiple reward models.","Experimental results confirm that data with varying preference strengths have different impacts on reward model performance.","We introduce a series of novel methods to mitigate the influence of incorrect and ambiguous preferences in the dataset and fully leverage high-quality preference data.","(2) From an algorithmic standpoint, we introduce contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses, thereby improving model generalization.","Furthermore, we employ meta-learning to enable the reward model to maintain the ability to differentiate subtle differences in out-of-distribution samples, and this approach can be utilized for iterative RLHF optimization."],"url":"http://arxiv.org/abs/2401.06080v1"}
{"created":"2024-01-11 17:42:47","title":"Chain of History: Learning and Forecasting with LLMs for Temporal Knowledge Graph Completion","abstract":"Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge. Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain. We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines. Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities. We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results. We also carry out sufficient ablation experiments to explore the key influencing factors when LLMs perform structured temporal knowledge inference tasks.","sentences":["Temporal Knowledge Graph Completion (TKGC) is a challenging task of predicting missing event links at future timestamps by leveraging established temporal structural knowledge.","Given the formidable generative capabilities inherent in LLMs (LLMs), this paper proposes a novel approach to conceptualize temporal link prediction as an event generation task within the context of a historical event chain.","We employ efficient fine-tuning methods to make LLMs adapt to specific graph textual information and patterns discovered in temporal timelines.","Furthermore, we introduce structure-based historical data augmentation and the integration of reverse knowledge to emphasize LLMs' awareness of structural information, thereby enhancing their reasoning capabilities.","We conduct thorough experiments on multiple widely used datasets and find that our fine-tuned model outperforms existing embedding-based models on multiple metrics, achieving SOTA results.","We also carry out sufficient ablation experiments to explore the key influencing factors when LLMs perform structured temporal knowledge inference tasks."],"url":"http://arxiv.org/abs/2401.06072v1"}
{"created":"2024-01-11 17:41:57","title":"LEGO:Language Enhanced Multi-modal Grounding Model","abstract":"Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities. However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities. Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding. To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks. In this paper, we propose LEGO, a language enhanced multi-modal grounding model. Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input. It demonstrates precise identification and localization of specific regions in images or moments in videos. To achieve this objective, we design a diversified dataset construction pipeline, resulting in a multi-modal, multi-granularity dataset for model training. The code, dataset, and demo of our model can be found at https: //github.com/lzw-lzw/LEGO.","sentences":["Multi-modal large language models have demonstrated impressive performance across various tasks in different modalities.","However, existing multi-modal models primarily emphasize capturing global information within each modality while neglecting the importance of perceiving local information across modalities.","Consequently, these models lack the ability to effectively understand the fine-grained details of input data, limiting their performance in tasks that require a more nuanced understanding.","To address this limitation, there is a compelling need to develop models that enable fine-grained understanding across multiple modalities, thereby enhancing their applicability to a wide range of tasks.","In this paper, we propose LEGO, a language enhanced multi-modal grounding model.","Beyond capturing global information like other multi-modal models, our proposed model excels at tasks demanding a detailed understanding of local information within the input.","It demonstrates precise identification and localization of specific regions in images or moments in videos.","To achieve this objective, we design a diversified dataset construction pipeline, resulting in a multi-modal, multi-granularity dataset for model training.","The code, dataset, and demo of our model can be found at https: //github.com/lzw-lzw/LEGO."],"url":"http://arxiv.org/abs/2401.06071v1"}
{"created":"2024-01-11 17:31:42","title":"DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models","abstract":"In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations. Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations.","sentences":["In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters.","However, conventional MoE architectures like GShard, which activate the top-$K$ out of $N$ experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge.","In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization.","It involves two principal strategies: (1) finely segmenting the experts into $mN$ ones and activating $mK$ from them, allowing for a more flexible combination of activated experts; (2) isolating $K_s$ experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts.","Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 times the expert parameters and computation.","In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which set the upper bound of MoE models.","Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with LLaMA2 7B, with only about 40% of computations.","Further, our preliminary efforts to scale up DeepSeekMoE to 145B parameters consistently validate its substantial advantages over the GShard architecture, and show its performance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%) of computations."],"url":"http://arxiv.org/abs/2401.06066v1"}
{"created":"2024-01-11 17:24:49","title":"Investigating Data Contamination for Pre-training Language Models","abstract":"Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks. However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance. There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks. In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}. We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data. We also investigate the effects of repeating contamination for various downstream tasks. Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy. Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies.","sentences":["Language models pre-trained on web-scale corpora demonstrate impressive capabilities on diverse downstream tasks.","However, there is increasing concern whether such capabilities might arise from evaluation datasets being included in the pre-training corpus -- a phenomenon known as \\textit{data contamination} -- in a manner that artificially increases performance.","There has been little understanding of how this potential contamination might influence LMs' performance on downstream tasks.","In this paper, we explore the impact of data contamination at the pre-training stage by pre-training a series of GPT-2 models \\textit{from scratch}.","We highlight the effect of both text contamination (\\textit{i.e.}\\ input text of the evaluation samples) and ground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and the desired outputs) from evaluation data.","We also investigate the effects of repeating contamination for various downstream tasks.","Additionally, we examine the prevailing n-gram-based definitions of contamination within current LLM reports, pinpointing their limitations and inadequacy.","Our findings offer new insights into data contamination's effects on language model capabilities and underscore the need for independent, comprehensive contamination assessments in LLM studies."],"url":"http://arxiv.org/abs/2401.06059v1"}
{"created":"2024-01-11 17:20:34","title":"MatSynth: A Modern PBR Materials Dataset","abstract":"We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR materials. Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries. Given their importance, significant research effort was dedicated to their representation, creation and acquisition. However, in the past 6 years, most research in material acquisiton or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials. With this dataset we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available. We carefully discuss the data collection process and demonstrate the benefits of this dataset on material acquisition and generation applications. The complete data further contains metadata with each material's origin, license, category, tags, creation method and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings. The MatSynth dataset is released through the project page at: https://www.gvecchio.com/matsynth.","sentences":["We introduce MatSynth, a dataset of $4,000+$ CC0 ultra-high resolution PBR materials.","Materials are crucial components of virtual relightable assets, defining the interaction of light at the surface of geometries.","Given their importance, significant research effort was dedicated to their representation, creation and acquisition.","However, in the past 6 years, most research in material acquisiton or generation relied either on the same unique dataset, or on company-owned huge library of procedural materials.","With this dataset we propose a significantly larger, more diverse, and higher resolution set of materials than previously publicly available.","We carefully discuss the data collection process and demonstrate the benefits of this dataset on material acquisition and generation applications.","The complete data further contains metadata with each material's origin, license, category, tags, creation method and, when available, descriptions and physical size, as well as 3M+ renderings of the augmented materials, in 1K, under various environment lightings.","The MatSynth dataset is released through the project page at: https://www.gvecchio.com/matsynth."],"url":"http://arxiv.org/abs/2401.06056v1"}
{"created":"2024-01-11 17:15:16","title":"Fast High Dynamic Range Radiance Fields for Dynamic Scenes","abstract":"Neural Radiances Fields (NeRF) and their extensions have shown great success in representing 3D scenes and synthesizing novel-view images. However, most NeRF methods take in low-dynamic-range (LDR) images, which may lose details, especially with nonuniform illumination. Some previous NeRF methods attempt to introduce high-dynamic-range (HDR) techniques but mainly target static scenes. To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images captured with various exposures. A learnable exposure mapping function is constructed to obtain adaptive exposure values for each image. Based on the monotonically increasing prior, a camera response function is designed for stable learning. With the proposed model, high-quality novel-view images at any time point can be rendered with any desired exposure. We further construct a dataset containing multiple dynamic scenes captured with diverse exposures for evaluation. All the datasets and code are available at \\url{https://guanjunwu.github.io/HDR-HexPlane/}.","sentences":["Neural Radiances Fields (NeRF) and their extensions have shown great success in representing 3D scenes and synthesizing novel-view images.","However, most NeRF methods take in low-dynamic-range (LDR) images, which may lose details, especially with nonuniform illumination.","Some previous NeRF methods attempt to introduce high-dynamic-range (HDR) techniques but mainly target static scenes.","To extend HDR NeRF methods to wider applications, we propose a dynamic HDR NeRF framework, named HDR-HexPlane, which can learn 3D scenes from dynamic 2D images captured with various exposures.","A learnable exposure mapping function is constructed to obtain adaptive exposure values for each image.","Based on the monotonically increasing prior, a camera response function is designed for stable learning.","With the proposed model, high-quality novel-view images at any time point can be rendered with any desired exposure.","We further construct a dataset containing multiple dynamic scenes captured with diverse exposures for evaluation.","All the datasets and code are available at \\url{https://guanjunwu.github.io/HDR-HexPlane/}."],"url":"http://arxiv.org/abs/2401.06052v1"}
{"created":"2024-01-11 17:09:40","title":"On the Power of Graph Neural Networks and Feature Augmentation Strategies to Classify Social Networks","abstract":"This paper studies four Graph Neural Network architectures (GNNs) for a graph classification task on a synthetic dataset created using classic generative models of Network Science. Since the synthetic networks do not contain (node or edge) features, five different augmentation strategies (artificial feature types) are applied to nodes. All combinations of the 4 GNNs (GCN with Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types (constant 1, noise, degree, normalized degree and ID -- a vector of the number of cycles of various lengths) are studied and their performances compared as a function of the hidden dimension of artificial neural networks used in the GNNs. The generalisation ability of these models is also analysed using a second synthetic network dataset (containing networks of different sizes).Our results point towards the balanced importance of the computational power of the GNN architecture and the the information level provided by the artificial features. GNN architectures with higher computational power, like GIN and GATv2, perform well for most augmentation strategies. On the other hand, artificial features with higher information content, like ID or degree, not only consistently outperform other augmentation strategies, but can also help GNN architectures with lower computational power to achieve good performance.","sentences":["This paper studies four Graph Neural Network architectures (GNNs) for a graph classification task on a synthetic dataset created using classic generative models of Network Science.","Since the synthetic networks do not contain (node or edge) features, five different augmentation strategies (artificial feature types) are applied to nodes.","All combinations of the 4 GNNs (GCN with Hierarchical and Global aggregation, GIN and GATv2) and the 5 feature types (constant 1, noise, degree, normalized degree and ID -- a vector of the number of cycles of various lengths) are studied and their performances compared as a function of the hidden dimension of artificial neural networks used in the GNNs.","The generalisation ability of these models is also analysed using a second synthetic network dataset (containing networks of different sizes).Our results point towards the balanced importance of the computational power of the GNN architecture and the the information level provided by the artificial features.","GNN architectures with higher computational power, like GIN and GATv2, perform well for most augmentation strategies.","On the other hand, artificial features with higher information content, like ID or degree, not only consistently outperform other augmentation strategies, but can also help GNN architectures with lower computational power to achieve good performance."],"url":"http://arxiv.org/abs/2401.06048v1"}
{"created":"2024-01-11 17:07:47","title":"Computing Data Distribution from Query Selectivities","abstract":"We are given a set $\\mathcal{Z}=\\{(R_1,s_1),\\ldots, (R_n,s_n)\\}$, where each $R_i$ is a \\emph{range} in $\\Re^d$, such as rectangle or ball, and $s_i \\in [0,1]$ denotes its \\emph{selectivity}. The goal is to compute a small-size \\emph{discrete data distribution} $\\mathcal{D}=\\{(q_1,w_1),\\ldots, (q_m,w_m)\\}$, where $q_j\\in \\Re^d$ and $w_j\\in [0,1]$ for each $1\\leq j\\leq m$, and $\\sum_{1\\leq j\\leq m}w_j= 1$, such that $\\mathcal{D}$ is the most \\emph{consistent} with $\\mathcal{Z}$, i.e., $\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})=\\frac{1}{n}\\sum_{i=1}^n\\! \\lvert{s_i-\\sum_{j=1}^m w_j\\cdot 1(q_j\\in R_i)}\\rvert^p$ is minimized. In a database setting, $\\mathcal{Z}$ corresponds to a workload of range queries over some table, together with their observed selectivities (i.e., fraction of tuples returned), and $\\mathcal{D}$ can be used as compact model for approximating the data distribution within the table without accessing the underlying contents.   In this paper, we obtain both upper and lower bounds for this problem. In particular, we show that the problem of finding the best data distribution from selectivity queries is $\\mathsf{NP}$-complete. On the positive side, we describe a Monte Carlo algorithm that constructs, in time $O((n+\\delta^{-d})\\delta^{-2}\\mathop{\\mathrm{polylog}})$, a discrete distribution $\\tilde{\\mathcal{D}}$ of size $O(\\delta^{-2})$, such that $\\mathrm{err}_p(\\tilde{\\mathcal{D}},\\mathcal{Z})\\leq \\min_{\\mathcal{D}}\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})+\\delta$ (for $p=1,2,\\infty$) where the minimum is taken over all discrete distributions. We also establish conditional lower bounds, which strongly indicate the infeasibility of relative approximations as well as removal of the exponential dependency on the dimension for additive approximations. This suggests that significant improvements to our algorithm are unlikely.","sentences":["We are given a set $\\mathcal{Z}=\\{(R_1,s_1),\\ldots, (R_n,s_n)\\}$, where each $R_i$ is a \\emph{range} in $\\Re^d$, such as rectangle or ball, and $s_i \\in","[0,1]$ denotes its \\emph{selectivity}.","The goal is to compute a small-size \\emph{discrete data distribution} $\\mathcal{D}=\\{(q_1,w_1),\\ldots, (q_m,w_m)\\}$, where $q_j\\in \\Re^d$ and $w_j\\in","[0,1]$ for each $1\\leq j\\leq m$, and $\\sum_{1\\leq j\\leq m}w_j= 1$, such that $\\mathcal{D}$ is the most \\emph{consistent} with $\\mathcal{Z}$, i.e., $\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})=\\frac{1}{n}\\sum_{i=1}^n\\! \\lvert{s_i-\\sum_{j=1}^m w_j\\cdot 1(q_j\\in R_i)}\\rvert^p$ is minimized.","In a database setting, $\\mathcal{Z}$ corresponds to a workload of range queries over some table, together with their observed selectivities (i.e., fraction of tuples returned), and $\\mathcal{D}$ can be used as compact model for approximating the data distribution within the table without accessing the underlying contents.   ","In this paper, we obtain both upper and lower bounds for this problem.","In particular, we show that the problem of finding the best data distribution from selectivity queries is $\\mathsf{NP}$-complete.","On the positive side, we describe a Monte Carlo algorithm that constructs, in time $O((n+\\delta^{-d})\\delta^{-2}\\mathop{\\mathrm{polylog}})$, a discrete distribution $\\tilde{\\mathcal{D}}$ of size $O(\\delta^{-2})$, such that $\\mathrm{err}_p(\\tilde{\\mathcal{D}},\\mathcal{Z})\\leq \\min_{\\mathcal{D}}\\mathrm{err}_p(\\mathcal{D},\\mathcal{Z})+\\delta$ (for $p=1,2,\\infty$) where the minimum is taken over all discrete distributions.","We also establish conditional lower bounds, which strongly indicate the infeasibility of relative approximations as well as removal of the exponential dependency on the dimension for additive approximations.","This suggests that significant improvements to our algorithm are unlikely."],"url":"http://arxiv.org/abs/2401.06047v1"}
{"created":"2024-01-11 17:02:31","title":"Safeguarding DeFi Smart Contracts against Oracle Deviations","abstract":"This paper presents OVer, a framework designed to automatically analyze the behavior of decentralized finance (DeFi) protocols when subjected to a \"skewed\" oracle input. OVer firstly performs symbolic analysis on the given contract and constructs a model of constraints. Then, the framework leverages an SMT solver to identify parameters that allow its secure operation. Furthermore, guard statements may be generated for smart contracts that may use the oracle values, thus effectively preventing oracle manipulation attacks. Empirical results show that OVer can successfully analyze all 10 benchmarks collected, which encompass a diverse range of DeFi protocols. Additionally, this paper also illustrates that current parameters utilized in the majority of benchmarks are inadequate to ensure safety when confronted with significant oracle deviations.","sentences":["This paper presents OVer, a framework designed to automatically analyze the behavior of decentralized finance (DeFi) protocols when subjected to a \"skewed\" oracle input.","OVer firstly performs symbolic analysis on the given contract and constructs a model of constraints.","Then, the framework leverages an SMT solver to identify parameters that allow its secure operation.","Furthermore, guard statements may be generated for smart contracts that may use the oracle values, thus effectively preventing oracle manipulation attacks.","Empirical results show that OVer can successfully analyze all 10 benchmarks collected, which encompass a diverse range of DeFi protocols.","Additionally, this paper also illustrates that current parameters utilized in the majority of benchmarks are inadequate to ensure safety when confronted with significant oracle deviations."],"url":"http://arxiv.org/abs/2401.06044v1"}
{"created":"2024-01-11 16:55:48","title":"Wavelet-Inspired Multiscale Graph Convolutional Recurrent Network for Traffic Forecasting","abstract":"Traffic forecasting is the foundation for intelligent transportation systems. Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting. However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale. To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method. In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the information from all streams for traffic metrics reconstruction and prediction. Furthermore, road-network-informed graphs and data-driven graph learning are combined to accurately capture spatial correlation. The proposed method can offer well-defined interpretability, powerful learning capability, and competitive forecasting performance on real-world traffic data sets.","sentences":["Traffic forecasting is the foundation for intelligent transportation systems.","Spatiotemporal graph neural networks have demonstrated state-of-the-art performance in traffic forecasting.","However, these methods do not explicitly model some of the natural characteristics in traffic data, such as the multiscale structure that encompasses spatial and temporal variations at different levels of granularity or scale.","To that end, we propose a Wavelet-Inspired Graph Convolutional Recurrent Network (WavGCRN) which combines multiscale analysis (MSA)-based method with Deep Learning (DL)-based method.","In WavGCRN, the traffic data is decomposed into time-frequency components with Discrete Wavelet Transformation (DWT), constructing a multi-stream input structure; then Graph Convolutional Recurrent networks (GCRNs) are employed as encoders for each stream, extracting spatiotemporal features in different scales; and finally the learnable Inversed DWT and GCRN are combined as the decoder, fusing the information from all streams for traffic metrics reconstruction and prediction.","Furthermore, road-network-informed graphs and data-driven graph learning are combined to accurately capture spatial correlation.","The proposed method can offer well-defined interpretability, powerful learning capability, and competitive forecasting performance on real-world traffic data sets."],"url":"http://arxiv.org/abs/2401.06040v1"}
{"created":"2024-01-11 16:48:44","title":"RAVEN: Rethinking Adversarial Video Generation with Efficient Tri-plane Networks","abstract":"We present a novel unconditional video generative model designed to address long-term spatial and temporal dependencies. To capture these dependencies, our approach incorporates a hybrid explicit-implicit tri-plane representation inspired by 3D-aware generative frameworks developed for three-dimensional object representation and employs a singular latent code to model an entire video sequence. Individual video frames are then synthesized from an intermediate tri-plane representation, which itself is derived from the primary latent code. This novel strategy reduces computational complexity by a factor of $2$ as measured in FLOPs. Consequently, our approach facilitates the efficient and temporally coherent generation of videos. Moreover, our joint frame modeling approach, in contrast to autoregressive methods, mitigates the generation of visual artifacts. We further enhance the model's capabilities by integrating an optical flow-based module within our Generative Adversarial Network (GAN) based generator architecture, thereby compensating for the constraints imposed by a smaller generator size. As a result, our model is capable of synthesizing high-fidelity video clips at a resolution of $256\\times256$ pixels, with durations extending to more than $5$ seconds at a frame rate of 30 fps. The efficacy and versatility of our approach are empirically validated through qualitative and quantitative assessments across three different datasets comprising both synthetic and real video clips.","sentences":["We present a novel unconditional video generative model designed to address long-term spatial and temporal dependencies.","To capture these dependencies, our approach incorporates a hybrid explicit-implicit tri-plane representation inspired by 3D-aware generative frameworks developed for three-dimensional object representation and employs a singular latent code to model an entire video sequence.","Individual video frames are then synthesized from an intermediate tri-plane representation, which itself is derived from the primary latent code.","This novel strategy reduces computational complexity by a factor of $2$ as measured in FLOPs.","Consequently, our approach facilitates the efficient and temporally coherent generation of videos.","Moreover, our joint frame modeling approach, in contrast to autoregressive methods, mitigates the generation of visual artifacts.","We further enhance the model's capabilities by integrating an optical flow-based module within our Generative Adversarial Network (GAN) based generator architecture, thereby compensating for the constraints imposed by a smaller generator size.","As a result, our model is capable of synthesizing high-fidelity video clips at a resolution of $256\\times256$ pixels, with durations extending to more than $5$ seconds at a frame rate of 30 fps.","The efficacy and versatility of our approach are empirically validated through qualitative and quantitative assessments across three different datasets comprising both synthetic and real video clips."],"url":"http://arxiv.org/abs/2401.06035v1"}
{"created":"2024-01-11 16:48:00","title":"LinguAlchemy: Fusing Typological and Geographical Elements for Unseen Language Generalization","abstract":"Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages. Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline. This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology. In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints. LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization. We further introduce AlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts the linguistic regularization weights automatically, alleviating the need for hyperparameter search. LinguAlchemy enables better cross-lingual generalization to unseen languages which is vital for better inclusivity and accessibility of PLMs.","sentences":["Pretrained language models (PLMs) have shown remarkable generalization toward multiple tasks and languages.","Nonetheless, the generalization of PLMs towards unseen languages is poor, resulting in significantly worse language performance, or even generating nonsensical responses that are comparable to a random baseline.","This limitation has been a longstanding problem of PLMs raising the problem of diversity and equal access to language modeling technology.","In this work, we solve this limitation by introducing LinguAlchemy, a regularization technique that incorporates various aspects of languages covering typological, geographical, and phylogenetic constraining the resulting representation of PLMs to better characterize the corresponding linguistics constraints.","LinguAlchemy significantly improves the accuracy performance of mBERT and XLM-R on unseen languages by ~18% and ~2%, respectively compared to fully finetuned models and displaying a high degree of unseen language generalization.","We further introduce AlchemyScale and AlchemyTune, extension of LinguAlchemy which adjusts the linguistic regularization weights automatically, alleviating the need for hyperparameter search.","LinguAlchemy enables better cross-lingual generalization to unseen languages which is vital for better inclusivity and accessibility of PLMs."],"url":"http://arxiv.org/abs/2401.06034v1"}
{"created":"2024-01-11 16:43:16","title":"GE-AdvGAN: Improving the transferability of adversarial samples by gradient editing-based adversarial generative model","abstract":"Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio. Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios. The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications. However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods. Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation. Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency. The main approach is via optimising the training process of the generator parameters. With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models. Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms. The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm. The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN","sentences":["Adversarial generative models, such as Generative Adversarial Networks (GANs), are widely applied for generating various types of data, i.e., images, text, and audio.","Accordingly, its promising performance has led to the GAN-based adversarial attack methods in the white-box and black-box attack scenarios.","The importance of transferable black-box attacks lies in their ability to be effective across different models and settings, more closely aligning with real-world applications.","However, it remains challenging to retain the performance in terms of transferable adversarial examples for such methods.","Meanwhile, we observe that some enhanced gradient-based transferable adversarial attack algorithms require prolonged time for adversarial sample generation.","Thus, in this work, we propose a novel algorithm named GE-AdvGAN to enhance the transferability of adversarial samples whilst improving the algorithm's efficiency.","The main approach is via optimising the training process of the generator parameters.","With the functional and characteristic similarity analysis, we introduce a novel gradient editing (GE) mechanism and verify its feasibility in generating transferable samples on various models.","Moreover, by exploring the frequency domain information to determine the gradient editing direction, GE-AdvGAN can generate highly transferable adversarial samples while minimizing the execution time in comparison to the state-of-the-art transferable adversarial attack algorithms.","The performance of GE-AdvGAN is comprehensively evaluated by large-scale experiments on different datasets, which results demonstrate the superiority of our algorithm.","The code for our algorithm is available at: https://github.com/LMBTough/GE-advGAN"],"url":"http://arxiv.org/abs/2401.06031v1"}
{"created":"2024-01-11 16:42:10","title":"Can We Trust the Unlabeled Target Data? Towards Backdoor Attack and Defense on Model Adaptation","abstract":"Model adaptation tackles the distribution shift problem with a pre-trained model instead of raw data, becoming a popular paradigm due to its great privacy protection. Existing methods always assume adapting to a clean target domain, overlooking the security risks of unlabeled samples. In this paper, we explore the potential backdoor attacks on model adaptation launched by well-designed poisoning target data. Concretely, we provide two backdoor triggers with two poisoning strategies for different prior knowledge owned by attackers. These attacks achieve a high success rate and keep the normal performance on clean samples in the test stage. To defend against backdoor embedding, we propose a plug-and-play method named MixAdapt, combining it with existing adaptation algorithms. Experiments across commonly used benchmarks and adaptation methods demonstrate the effectiveness of MixAdapt. We hope this work will shed light on the safety of learning with unlabeled data.","sentences":["Model adaptation tackles the distribution shift problem with a pre-trained model instead of raw data, becoming a popular paradigm due to its great privacy protection.","Existing methods always assume adapting to a clean target domain, overlooking the security risks of unlabeled samples.","In this paper, we explore the potential backdoor attacks on model adaptation launched by well-designed poisoning target data.","Concretely, we provide two backdoor triggers with two poisoning strategies for different prior knowledge owned by attackers.","These attacks achieve a high success rate and keep the normal performance on clean samples in the test stage.","To defend against backdoor embedding, we propose a plug-and-play method named MixAdapt, combining it with existing adaptation algorithms.","Experiments across commonly used benchmarks and adaptation methods demonstrate the effectiveness of MixAdapt.","We hope this work will shed light on the safety of learning with unlabeled data."],"url":"http://arxiv.org/abs/2401.06030v1"}
{"created":"2024-01-11 16:32:11","title":"Topology-Driven Parallel Trajectory Optimization in Dynamic Environments","abstract":"Ground robots navigating in complex, dynamic environments must compute collision-free trajectories to avoid obstacles safely and efficiently. Nonconvex optimization is a popular method to compute a trajectory in real-time. However, these methods often converge to locally optimal solutions and frequently switch between different local minima, leading to inefficient and unsafe robot motion. In this work, We propose a novel topology-driven trajectory optimization strategy for dynamic environments that plans multiple distinct evasive trajectories to enhance the robot's behavior and efficiency. A global planner iteratively generates trajectories in distinct homotopy classes. These trajectories are then optimized by local planners working in parallel. While each planner shares the same navigation objectives, they are locally constrained to a specific homotopy class, meaning each local planner attempts a different evasive maneuver. The robot then executes the feasible trajectory with the lowest cost in a receding horizon manner. We demonstrate, on a mobile robot navigating among pedestrians, that our approach leads to faster and safer trajectories than existing planners.","sentences":["Ground robots navigating in complex, dynamic environments must compute collision-free trajectories to avoid obstacles safely and efficiently.","Nonconvex optimization is a popular method to compute a trajectory in real-time.","However, these methods often converge to locally optimal solutions and frequently switch between different local minima, leading to inefficient and unsafe robot motion.","In this work, We propose a novel topology-driven trajectory optimization strategy for dynamic environments that plans multiple distinct evasive trajectories to enhance the robot's behavior and efficiency.","A global planner iteratively generates trajectories in distinct homotopy classes.","These trajectories are then optimized by local planners working in parallel.","While each planner shares the same navigation objectives, they are locally constrained to a specific homotopy class, meaning each local planner attempts a different evasive maneuver.","The robot then executes the feasible trajectory with the lowest cost in a receding horizon manner.","We demonstrate, on a mobile robot navigating among pedestrians, that our approach leads to faster and safer trajectories than existing planners."],"url":"http://arxiv.org/abs/2401.06021v1"}
{"created":"2024-01-11 16:30:07","title":"Automatic UAV-based Airport Pavement Inspection Using Mixed Real and Virtual Scenarios","abstract":"Runway and taxiway pavements are exposed to high stress during their projected lifetime, which inevitably leads to a decrease in their condition over time. To make sure airport pavement condition ensure uninterrupted and resilient operations, it is of utmost importance to monitor their condition and conduct regular inspections. UAV-based inspection is recently gaining importance due to its wide range monitoring capabilities and reduced cost. In this work, we propose a vision-based approach to automatically identify pavement distress using images captured by UAVs. The proposed method is based on Deep Learning (DL) to segment defects in the image. The DL architecture leverages the low computational capacities of embedded systems in UAVs by using an optimised implementation of EfficientNet feature extraction and Feature Pyramid Network segmentation. To deal with the lack of annotated data for training we have developed a synthetic dataset generation methodology to extend available distress datasets. We demonstrate that the use of a mixed dataset composed of synthetic and real training images yields better results when testing the training models in real application scenarios.","sentences":["Runway and taxiway pavements are exposed to high stress during their projected lifetime, which inevitably leads to a decrease in their condition over time.","To make sure airport pavement condition ensure uninterrupted and resilient operations, it is of utmost importance to monitor their condition and conduct regular inspections.","UAV-based inspection is recently gaining importance due to its wide range monitoring capabilities and reduced cost.","In this work, we propose a vision-based approach to automatically identify pavement distress using images captured by UAVs.","The proposed method is based on Deep Learning (DL) to segment defects in the image.","The DL architecture leverages the low computational capacities of embedded systems in UAVs by using an optimised implementation of EfficientNet feature extraction and Feature Pyramid Network segmentation.","To deal with the lack of annotated data for training we have developed a synthetic dataset generation methodology to extend available distress datasets.","We demonstrate that the use of a mixed dataset composed of synthetic and real training images yields better results when testing the training models in real application scenarios."],"url":"http://arxiv.org/abs/2401.06019v1"}
{"created":"2024-01-11 16:22:42","title":"Surgical-DINO: Adapter Learning of Foundation Model for Depth Estimation in Endoscopic Surgery","abstract":"Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization. Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications. This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation. Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery. We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning. During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene. Results: Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks. The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation. Conclusion: Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation. There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly. Code is available at https://github.com/BeileiCui/SurgicalDINO.","sentences":["Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction, surgical navigation and augmented reality visualization.","Although the foundation model exhibits outstanding performance in many vision tasks, including depth estimation (e.g., DINOv2), recent works observed its limitations in medical and surgical domain-specific applications.","This work presents a low-ranked adaptation (LoRA) of the foundation model for surgical depth estimation.","Methods: We design a foundation model-based depth estimation method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for depth estimation in endoscopic surgery.","We build LoRA layers and integrate them into DINO to adapt with surgery-specific domain knowledge instead of conventional fine-tuning.","During training, we freeze the DINO image encoder, which shows excellent visual representation capacity, and only optimize the LoRA layers and depth decoder to integrate features from the surgical scene.","Results:","Our model is extensively validated on a MICCAI challenge dataset of SCARED, which is collected from da Vinci Xi endoscope surgery.","We empirically show that Surgical-DINO significantly outperforms all the state-of-the-art models in endoscopic depth estimation tasks.","The analysis with ablation studies has shown evidence of the remarkable effect of our LoRA layers and adaptation.","Conclusion: Surgical-DINO shed some light on the successful adaptation of the foundation models into the surgical domain for depth estimation.","There is clear evidence in the results that zero-shot prediction on pre-trained weights in computer vision datasets or naive fine-tuning is not sufficient to use the foundation model in the surgical domain directly.","Code is available at https://github.com/BeileiCui/SurgicalDINO."],"url":"http://arxiv.org/abs/2401.06013v1"}
{"created":"2024-01-11 16:16:20","title":"Attention to detail: inter-resolution knowledge distillation","abstract":"The development of computer vision solutions for gigapixel images in digital pathology is hampered by significant computational limitations due to the large size of whole slide images. In particular, digitizing biopsies at high resolutions is a time-consuming process, which is necessary due to the worsening results from the decrease in image detail. To alleviate this issue, recent literature has proposed using knowledge distillation to enhance the model performance at reduced image resolutions. In particular, soft labels and features extracted at the highest magnification level are distilled into a model that takes lower-magnification images as input. However, this approach fails to transfer knowledge about the most discriminative image regions in the classification process, which may be lost when the resolution is decreased. In this work, we propose to distill this information by incorporating attention maps during training. In particular, our formulation leverages saliency maps of the target class via grad-CAMs, which guides the lower-resolution Student model to match the Teacher distribution by minimizing the l2 distance between them. Comprehensive experiments on prostate histology image grading demonstrate that the proposed approach substantially improves the model performance across different image resolutions compared to previous literature.","sentences":["The development of computer vision solutions for gigapixel images in digital pathology is hampered by significant computational limitations due to the large size of whole slide images.","In particular, digitizing biopsies at high resolutions is a time-consuming process, which is necessary due to the worsening results from the decrease in image detail.","To alleviate this issue, recent literature has proposed using knowledge distillation to enhance the model performance at reduced image resolutions.","In particular, soft labels and features extracted at the highest magnification level are distilled into a model that takes lower-magnification images as input.","However, this approach fails to transfer knowledge about the most discriminative image regions in the classification process, which may be lost when the resolution is decreased.","In this work, we propose to distill this information by incorporating attention maps during training.","In particular, our formulation leverages saliency maps of the target class via grad-CAMs, which guides the lower-resolution Student model to match the Teacher distribution by minimizing the l2 distance between them.","Comprehensive experiments on prostate histology image grading demonstrate that the proposed approach substantially improves the model performance across different image resolutions compared to previous literature."],"url":"http://arxiv.org/abs/2401.06010v1"}
{"created":"2024-01-11 16:14:30","title":"Sea ice detection using concurrent multispectral and synthetic aperture radar imagery","abstract":"Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea ice mapping due to its spatio-temporal coverage and the ability to detect sea ice independent of cloud and lighting conditions. Automatic sea ice detection using SAR imagery remains problematic due to the presence of ambiguous signal and noise within the image. Conversely, ice and water are easily distinguishable using multispectral imagery (MSI), but in the polar regions the ocean's surface is often occluded by cloud or the sun may not appear above the horizon for many months. To address some of these limitations, this paper proposes a new tool trained using concurrent multispectral Visible and SAR imagery for sea Ice Detection (ViSual\\_IceD). ViSual\\_IceD is a convolution neural network (CNN) that builds on the classic U-Net architecture by containing two parallel encoder stages, enabling the fusion and concatenation of MSI and SAR imagery containing different spatial resolutions. The performance of ViSual\\_IceD is compared with U-Net models trained using concatenated MSI and SAR imagery as well as models trained exclusively on MSI or SAR imagery. ViSual\\_IceD outperforms the other networks, with a F1 score 1.60\\% points higher than the next best network, and results indicate that ViSual\\_IceD is selective in the image type it uses during image segmentation. Outputs from ViSual\\_IceD are compared to sea ice concentration products derived from the AMSR2 Passive Microwave (PMW) sensor. Results highlight how ViSual\\_IceD is a useful tool to use in conjunction with PMW data, particularly in coastal regions. As the spatial-temporal coverage of MSI and SAR imagery continues to increase, ViSual\\_IceD provides a new opportunity for robust, accurate sea ice coverage detection in polar regions.","sentences":["Synthetic Aperture Radar (SAR) imagery is the primary data type used for sea ice mapping due to its spatio-temporal coverage and the ability to detect sea ice independent of cloud and lighting conditions.","Automatic sea ice detection using SAR imagery remains problematic due to the presence of ambiguous signal and noise within the image.","Conversely, ice and water are easily distinguishable using multispectral imagery (MSI), but in the polar regions the ocean's surface is often occluded by cloud or the sun may not appear above the horizon for many months.","To address some of these limitations, this paper proposes a new tool trained using concurrent multispectral Visible and SAR imagery for sea Ice Detection (ViSual\\_IceD).","ViSual\\_IceD is a convolution neural network (CNN) that builds on the classic U-Net architecture by containing two parallel encoder stages, enabling the fusion and concatenation of MSI and SAR imagery containing different spatial resolutions.","The performance of ViSual\\_IceD is compared with U-Net models trained using concatenated MSI and SAR imagery as well as models trained exclusively on MSI or SAR imagery.","ViSual\\_IceD outperforms the other networks, with a F1 score 1.60\\% points higher than the next best network, and results indicate that ViSual\\_IceD is selective in the image type it uses during image segmentation.","Outputs from ViSual\\_IceD are compared to sea ice concentration products derived from the AMSR2 Passive Microwave (PMW) sensor.","Results highlight how ViSual\\_IceD is a useful tool to use in conjunction with PMW data, particularly in coastal regions.","As the spatial-temporal coverage of MSI and SAR imagery continues to increase, ViSual\\_IceD provides a new opportunity for robust, accurate sea ice coverage detection in polar regions."],"url":"http://arxiv.org/abs/2401.06009v1"}
{"created":"2024-01-11 16:06:36","title":"TRIPS: Trilinear Point Splatting for Real-Time Radiance Field Rendering","abstract":"Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency. However, also latest approaches in this domain are not without their shortcomings. 3D Gaussian Splatting [Kerbl and Kopanas et al. 2023] struggles when tasked with rendering highly detailed scenes, due to blurring and cloudy artifacts. On the other hand, ADOP [R\\\"uckert et al. 2022] can accommodate crisper images, but the neural reconstruction network decreases performance, it grapples with temporal instability and it is unable to effectively address large gaps in the point cloud.   In this paper, we present TRIPS (Trilinear Point Splatting), an approach that combines ideas from both Gaussian Splatting and ADOP. The fundamental concept behind our novel technique involves rasterizing points into a screen-space image pyramid, with the selection of the pyramid layer determined by the projected point size. This approach allows rendering arbitrarily large points using a single trilinear write. A lightweight neural network is then used to reconstruct a hole-free image including detail beyond splat resolution. Importantly, our render pipeline is entirely differentiable, allowing for automatic optimization of both point sizes and positions.   Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art methods in terms of rendering quality while maintaining a real-time frame rate of 60 frames per second on readily available hardware. This performance extends to challenging scenarios, such as scenes featuring intricate geometry, expansive landscapes, and auto-exposed footage.","sentences":["Point-based radiance field rendering has demonstrated impressive results for novel view synthesis, offering a compelling blend of rendering quality and computational efficiency.","However, also latest approaches in this domain are not without their shortcomings.","3D Gaussian Splatting [Kerbl and Kopanas et al. 2023] struggles when tasked with rendering highly detailed scenes, due to blurring and cloudy artifacts.","On the other hand, ADOP [R\\\"uckert et al. 2022] can accommodate crisper images, but the neural reconstruction network decreases performance, it grapples with temporal instability and it is unable to effectively address large gaps in the point cloud.   ","In this paper, we present TRIPS (Trilinear Point Splatting), an approach that combines ideas from both Gaussian Splatting and ADOP.","The fundamental concept behind our novel technique involves rasterizing points into a screen-space image pyramid, with the selection of the pyramid layer determined by the projected point size.","This approach allows rendering arbitrarily large points using a single trilinear write.","A lightweight neural network is then used to reconstruct a hole-free image including detail beyond splat resolution.","Importantly, our render pipeline is entirely differentiable, allowing for automatic optimization of both point sizes and positions.   ","Our evaluation demonstrate that TRIPS surpasses existing state-of-the-art methods in terms of rendering quality while maintaining a real-time frame rate of 60 frames per second on readily available hardware.","This performance extends to challenging scenarios, such as scenes featuring intricate geometry, expansive landscapes, and auto-exposed footage."],"url":"http://arxiv.org/abs/2401.06003v1"}
{"created":"2024-01-11 15:59:11","title":"Boosting Mixed-Initiative Co-Creativity in Game Design: A Tutorial","abstract":"In recent years, there has been a growing application of mixed-initiative co-creative approaches in the creation of video games. The rapid advances in the capabilities of artificial intelligence (AI) systems further propel creative collaboration between humans and computational agents. In this tutorial, we present guidelines for researchers and practitioners to develop game design tools with a high degree of mixed-initiative co-creativity (MI-CCy). We begin by reviewing a selection of current works that will serve as case studies and categorize them by the type of game content they address. We introduce the MI-CCy Quantifier, a framework that can be used by researchers and developers to assess co-creative tools on their level of MI-CCy through a visual scheme of quantifiable criteria scales. We demonstrate the usage of the MI-CCy Quantifier by applying it to the selected works. This analysis enabled us to discern prevalent patterns within these tools, as well as features that contribute to a higher level of MI-CCy. We highlight current gaps in MI-CCy approaches within game design, which we propose as pivotal aspects to tackle in the development of forthcoming approaches.","sentences":["In recent years, there has been a growing application of mixed-initiative co-creative approaches in the creation of video games.","The rapid advances in the capabilities of artificial intelligence (AI) systems further propel creative collaboration between humans and computational agents.","In this tutorial, we present guidelines for researchers and practitioners to develop game design tools with a high degree of mixed-initiative co-creativity (MI-CCy).","We begin by reviewing a selection of current works that will serve as case studies and categorize them by the type of game content they address.","We introduce the MI-CCy Quantifier, a framework that can be used by researchers and developers to assess co-creative tools on their level of MI-CCy through a visual scheme of quantifiable criteria scales.","We demonstrate the usage of the MI-CCy Quantifier by applying it to the selected works.","This analysis enabled us to discern prevalent patterns within these tools, as well as features that contribute to a higher level of MI-CCy.","We highlight current gaps in MI-CCy approaches within game design, which we propose as pivotal aspects to tackle in the development of forthcoming approaches."],"url":"http://arxiv.org/abs/2401.05999v1"}
{"created":"2024-01-11 15:57:38","title":"Combating Adversarial Attacks with Multi-Agent Debate","abstract":"While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858. One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325. We implement multi-agent debate between current state-of-the-art language models and evaluate models' susceptibility to red team attacks in both single- and multi-agent settings. We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models. We also find marginal improvements through the general usage of multi-agent interactions. We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics.","sentences":["While state-of-the-art language models have achieved impressive results, they remain susceptible to inference-time adversarial attacks, such as adversarial prompts generated by red teams arXiv:2209.07858.","One approach proposed to improve the general quality of language model generations is multi-agent debate, where language models self-evaluate through discussion and feedback arXiv:2305.14325.","We implement multi-agent debate between current state-of-the-art language models and evaluate models' susceptibility to red team attacks in both single- and multi-agent settings.","We find that multi-agent debate can reduce model toxicity when jailbroken or less capable models are forced to debate with non-jailbroken or more capable models.","We also find marginal improvements through the general usage of multi-agent interactions.","We further perform adversarial prompt content classification via embedding clustering, and analyze the susceptibility of different models to different types of attack topics."],"url":"http://arxiv.org/abs/2401.05998v1"}
{"created":"2024-01-11 15:52:55","title":"A Multi-Embedding Convergence Network on Siamese Architecture for Fake Reviews","abstract":"In this new digital era, accessibility to real-world events is moving towards web-based modules. This is mostly visible on e-commerce websites where there is limited availability of physical verification. With this unforeseen development, we depend on the verification in the virtual world to influence our decisions. One of the decision making process is deeply based on review reading. Reviews play an important part in this transactional process. And seeking a real review can be very tenuous work for the user. On the other hand, fake review heavily impacts these transaction records of a product. The article presents an implementation of a Siamese network for detecting fake reviews. The fake reviews dataset, consisting of 40K reviews, preprocessed with different techniques. The cleaned data is passed through embeddings generated by MiniLM BERT for contextual relationship and Word2Vec for semantic relationship to form vectors. Further, the embeddings are trained in a Siamese network with LSTM layers connected to fuzzy logic for decision-making. The results show that fake reviews can be detected with high accuracy on a siamese network for prediction and verification.","sentences":["In this new digital era, accessibility to real-world events is moving towards web-based modules.","This is mostly visible on e-commerce websites where there is limited availability of physical verification.","With this unforeseen development, we depend on the verification in the virtual world to influence our decisions.","One of the decision making process is deeply based on review reading.","Reviews play an important part in this transactional process.","And seeking a real review can be very tenuous work for the user.","On the other hand, fake review heavily impacts these transaction records of a product.","The article presents an implementation of a Siamese network for detecting fake reviews.","The fake reviews dataset, consisting of 40K reviews, preprocessed with different techniques.","The cleaned data is passed through embeddings generated by MiniLM BERT for contextual relationship and Word2Vec for semantic relationship to form vectors.","Further, the embeddings are trained in a Siamese network with LSTM layers connected to fuzzy logic for decision-making.","The results show that fake reviews can be detected with high accuracy on a siamese network for prediction and verification."],"url":"http://arxiv.org/abs/2401.05995v1"}
{"created":"2024-01-11 15:52:20","title":"MGARD: A multigrid framework for high-performance, error-controlled data compression and refactoring","abstract":"We describe MGARD, a software providing MultiGrid Adaptive Reduction for floating-point scientific data on structured and unstructured grids. With exceptional data compression capability and precise error control, MGARD addresses a wide range of requirements, including storage reduction, high-performance I/O, and in-situ data analysis. It features a unified application programming interface (API) that seamlessly operates across diverse computing architectures. MGARD has been optimized with highly-tuned GPU kernels and efficient memory and device management mechanisms, ensuring scalable and rapid operations.","sentences":["We describe MGARD, a software providing MultiGrid Adaptive Reduction for floating-point scientific data on structured and unstructured grids.","With exceptional data compression capability and precise error control, MGARD addresses a wide range of requirements, including storage reduction, high-performance I/O, and in-situ data analysis.","It features a unified application programming interface (API) that seamlessly operates across diverse computing architectures.","MGARD has been optimized with highly-tuned GPU kernels and efficient memory and device management mechanisms, ensuring scalable and rapid operations."],"url":"http://arxiv.org/abs/2401.05994v1"}
{"created":"2024-01-11 15:42:52","title":"Reconstruction as a service: a data space for off-site image reconstruction in magnetic particle imaging","abstract":"Magnetic particle imaging (MPI) is an emerging medical imaging modality which offers a unique combination of high temporal and spatial resolution, sensitivity and biocompatibility. For system-matrix (SM) based image reconstruction in MPI, a huge amount of calibration data needs to be acquired prior to reconstruction in a time-consuming procedure. Conventionally, the data is recorded on-site inside the scanning device, which significantly limits the time that the scanning device is available for patient care in a clinical setting. Due to its size, handling the calibration data can be challenging. To solve these issues of recording and handling the data, data spaces could be used, as it has been shown that the calibration data can be measured in dedicated devices off-site. We propose a data space aimed at improving the efficiency of SM-based image reconstruction in MPI. The data space consists of imaging facilities, calibration data providers and reconstruction experts. Its specifications follow the reference architecture model of international data spaces (IDS). Use-cases of image reconstruction in MPI are formulated. The stakeholders and tasks are listed and mapped to the terminology of IDS. The signal chain in MPI is analysed to identify a minimum information model which is used by the data space.","sentences":["Magnetic particle imaging (MPI) is an emerging medical imaging modality which offers a unique combination of high temporal and spatial resolution, sensitivity and biocompatibility.","For system-matrix (SM) based image reconstruction in MPI, a huge amount of calibration data needs to be acquired prior to reconstruction in a time-consuming procedure.","Conventionally, the data is recorded on-site inside the scanning device, which significantly limits the time that the scanning device is available for patient care in a clinical setting.","Due to its size, handling the calibration data can be challenging.","To solve these issues of recording and handling the data, data spaces could be used, as it has been shown that the calibration data can be measured in dedicated devices off-site.","We propose a data space aimed at improving the efficiency of SM-based image reconstruction in MPI.","The data space consists of imaging facilities, calibration data providers and reconstruction experts.","Its specifications follow the reference architecture model of international data spaces (IDS).","Use-cases of image reconstruction in MPI are formulated.","The stakeholders and tasks are listed and mapped to the terminology of IDS.","The signal chain in MPI is analysed to identify a minimum information model which is used by the data space."],"url":"http://arxiv.org/abs/2401.05987v1"}
{"created":"2024-01-11 15:41:21","title":"LogPTR: Variable-Aware Log Parsing with Pointer Network","abstract":"Due to the sheer size of software logs, developers rely on automated log analysis. Log parsing, which parses semi-structured logs into a structured format, is a prerequisite of automated log analysis. However, existing log parsers are unsatisfactory when applied in practice because: 1) they ignore categories of variables, and 2) have poor generalization ability. To address the limitations of existing approaches, we propose LogPTR, the first end-to-end variable-aware log parser that can extract the static and dynamic parts in logs, and further identify the categories of variables. The key of LogPTR is using pointer network to copy words from the log message. We have performed extensive experiments on 16 public log datasets and the results show that LogPTR outperforms state-of-the-art log parsers both on general log parsing that extracts the log template and variable-aware log parsing that further identifies the category of variables.","sentences":["Due to the sheer size of software logs, developers rely on automated log analysis.","Log parsing, which parses semi-structured logs into a structured format, is a prerequisite of automated log analysis.","However, existing log parsers are unsatisfactory when applied in practice because: 1) they ignore categories of variables, and 2) have poor generalization ability.","To address the limitations of existing approaches, we propose LogPTR, the first end-to-end variable-aware log parser that can extract the static and dynamic parts in logs, and further identify the categories of variables.","The key of LogPTR is using pointer network to copy words from the log message.","We have performed extensive experiments on 16 public log datasets and the results show that LogPTR outperforms state-of-the-art log parsers both on general log parsing that extracts the log template and variable-aware log parsing that further identifies the category of variables."],"url":"http://arxiv.org/abs/2401.05986v1"}
{"created":"2024-01-11 15:38:31","title":"HybridOctree_Hex: Hybrid Octree-Based Adaptive All-Hexahedral Mesh Generation with Jacobian Control","abstract":"We present a new software package \"HybridOctree_Hex\" for adaptive all-hexahedral mesh generation based on hybrid octree and quality improvement with Jacobian control. The proposed HybridOctree_Hex begins by detecting curvatures and narrow regions of the input boundary to identify key surface features and initialize an octree structure. Subsequently, a strongly balanced octree is constructed using the balancing and pairing rules. Inspired by our earlier preliminary hybrid octree-based work, templates are designed to guarantee an all-hexahedral dual mesh generation directly from the strongly balanced octree. With these pre-defined templates, the sophisticated hybrid octree construction step is skipped to achieve an efficient implementation. After that, elements outside and around the boundary are removed to create a core mesh. The boundary points of the core mesh are connected to their corresponding closest points on the surface to fill the buffer zone and build the final mesh. Coupled with smart Laplacian smoothing, HybridOctree_Hex takes advantage of a delicate optimization-based quality improvement method considering geometric fitting, Jacobian and scaled Jacobian to achieve the minimum scaled Jacobian higher than $0.5$. We empirically verify the robustness and efficiency of our method by running the HybridOctree_Hex software on dozens of complex 3D models without any manual intervention or parameter adjustment. We provide the HybridOctree_Hex source codes, comprehensive results, encompassing mesh input/output files and statistical data presented at https://github.com/CMU-CBML/HybridOctree_Hex.","sentences":["We present a new software package \"HybridOctree_Hex\" for adaptive all-hexahedral mesh generation based on hybrid octree and quality improvement with Jacobian control.","The proposed HybridOctree_Hex begins by detecting curvatures and narrow regions of the input boundary to identify key surface features and initialize an octree structure.","Subsequently, a strongly balanced octree is constructed using the balancing and pairing rules.","Inspired by our earlier preliminary hybrid octree-based work, templates are designed to guarantee an all-hexahedral dual mesh generation directly from the strongly balanced octree.","With these pre-defined templates, the sophisticated hybrid octree construction step is skipped to achieve an efficient implementation.","After that, elements outside and around the boundary are removed to create a core mesh.","The boundary points of the core mesh are connected to their corresponding closest points on the surface to fill the buffer zone and build the final mesh.","Coupled with smart Laplacian smoothing, HybridOctree_Hex takes advantage of a delicate optimization-based quality improvement method considering geometric fitting, Jacobian and scaled Jacobian to achieve the minimum scaled Jacobian higher than $0.5$. We empirically verify the robustness and efficiency of our method by running the HybridOctree_Hex software on dozens of complex 3D models without any manual intervention or parameter adjustment.","We provide the HybridOctree_Hex source codes, comprehensive results, encompassing mesh input/output files and statistical data presented at https://github.com/CMU-CBML/HybridOctree_Hex."],"url":"http://arxiv.org/abs/2401.05984v1"}
{"created":"2024-01-11 15:22:55","title":"End-to-end Learnable Clustering for Intent Learning in Recommendation","abstract":"Mining users' intents plays a crucial role in sequential recommendation. The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering. While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues. Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance. Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data. To address these challenges, we propose a novel intent learning method called \\underline{ELCRec}, which integrates representation learning into an \\underline{E}nd-to-end \\underline{L}earnable \\underline{C}lustering framework for \\underline{Rec}ommendation. Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameters. Additionally, we design a clustering loss that guides the networks to differentiate between different cluster centers and pull similar samples towards their respective cluster centers. This allows simultaneous optimization of recommendation and clustering using mini-batch data. Moreover, we leverage the learned cluster centers as self-supervision signals for representation learning, resulting in further enhancement of recommendation performance. Extensive experiments conducted on open benchmarks and industry data validate the superiority, effectiveness, and efficiency of our proposed ELCRec method. Code is available at: https://github.com/yueliu1999/ELCRec.","sentences":["Mining users' intents plays a crucial role in sequential recommendation.","The recent approach, ICLRec, was introduced to extract underlying users' intents using contrastive learning and clustering.","While it has shown effectiveness, the existing method suffers from complex and cumbersome alternating optimization, leading to two main issues.","Firstly, the separation of representation learning and clustering optimization within a generalized expectation maximization (EM) framework often results in sub-optimal performance.","Secondly, performing clustering on the entire dataset hampers scalability for large-scale industry data.","To address these challenges, we propose a novel intent learning method called \\underline{ELCRec}, which integrates representation learning into an \\underline{E}nd-to-end \\underline{L}earnable \\underline{C}lustering framework for \\underline{Rec}ommendation.","Specifically, we encode users' behavior sequences and initialize the cluster centers as learnable network parameters.","Additionally, we design a clustering loss that guides the networks to differentiate between different cluster centers and pull similar samples towards their respective cluster centers.","This allows simultaneous optimization of recommendation and clustering using mini-batch data.","Moreover, we leverage the learned cluster centers as self-supervision signals for representation learning, resulting in further enhancement of recommendation performance.","Extensive experiments conducted on open benchmarks and industry data validate the superiority, effectiveness, and efficiency of our proposed ELCRec method.","Code is available at: https://github.com/yueliu1999/ELCRec."],"url":"http://arxiv.org/abs/2401.05975v1"}
{"created":"2024-01-11 15:19:21","title":"UAVD4L: A Large-Scale Dataset for UAV 6-DoF Localization","abstract":"Despite significant progress in global localization of Unmanned Aerial Vehicles (UAVs) in GPS-denied environments, existing methods remain constrained by the availability of datasets. Current datasets often focus on small-scale scenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV build-in sensor data. To address these limitations, we introduce a large-scale 6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF localization pipeline (UAVLoc), which consists of offline synthetic data generation and online visual localization. Additionally, based on the 6-DoF estimator, we design a hierarchical system for tracking ground target in 3D space. Experimental results on the new dataset demonstrate the effectiveness of the proposed approach. Code and dataset are available at https://github.com/RingoWRW/UAVD4L","sentences":["Despite significant progress in global localization of Unmanned Aerial Vehicles (UAVs) in GPS-denied environments, existing methods remain constrained by the availability of datasets.","Current datasets often focus on small-scale scenes and lack viewpoint variability, accurate ground truth (GT) pose, and UAV build-in sensor data.","To address these limitations, we introduce a large-scale 6-DoF UAV dataset for localization (UAVD4L) and develop a two-stage 6-DoF localization pipeline (UAVLoc), which consists of offline synthetic data generation and online visual localization.","Additionally, based on the 6-DoF estimator, we design a hierarchical system for tracking ground target in 3D space.","Experimental results on the new dataset demonstrate the effectiveness of the proposed approach.","Code and dataset are available at https://github.com/RingoWRW/UAVD4L"],"url":"http://arxiv.org/abs/2401.05971v1"}
{"created":"2024-01-11 15:16:20","title":"Spatial-Aware Deep Reinforcement Learning for the Traveling Officer Problem","abstract":"The traveling officer problem (TOP) is a challenging stochastic optimization task. In this problem, a parking officer is guided through a city equipped with parking sensors to fine as many parking offenders as possible. A major challenge in TOP is the dynamic nature of parking offenses, which randomly appear and disappear after some time, regardless of whether they have been fined. Thus, solutions need to dynamically adjust to currently fineable parking offenses while also planning ahead to increase the likelihood that the officer arrives during the offense taking place. Though various solutions exist, these methods often struggle to take the implications of actions on the ability to fine future parking violations into account. This paper proposes SATOP, a novel spatial-aware deep reinforcement learning approach for TOP. Our novel state encoder creates a representation of each action, leveraging the spatial relationships between parking spots, the agent, and the action. Furthermore, we propose a novel message-passing module for learning future inter-action correlations in the given environment. Thus, the agent can estimate the potential to fine further parking violations after executing an action. We evaluate our method using an environment based on real-world data from Melbourne. Our results show that SATOP consistently outperforms state-of-the-art TOP agents and is able to fine up to 22% more parking offenses.","sentences":["The traveling officer problem (TOP) is a challenging stochastic optimization task.","In this problem, a parking officer is guided through a city equipped with parking sensors to fine as many parking offenders as possible.","A major challenge in TOP is the dynamic nature of parking offenses, which randomly appear and disappear after some time, regardless of whether they have been fined.","Thus, solutions need to dynamically adjust to currently fineable parking offenses while also planning ahead to increase the likelihood that the officer arrives during the offense taking place.","Though various solutions exist, these methods often struggle to take the implications of actions on the ability to fine future parking violations into account.","This paper proposes SATOP, a novel spatial-aware deep reinforcement learning approach for TOP.","Our novel state encoder creates a representation of each action, leveraging the spatial relationships between parking spots, the agent, and the action.","Furthermore, we propose a novel message-passing module for learning future inter-action correlations in the given environment.","Thus, the agent can estimate the potential to fine further parking violations after executing an action.","We evaluate our method using an environment based on real-world data from Melbourne.","Our results show that SATOP consistently outperforms state-of-the-art TOP agents and is able to fine up to 22% more parking offenses."],"url":"http://arxiv.org/abs/2401.05969v1"}
{"created":"2024-01-11 15:13:31","title":"A Lightweight Feature Fusion Architecture For Resource-Constrained Crowd Counting","abstract":"Crowd counting finds direct applications in real-world situations, making computational efficiency and performance crucial. However, most of the previous methods rely on a heavy backbone and a complex downstream architecture that restricts the deployment. To address this challenge and enhance the versatility of crowd-counting models, we introduce two lightweight models. These models maintain the same downstream architecture while incorporating two distinct backbones: MobileNet and MobileViT. We leverage Adjacent Feature Fusion to extract diverse scale features from a Pre-Trained Model (PTM) and subsequently combine these features seamlessly. This approach empowers our models to achieve improved performance while maintaining a compact and efficient design. With the comparison of our proposed models with previously available state-of-the-art (SOTA) methods on ShanghaiTech-A ShanghaiTech-B and UCF-CC-50 dataset, it achieves comparable results while being the most computationally efficient model. Finally, we present a comparative study, an extensive ablation study, along with pruning to show the effectiveness of our models.","sentences":["Crowd counting finds direct applications in real-world situations, making computational efficiency and performance crucial.","However, most of the previous methods rely on a heavy backbone and a complex downstream architecture that restricts the deployment.","To address this challenge and enhance the versatility of crowd-counting models, we introduce two lightweight models.","These models maintain the same downstream architecture while incorporating two distinct backbones: MobileNet and MobileViT. We leverage Adjacent Feature Fusion to extract diverse scale features from a Pre-Trained Model (PTM) and subsequently combine these features seamlessly.","This approach empowers our models to achieve improved performance while maintaining a compact and efficient design.","With the comparison of our proposed models with previously available state-of-the-art (SOTA) methods on ShanghaiTech-A ShanghaiTech-B and UCF-CC-50 dataset, it achieves comparable results while being the most computationally efficient model.","Finally, we present a comparative study, an extensive ablation study, along with pruning to show the effectiveness of our models."],"url":"http://arxiv.org/abs/2401.05968v1"}
{"created":"2024-01-11 15:13:00","title":"Block-Diagonal Orthogonal Relation and Matrix Entity for Knowledge Graph Embedding","abstract":"The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts. While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations. To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations. This approach enhances the generality and flexibility of KGE models. The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters.","sentences":["The primary aim of Knowledge Graph embeddings (KGE) is to learn low-dimensional representations of entities and relations for predicting missing facts.","While rotation-based methods like RotatE and QuatE perform well in KGE, they face two challenges: limited model flexibility requiring proportional increases in relation size with entity dimension, and difficulties in generalizing the model for higher-dimensional rotations.","To address these issues, we introduce OrthogonalE, a novel KGE model employing matrices for entities and block-diagonal orthogonal matrices with Riemannian optimization for relations.","This approach enhances the generality and flexibility of KGE models.","The experimental results indicate that our new KGE model, OrthogonalE, is both general and flexible, significantly outperforming state-of-the-art KGE models while substantially reducing the number of relation parameters."],"url":"http://arxiv.org/abs/2401.05967v1"}
{"created":"2024-01-11 15:08:15","title":"HAP: SPMD DNN Training on Heterogeneous GPU Clusters with Automated Program Synthesis","abstract":"Single-Program-Multiple-Data (SPMD) parallelism has recently been adopted to train large deep neural networks (DNNs). Few studies have explored its applicability on heterogeneous clusters, to fully exploit available resources for large model learning. This paper presents \\OurSystem, an automated system designed to expedite SPMD DNN training on heterogeneous clusters. \\OurSystem jointly optimizes the tensor sharding strategy, sharding ratios across heterogeneous devices and the communication methods for tensor exchanges for optimized distributed training with SPMD parallelism. We novelly formulate model partitioning as a program synthesis problem, in which we generate a distributed program from scratch on a distributed instruction set that semantically resembles the program designed for a single device, and systematically explore the solution space with an A*-based search algorithm. We derive the optimal tensor sharding ratios by formulating it as a linear programming problem. Additionally, \\OurSystem explores tensor communication optimization in a heterogeneous cluster and integrates it as part of the program synthesis process, for automatically choosing optimal collective communication primitives and applying sufficient factor broadcasting technique. Extensive experiments on representative workloads demonstrate that \\OurSystem achieves up to 2.41x speed-up on heterogeneous clusters.","sentences":["Single-Program-Multiple-Data (SPMD) parallelism has recently been adopted to train large deep neural networks (DNNs).","Few studies have explored its applicability on heterogeneous clusters, to fully exploit available resources for large model learning.","This paper presents \\OurSystem, an automated system designed to expedite SPMD DNN training on heterogeneous clusters.","\\OurSystem jointly optimizes the tensor sharding strategy, sharding ratios across heterogeneous devices and the communication methods for tensor exchanges for optimized distributed training with SPMD parallelism.","We novelly formulate model partitioning as a program synthesis problem, in which we generate a distributed program from scratch on a distributed instruction set that semantically resembles the program designed for a single device, and systematically explore the solution space with an A*-based search algorithm.","We derive the optimal tensor sharding ratios by formulating it as a linear programming problem.","Additionally, \\OurSystem explores tensor communication optimization in a heterogeneous cluster and integrates it as part of the program synthesis process, for automatically choosing optimal collective communication primitives and applying sufficient factor broadcasting technique.","Extensive experiments on representative workloads demonstrate that \\OurSystem achieves up to 2.41x speed-up on heterogeneous clusters."],"url":"http://arxiv.org/abs/2401.05965v1"}
{"created":"2024-01-11 15:06:25","title":"An attempt to generate new bridge types from latent space of PixelCNN","abstract":"Try to generate new bridge types using generative artificial intelligence technology. Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained. The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given. From the obtained latent space sampling, new bridge types different from the training dataset can be generated. PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability. Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence. Multimodal models should be the way to achieve artificial general intelligence in the future.","sentences":["Try to generate new bridge types using generative artificial intelligence technology.","Using symmetric structured image dataset of three-span beam bridge, arch bridge, cable-stayed bridge and suspension bridge , based on Python programming language, TensorFlow and Keras deep learning platform framework , PixelCNN is constructed and trained.","The model can capture the statistical structure of the images and calculate the probability distribution of the next pixel when the previous pixels are given.","From the obtained latent space sampling, new bridge types different from the training dataset can be generated.","PixelCNN can organically combine different structural components on the basis of human original bridge types, creating new bridge types that have a certain degree of human original ability.","Autoregressive models cannot understand the meaning of the sequence, while multimodal models combine regression and autoregressive models to understand the sequence.","Multimodal models should be the way to achieve artificial general intelligence in the future."],"url":"http://arxiv.org/abs/2401.05964v1"}
{"created":"2024-01-11 15:02:31","title":"Securing an Application Layer Gateway: An Industrial Case Study","abstract":"Application Layer Gateways (ALGs) play a crucial role in securing critical systems, including railways, industrial automation, and defense applications, by segmenting networks at different levels of criticality. However, they require rigorous security testing to prevent software vulnerabilities, not only at the network level but also at the application layer (e.g., deep traffic inspection components). This paper presents a vulnerability-driven methodology for the comprehensive security testing of ALGs. We present the methodology in the context of an industrial case study in the railways domain, and a simulation-based testing environment to support the methodology.","sentences":["Application Layer Gateways (ALGs) play a crucial role in securing critical systems, including railways, industrial automation, and defense applications, by segmenting networks at different levels of criticality.","However, they require rigorous security testing to prevent software vulnerabilities, not only at the network level but also at the application layer (e.g., deep traffic inspection components).","This paper presents a vulnerability-driven methodology for the comprehensive security testing of ALGs.","We present the methodology in the context of an industrial case study in the railways domain, and a simulation-based testing environment to support the methodology."],"url":"http://arxiv.org/abs/2401.05961v1"}
{"created":"2024-01-11 15:02:15","title":"Machine Learning Insides OptVerse AI Solver: Design Principles and Applications","abstract":"In an era of digital ubiquity, efficient resource management and decision-making are paramount across numerous industries. To this end, we present a comprehensive study on the integration of machine learning (ML) techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the scarcity of real-world mathematical programming instances, and to surpass the capabilities of traditional optimization techniques. We showcase our methods for generating complex SAT and MILP instances utilizing generative models that mirror multifaceted structures of real-world problem. Furthermore, we introduce a training framework leveraging augmentation policies to maintain solvers' utility in dynamic environments. Besides the data generation and augmentation, our proposed approaches also include novel ML-driven policies for personalized solver strategies, with an emphasis on applications like graph convolutional networks for initial basis selection and reinforcement learning for advanced presolving and cut selection. Additionally, we detail the incorporation of state-of-the-art parameter tuning algorithms which markedly elevate solver performance. Compared with traditional solvers such as Gurobi and SCIP, our ML-augmented OptVerse AI Solver demonstrates superior speed and precision across both established benchmarks and real-world scenarios, reinforcing the practical imperative and effectiveness of machine learning techniques in mathematical programming solvers.","sentences":["In an era of digital ubiquity, efficient resource management and decision-making are paramount across numerous industries.","To this end, we present a comprehensive study on the integration of machine learning (ML) techniques into Huawei Cloud's OptVerse AI Solver, which aims to mitigate the scarcity of real-world mathematical programming instances, and to surpass the capabilities of traditional optimization techniques.","We showcase our methods for generating complex SAT and MILP instances utilizing generative models that mirror multifaceted structures of real-world problem.","Furthermore, we introduce a training framework leveraging augmentation policies to maintain solvers' utility in dynamic environments.","Besides the data generation and augmentation, our proposed approaches also include novel ML-driven policies for personalized solver strategies, with an emphasis on applications like graph convolutional networks for initial basis selection and reinforcement learning for advanced presolving and cut selection.","Additionally, we detail the incorporation of state-of-the-art parameter tuning algorithms which markedly elevate solver performance.","Compared with traditional solvers such as Gurobi and SCIP, our ML-augmented OptVerse AI Solver demonstrates superior speed and precision across both established benchmarks and real-world scenarios, reinforcing the practical imperative and effectiveness of machine learning techniques in mathematical programming solvers."],"url":"http://arxiv.org/abs/2401.05960v1"}
{"created":"2024-01-11 14:53:26","title":"A k-swap Local Search for Makespan Scheduling","abstract":"Local search is a widely used technique for tackling challenging optimization problems, offering significant advantages in terms of computational efficiency and exhibiting strong empirical behavior across a wide range of problem domains. In this paper, we address a scheduling problem on two identical parallel machines with the objective of \\emph{makespan minimization}. For this problem, we consider a local search neighborhood, called \\emph{$k$-swap}, which is a more generalized version of the widely-used \\emph{swap} and \\emph{jump} neighborhoods. The $k$-swap neighborhood is obtained by swapping at most $k$ jobs between two machines in our schedule. First, we propose an algorithm for finding an improving neighbor in the $k$-swap neighborhood which is faster than the naive approach, and prove an almost matching lower bound on any such an algorithm. Then, we analyze the number of local search steps required to converge to a local optimum with respect to the $k$-swap neighborhood. For the case $k = 2$ (similar to the swap neighborhood), we provide a polynomial upper bound on the number of local search steps, and for the case $k = 3$, we provide an exponential lower bound. Finally, we conduct computational experiments on various families of instances, and we discuss extensions to more than two machines in our schedule.","sentences":["Local search is a widely used technique for tackling challenging optimization problems, offering significant advantages in terms of computational efficiency and exhibiting strong empirical behavior across a wide range of problem domains.","In this paper, we address a scheduling problem on two identical parallel machines with the objective of \\emph{makespan minimization}.","For this problem, we consider a local search neighborhood, called \\emph{$k$-swap}, which is a more generalized version of the widely-used \\emph{swap} and \\emph{jump} neighborhoods.","The $k$-swap neighborhood is obtained by swapping at most $k$ jobs between two machines in our schedule.","First, we propose an algorithm for finding an improving neighbor in the $k$-swap neighborhood which is faster than the naive approach, and prove an almost matching lower bound on any such an algorithm.","Then, we analyze the number of local search steps required to converge to a local optimum with respect to the $k$-swap neighborhood.","For the case $k = 2$ (similar to the swap neighborhood), we provide a polynomial upper bound on the number of local search steps, and for the case $k = 3$, we provide an exponential lower bound.","Finally, we conduct computational experiments on various families of instances, and we discuss extensions to more than two machines in our schedule."],"url":"http://arxiv.org/abs/2401.05956v1"}
{"created":"2024-01-11 14:44:08","title":"LLM-as-a-Coauthor: The Challenges of Detecting LLM-Human Mixcase","abstract":"With the remarkable development and widespread applications of large language models (LLMs), the use of machine-generated text (MGT) is becoming increasingly common. This trend brings potential risks, particularly to the quality and completeness of information in fields such as news and education. Current research predominantly addresses the detection of pure MGT without adequately addressing mixed scenarios including AI-revised Human-Written Text (HWT) or human-revised MGT. To confront this challenge, we introduce mixcase, a novel concept representing a hybrid text form involving both machine-generated and human-generated content. We collected mixcase instances generated from multiple daily text-editing scenarios and composed MixSet, the first dataset dedicated to studying these mixed modification scenarios. We conduct experiments to evaluate the efficacy of popular MGT detectors, assessing their effectiveness, robustness, and generalization performance. Our findings reveal that existing detectors struggle to identify mixcase as a separate class or MGT, particularly in dealing with subtle modifications and style adaptability. This research underscores the urgent need for more fine-grain detectors tailored for mixcase, offering valuable insights for future research. Code and Models are available at https://github.com/Dongping-Chen/MixSet.","sentences":["With the remarkable development and widespread applications of large language models (LLMs), the use of machine-generated text (MGT) is becoming increasingly common.","This trend brings potential risks, particularly to the quality and completeness of information in fields such as news and education.","Current research predominantly addresses the detection of pure MGT without adequately addressing mixed scenarios including AI-revised Human-Written Text (HWT) or human-revised MGT.","To confront this challenge, we introduce mixcase, a novel concept representing a hybrid text form involving both machine-generated and human-generated content.","We collected mixcase instances generated from multiple daily text-editing scenarios and composed MixSet, the first dataset dedicated to studying these mixed modification scenarios.","We conduct experiments to evaluate the efficacy of popular MGT detectors, assessing their effectiveness, robustness, and generalization performance.","Our findings reveal that existing detectors struggle to identify mixcase as a separate class or MGT, particularly in dealing with subtle modifications and style adaptability.","This research underscores the urgent need for more fine-grain detectors tailored for mixcase, offering valuable insights for future research.","Code and Models are available at https://github.com/Dongping-Chen/MixSet."],"url":"http://arxiv.org/abs/2401.05952v1"}
{"created":"2024-01-11 14:38:19","title":"Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks","abstract":"In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings. Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters. Despite being widely applied, in-context learning is vulnerable to malicious attacks. In this work, we raise security concerns regarding this paradigm. Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model. Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning. Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions. ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model's generality. Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method. Extensive experimental results across several language models, ranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models. Our findings highlight the vulnerabilities of language models, and we hope this work will raise awareness of the possible security threats associated with in-context learning.","sentences":["In-context learning, a paradigm bridging the gap between pre-training and fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in few-shot settings.","Unlike traditional fine-tuning methods, in-context learning adapts pre-trained models to unseen tasks without updating any parameters.","Despite being widely applied, in-context learning is vulnerable to malicious attacks.","In this work, we raise security concerns regarding this paradigm.","Our studies demonstrate that an attacker can manipulate the behavior of large language models by poisoning the demonstration context, without the need for fine-tuning the model.","Specifically, we have designed a new backdoor attack method, named ICLAttack, to target large language models based on in-context learning.","Our method encompasses two types of attacks: poisoning demonstration examples and poisoning prompts, which can make models behave in accordance with predefined intentions.","ICLAttack does not require additional fine-tuning to implant a backdoor, thus preserving the model's generality.","Furthermore, the poisoned examples are correctly labeled, enhancing the natural stealth of our attack method.","Extensive experimental results across several language models, ranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of our attack method, exemplified by a high average attack success rate of 95.0% across the three datasets on OPT models.","Our findings highlight the vulnerabilities of language models, and we hope this work will raise awareness of the possible security threats associated with in-context learning."],"url":"http://arxiv.org/abs/2401.05949v1"}
{"created":"2024-01-11 14:34:56","title":"Blockchain-based Decentralized Time Lock Machines: Automated Reveal of Time-sensitive Information","abstract":"Conditional Information Reveal (CIR) automates the release of information upon meeting specific pre-defined conditions, such as time or location. This paper advances the understanding and implementation of CIR by introducing a new paradigm to highlight the security challenges in CIR design, and proposes a decentralized architecture as a design guideline for secure CIR systems. Furthermore, in the context of time-sensitive data sharing, this paper proposes a practical timed-release cryptography system employing the proposed architecture and a novel verifiable secret sharing scheme. Key achievements of this study include the creation of an open-source prototype for practical deployment and a comprehensive system evaluation that highlights the enhanced security and efficiency of the proposed system. Furthermore, the paper delves into the application of this system in E-voting scenarios, illustrating its capacity to secure and ensure fair electronic voting processes.","sentences":["Conditional Information Reveal (CIR) automates the release of information upon meeting specific pre-defined conditions, such as time or location.","This paper advances the understanding and implementation of CIR by introducing a new paradigm to highlight the security challenges in CIR design, and proposes a decentralized architecture as a design guideline for secure CIR systems.","Furthermore, in the context of time-sensitive data sharing, this paper proposes a practical timed-release cryptography system employing the proposed architecture and a novel verifiable secret sharing scheme.","Key achievements of this study include the creation of an open-source prototype for practical deployment and a comprehensive system evaluation that highlights the enhanced security and efficiency of the proposed system.","Furthermore, the paper delves into the application of this system in E-voting scenarios, illustrating its capacity to secure and ensure fair electronic voting processes."],"url":"http://arxiv.org/abs/2401.05947v1"}
{"created":"2024-01-11 14:30:30","title":"Learning Cognitive Maps from Transformer Representations for Efficient Planning in Partially Observed Environments","abstract":"Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains the near perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest path problems exponentially faster. Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context path planning problems.","sentences":["Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation.","In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard.","We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions.","After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices.","These maps are then paired with an external solver to solve (constrained) path planning problems.","First, we show that a TDB trained on POEs (a) retains the near perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest path problems exponentially faster.","Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models.","Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context path planning problems."],"url":"http://arxiv.org/abs/2401.05946v1"}
{"created":"2024-01-11 14:28:13","title":"SoK: Analysis techniques for WebAssembly","abstract":"WebAssembly is a low-level bytecode language that allows high-level languages like C, C++, and Rust to be executed in the browser at near-native performance. In recent years, WebAssembly has gained widespread adoption is now natively supported by all modern browsers. However, vulnerabilities in memory-unsafe languages, like C and C++, can translate into vulnerabilities in WebAssembly binaries. Unfortunately, most WebAssembly binaries are compiled from such memory-unsafe languages, and these vulnerabilities have been shown to be practical in real-world scenarios. WebAssembly smart contracts have also been found to be vulnerable, causing significant financial loss. Additionally, WebAssembly has been used for malicious purposes like cryptojacking. To address these issues, several analysis techniques for WebAssembly binaries have been proposed. In this paper, we conduct a comprehensive literature review of these techniques and categorize them based on their analysis strategy and objectives. Furthermore, we compare and evaluate the techniques using quantitative data, highlighting their strengths and weaknesses. In addition, one of the main contributions of this paper is the identification of future research directions based on the thorough literature review conducted.","sentences":["WebAssembly is a low-level bytecode language that allows high-level languages like C, C++, and Rust to be executed in the browser at near-native performance.","In recent years, WebAssembly has gained widespread adoption is now natively supported by all modern browsers.","However, vulnerabilities in memory-unsafe languages, like C and C++, can translate into vulnerabilities in WebAssembly binaries.","Unfortunately, most WebAssembly binaries are compiled from such memory-unsafe languages, and these vulnerabilities have been shown to be practical in real-world scenarios.","WebAssembly smart contracts have also been found to be vulnerable, causing significant financial loss.","Additionally, WebAssembly has been used for malicious purposes like cryptojacking.","To address these issues, several analysis techniques for WebAssembly binaries have been proposed.","In this paper, we conduct a comprehensive literature review of these techniques and categorize them based on their analysis strategy and objectives.","Furthermore, we compare and evaluate the techniques using quantitative data, highlighting their strengths and weaknesses.","In addition, one of the main contributions of this paper is the identification of future research directions based on the thorough literature review conducted."],"url":"http://arxiv.org/abs/2401.05943v1"}
{"created":"2024-01-11 14:27:43","title":"Mutation-based Consistency Testing for Evaluating the Code Understanding Capability of LLMs","abstract":"Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing. However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.   In this paper, we propose a novel method to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets. Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description. We apply different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs. We then use these pairs to test the ability of LLMs to correctly detect the inconsistencies.   We propose a new LLM testing method, called Mutation-based Consistency Testing (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of six programming languages (Python, C++, Java, Go, JavaScript, and Rust). We compare the performance of the LLMs across different types of code mutations and programming languages and analyze the results. We find that the LLMs show significant variation in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language.","sentences":["Large Language Models (LLMs) have shown remarkable capabilities in processing both natural and programming languages, which have enabled various applications in software engineering, such as requirement engineering, code generation, and software testing.","However, existing code generation benchmarks do not necessarily assess the code understanding performance of LLMs, especially for the subtle inconsistencies that may arise between code and its semantics described in natural language.   ","In this paper, we propose a novel method to systematically assess the code understanding performance of LLMs, particularly focusing on subtle differences between code and its descriptions, by introducing code mutations to existing code generation datasets.","Code mutations are small changes that alter the semantics of the original code, creating a mismatch with the natural language description.","We apply different types of code mutations, such as operator replacement and statement deletion, to generate inconsistent code-description pairs.","We then use these pairs to test the ability of LLMs to correctly detect the inconsistencies.   ","We propose a new LLM testing method, called Mutation-based Consistency Testing (MCT), and conduct a case study on the two popular LLMs, GPT-3.5 and GPT-4, using the state-of-the-art code generation benchmark, HumanEval-X, which consists of six programming languages (Python, C++, Java, Go, JavaScript, and Rust).","We compare the performance of the LLMs across different types of code mutations and programming languages and analyze the results.","We find that the LLMs show significant variation in their code understanding performance and that they have different strengths and weaknesses depending on the mutation type and language."],"url":"http://arxiv.org/abs/2401.05940v1"}
{"created":"2024-01-11 14:27:12","title":"DREQ: Document Re-Ranking Using Entity-based Query Understanding","abstract":"While entity-oriented neural IR models have advanced significantly, they often overlook a key nuance: the varying degrees of influence individual entities within a document have on its overall relevance. Addressing this gap, we present DREQ, an entity-oriented dense document re-ranking model. Uniquely, we emphasize the query-relevant entities within a document's representation while simultaneously attenuating the less relevant ones, thus obtaining a query-specific entity-centric document representation. We then combine this entity-centric document representation with the text-centric representation of the document to obtain a \"hybrid\" representation of the document. We learn a relevance score for the document using this hybrid representation. Using four large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural and non-neural re-ranking methods, highlighting the effectiveness of our entity-oriented representation approach.","sentences":["While entity-oriented neural IR models have advanced significantly, they often overlook a key nuance: the varying degrees of influence individual entities within a document have on its overall relevance.","Addressing this gap, we present DREQ, an entity-oriented dense document re-ranking model.","Uniquely, we emphasize the query-relevant entities within a document's representation while simultaneously attenuating the less relevant ones, thus obtaining a query-specific entity-centric document representation.","We then combine this entity-centric document representation with the text-centric representation of the document to obtain a \"hybrid\" representation of the document.","We learn a relevance score for the document using this hybrid representation.","Using four large-scale benchmarks, we show that DREQ outperforms state-of-the-art neural and non-neural re-ranking methods, highlighting the effectiveness of our entity-oriented representation approach."],"url":"http://arxiv.org/abs/2401.05939v1"}
{"created":"2024-01-11 14:11:30","title":"Time Series Forecasting of HIV/AIDS in the Philippines Using Deep Learning: Does COVID-19 Epidemic Matter?","abstract":"With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS epidemic in the Philippines is the one that is spreading the quickest in the western Pacific. Although the full effects of COVID-19 on HIV services and development are still unknown, it is predicted that such disruptions could lead to a significant increase in HIV casualties. Therefore, the nation needs some modeling and forecasting techniques to foresee the spread pattern and enhance the governments prevention, treatment, testing, and care program. In this study, the researcher uses Multilayer Perceptron Neural Network to forecast time series during the period when the COVID-19 pandemic strikes the nation, using statistics taken from the HIV/AIDS and ART Registry of the Philippines. After training, validation, and testing of data, the study finds that the predicted cumulative cases in the nation by 2030 will reach 145,273. Additionally, there is very little difference between observed and anticipated HIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as well as a greater coefficient of determination. Further research revealed that the Philippines seems far from achieving Sustainable Development Goal 3 of Project 2030 due to an increase in the nations rate of new HIV infections. Despite the detrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, the Philippine government, under the Marcos administration, must continue to adhere to the United Nations 90-90-90 targets by enhancing its ART program and ensuring that all vital health services are readily accessible and available.","sentences":["With a 676% growth rate in HIV incidence between 2010 and 2021, the HIV/AIDS epidemic in the Philippines is the one that is spreading the quickest in the western Pacific.","Although the full effects of COVID-19 on HIV services and development are still unknown, it is predicted that such disruptions could lead to a significant increase in HIV casualties.","Therefore, the nation needs some modeling and forecasting techniques to foresee the spread pattern and enhance the governments prevention, treatment, testing, and care program.","In this study, the researcher uses Multilayer Perceptron Neural Network to forecast time series during the period when the COVID-19 pandemic strikes the nation, using statistics taken from the HIV/AIDS and ART Registry of the Philippines.","After training, validation, and testing of data, the study finds that the predicted cumulative cases in the nation by 2030 will reach 145,273.","Additionally, there is very little difference between observed and anticipated HIV epidemic levels, as evidenced by reduced RMSE, MAE, and MAPE values as well as a greater coefficient of determination.","Further research revealed that the Philippines seems far from achieving Sustainable Development Goal 3 of Project 2030 due to an increase in the nations rate of new HIV infections.","Despite the detrimental effects of COVID-19 spread on HIV/AIDS efforts nationwide, the Philippine government, under the Marcos administration, must continue to adhere to the United Nations 90-90-90 targets by enhancing its ART program and ensuring that all vital health services are readily accessible and available."],"url":"http://arxiv.org/abs/2401.05933v1"}
{"created":"2024-01-11 14:11:12","title":"DiffDA: a diffusion model for weather-scale data assimilation","abstract":"The generation of initial conditions via accurate data assimilation is crucial for reliable weather forecasting and climate modeling. We propose the DiffDA as a machine learning based data assimilation method capable of assimilating atmospheric variables using predicted states and sparse observations. We adapt the pretrained GraphCast weather forecast model as a denoising diffusion model. Our method applies two-phase conditioning: on the predicted state during both training and inference, and on sparse observations during inference only. As a byproduct, this strategy also enables the post-processing of predictions into the future, for which no observations are available.Through experiments based on a reanalysis dataset, we have verified that our method can produce assimilated global atmospheric data consistent with observations at 0.25degree resolution. The experiments also show that the initial conditions that are generated via our approach can be used for forecast models with a loss of lead time of at most 24 hours when compared to initial conditions of state-of-the-art data assimilation suites. This enables to apply the method to real world applications such as the creation of reanalysis datasets with autoregressive data assimilation.","sentences":["The generation of initial conditions via accurate data assimilation is crucial for reliable weather forecasting and climate modeling.","We propose the DiffDA as a machine learning based data assimilation method capable of assimilating atmospheric variables using predicted states and sparse observations.","We adapt the pretrained GraphCast weather forecast model as a denoising diffusion model.","Our method applies two-phase conditioning: on the predicted state during both training and inference, and on sparse observations during inference only.","As a byproduct, this strategy also enables the post-processing of predictions into the future, for which no observations are available.","Through experiments based on a reanalysis dataset, we have verified that our method can produce assimilated global atmospheric data consistent with observations at 0.25degree resolution.","The experiments also show that the initial conditions that are generated via our approach can be used for forecast models with a loss of lead time of at most 24 hours when compared to initial conditions of state-of-the-art data assimilation suites.","This enables to apply the method to real world applications such as the creation of reanalysis datasets with autoregressive data assimilation."],"url":"http://arxiv.org/abs/2401.05932v1"}
{"created":"2024-01-11 14:09:09","title":"SH2: Self-Highlighted Hesitation Helps You Decode More Truthfully","abstract":"Large language models (LLMs) demonstrate great performance in text generation. However, LLMs are still suffering from hallucinations. In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully. SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others. Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives. Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation. During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation. Experimental results demonstrate that our SH2, requiring no additional data or models, can effectively help LLMs elicit factual knowledge and distinguish hallucinated contexts. Significant and consistent improvements are achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks.","sentences":["Large language models (LLMs) demonstrate great performance in text generation.","However, LLMs are still suffering from hallucinations.","In this work, we propose an inference-time method, Self-Highlighted Hesitation (SH2), to help LLMs decode more truthfully.","SH2 is based on a simple fact rooted in information theory that for an LLM, the tokens predicted with lower probabilities are prone to be more informative than others.","Our analysis shows that the tokens assigned with lower probabilities by an LLM are more likely to be closely related to factual information, such as nouns, proper nouns, and adjectives.","Therefore, we propose to ''highlight'' the factual information by selecting the tokens with the lowest probabilities and concatenating them to the original context, thus forcing the model to repeatedly read and hesitate on these tokens before generation.","During decoding, we also adopt contrastive decoding to emphasize the difference in the output probabilities brought by the hesitation.","Experimental results demonstrate that our SH2, requiring no additional data or models, can effectively help LLMs elicit factual knowledge and distinguish hallucinated contexts.","Significant and consistent improvements are achieved by SH2 for LLaMA-7b and LLaMA2-7b on multiple hallucination tasks."],"url":"http://arxiv.org/abs/2401.05930v1"}
{"created":"2024-01-11 14:07:47","title":"Mitigating Unhelpfulness in Emotional Support Conversations with Multifaceted AI Feedback","abstract":"An emotional support conversation system aims to alleviate users' emotional distress and assist them in addressing their challenges. To generate supportive responses, it is critical to consider multiple factors such as empathy, support strategies, and response coherence, as established in prior methods. Nonetheless, previous models occasionally generate unhelpful responses, which intend to provide support but display counterproductive effects. According to psychology and communication theories, poor performance in just one contributing factor might cause a response to be unhelpful. From the model training perspective, since these models have not been exposed to unhelpful responses during their training phase, they are unable to distinguish if the tokens they generate might result in unhelpful responses during inference. To address this issue, we introduce a novel model-agnostic framework named mitigating unhelpfulness with multifaceted AI feedback for emotional support (Muffin). Specifically, Muffin employs a multifaceted AI feedback module to assess the helpfulness of responses generated by a specific model with consideration of multiple factors. Using contrastive learning, it then reduces the likelihood of the model generating unhelpful responses compared to the helpful ones. Experimental results demonstrate that Muffin effectively mitigates the generation of unhelpful responses while slightly increasing response fluency and relevance.","sentences":["An emotional support conversation system aims to alleviate users' emotional distress and assist them in addressing their challenges.","To generate supportive responses, it is critical to consider multiple factors such as empathy, support strategies, and response coherence, as established in prior methods.","Nonetheless, previous models occasionally generate unhelpful responses, which intend to provide support but display counterproductive effects.","According to psychology and communication theories, poor performance in just one contributing factor might cause a response to be unhelpful.","From the model training perspective, since these models have not been exposed to unhelpful responses during their training phase, they are unable to distinguish if the tokens they generate might result in unhelpful responses during inference.","To address this issue, we introduce a novel model-agnostic framework named mitigating unhelpfulness with multifaceted AI feedback for emotional support (Muffin).","Specifically, Muffin employs a multifaceted AI feedback module to assess the helpfulness of responses generated by a specific model with consideration of multiple factors.","Using contrastive learning, it then reduces the likelihood of the model generating unhelpful responses compared to the helpful ones.","Experimental results demonstrate that Muffin effectively mitigates the generation of unhelpful responses while slightly increasing response fluency and relevance."],"url":"http://arxiv.org/abs/2401.05928v1"}
