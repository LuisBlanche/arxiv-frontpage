{"created":"2024-01-03 18:59:17","title":"Architectural Design for Secure Smart Contract Development","abstract":"As time progresses, the need for more secure applications grows exponentially. The different types of sensitive information that is being transferred virtually has sparked a rise in systems that leverage blockchain. Different sectors are beginning to use this disruptive technology to evaluate the risks and benefits. Sectors like finance, medicine, higher education, and wireless communication have research regarding blockchain. Futhermore, the need for security standards in this area of research is pivotal. In recent past, several attacks on blockchain infrastructures have resulted in hundreds of millions dollars lost and sensitive information compromised. Some of these attacks include DAO attacks, bZx attacks, and Parity Multisignature Wallet Double Attacks which targeted vulnerabilities within smart contracts on the Ethereum network. These attacks exposed the weaknesses of current smart contract development practices which has led to the increase in distrust and adoption of systems that leverage blockchain for its functionality. In this paper, I identify common software vulnerabilities and attacks on blockchain infrastructures, thoroughly detail the smart contract development process and propose a model for ensuring a stronger security standard for future systems leveraging smart contracts. The purpose for proposing a model is to promote trust among end users in the system which is a foundational element for blockchain adoption in the future.","sentences":["As time progresses, the need for more secure applications grows exponentially.","The different types of sensitive information that is being transferred virtually has sparked a rise in systems that leverage blockchain.","Different sectors are beginning to use this disruptive technology to evaluate the risks and benefits.","Sectors like finance, medicine, higher education, and wireless communication have research regarding blockchain.","Futhermore, the need for security standards in this area of research is pivotal.","In recent past, several attacks on blockchain infrastructures have resulted in hundreds of millions dollars lost and sensitive information compromised.","Some of these attacks include DAO attacks, bZx attacks, and Parity Multisignature Wallet Double Attacks which targeted vulnerabilities within smart contracts on the Ethereum network.","These attacks exposed the weaknesses of current smart contract development practices which has led to the increase in distrust and adoption of systems that leverage blockchain for its functionality.","In this paper, I identify common software vulnerabilities and attacks on blockchain infrastructures, thoroughly detail the smart contract development process and propose a model for ensuring a stronger security standard for future systems leveraging smart contracts.","The purpose for proposing a model is to promote trust among end users in the system which is a foundational element for blockchain adoption in the future."],"url":"http://arxiv.org/abs/2401.01891v1"}
{"created":"2024-01-03 18:57:27","title":"LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry","abstract":"Visual odometry estimates the motion of a moving camera based on visual input. Existing methods, mostly focusing on two-view point tracking, often ignore the rich temporal context in the image sequence, thereby overlooking the global motion patterns and providing no assessment of the full trajectory reliability. These shortcomings hinder performance in scenarios with occlusion, dynamic objects, and low-texture areas. To address these challenges, we present the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively combines visual, inter-track, and temporal cues with mindfully selected anchors for dynamic track estimation. Moreover, LEAP's temporal probabilistic formulation integrates distribution updates into a learnable iterative refinement module to reason about point-wise uncertainty. Based on these traits, we develop LEAP-VO, a robust visual odometry system adept at handling occlusions and dynamic scenes. Our mindful integration showcases a novel practice by employing long-term point tracking as the front-end. Extensive experiments demonstrate that the proposed pipeline significantly outperforms existing baselines across various visual odometry benchmarks.","sentences":["Visual odometry estimates the motion of a moving camera based on visual input.","Existing methods, mostly focusing on two-view point tracking, often ignore the rich temporal context in the image sequence, thereby overlooking the global motion patterns and providing no assessment of the full trajectory reliability.","These shortcomings hinder performance in scenarios with occlusion, dynamic objects, and low-texture areas.","To address these challenges, we present the Long-term Effective Any Point Tracking (LEAP) module.","LEAP innovatively combines visual, inter-track, and temporal cues with mindfully selected anchors for dynamic track estimation.","Moreover, LEAP's temporal probabilistic formulation integrates distribution updates into a learnable iterative refinement module to reason about point-wise uncertainty.","Based on these traits, we develop LEAP-VO, a robust visual odometry system adept at handling occlusions and dynamic scenes.","Our mindful integration showcases a novel practice by employing long-term point tracking as the front-end.","Extensive experiments demonstrate that the proposed pipeline significantly outperforms existing baselines across various visual odometry benchmarks."],"url":"http://arxiv.org/abs/2401.01887v1"}
{"created":"2024-01-03 18:55:16","title":"From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations","abstract":"We present a framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction. Given speech audio, we output multiple possibilities of gestural motion for an individual, including face, body, and hands. The key behind our method is in combining the benefits of sample diversity from vector quantization with the high-frequency details obtained through diffusion to generate more dynamic, expressive motion. We visualize the generated motion using highly photorealistic avatars that can express crucial nuances in gestures (e.g. sneers and smirks). To facilitate this line of research, we introduce a first-of-its-kind multi-view conversational dataset that allows for photorealistic reconstruction. Experiments show our model generates appropriate and diverse gestures, outperforming both diffusion- and VQ-only methods. Furthermore, our perceptual evaluation highlights the importance of photorealism (vs. meshes) in accurately assessing subtle motion details in conversational gestures. Code and dataset available online.","sentences":["We present a framework for generating full-bodied photorealistic avatars that gesture according to the conversational dynamics of a dyadic interaction.","Given speech audio, we output multiple possibilities of gestural motion for an individual, including face, body, and hands.","The key behind our method is in combining the benefits of sample diversity from vector quantization with the high-frequency details obtained through diffusion to generate more dynamic, expressive motion.","We visualize the generated motion using highly photorealistic avatars that can express crucial nuances in gestures (e.g. sneers and smirks).","To facilitate this line of research, we introduce a first-of-its-kind multi-view conversational dataset that allows for photorealistic reconstruction.","Experiments show our model generates appropriate and diverse gestures, outperforming both diffusion- and VQ-only methods.","Furthermore, our perceptual evaluation highlights the importance of photorealism (vs. meshes) in accurately assessing subtle motion details in conversational gestures.","Code and dataset available online."],"url":"http://arxiv.org/abs/2401.01885v1"}
{"created":"2024-01-03 18:53:53","title":"A rewriting-logic-with-SMT-based formal analysis and parameter synthesis framework for parametric time Petri nets","abstract":"This paper presents a concrete and a symbolic rewriting logic semantics for parametric time Petri nets with inhibitor arcs (PITPNs), a flexible model of timed systems where parameters are allowed in firing bounds. We prove that our semantics is bisimilar to the \"standard\" semantics of PITPNs. This allows us to use the rewriting logic tool Maude, combined with SMT solving, to provide sound and complete formal analyses for PITPNs. We develop and implement a new general folding approach for symbolic reachability, so that Maude-with-SMT reachability analysis terminates whenever the parametric state-class graph of the PITPN is finite. Our work opens up the possibility of using the many formal analysis capabilities of Maude -- including full LTL model checking, analysis with user-defined analysis strategies, and even statistical model checking -- for such nets. We illustrate this by explaining how almost all formal analysis and parameter synthesis methods supported by the state-of-the-art PITPN tool Romeo can be performed using Maude with SMT. In addition, we also support analysis and parameter synthesis from parametric initial markings, as well as full LTL model checking and analysis with user-defined execution strategies. Experiments show that our methods outperform Romeo in many cases.","sentences":["This paper presents a concrete and a symbolic rewriting logic semantics for parametric time Petri nets with inhibitor arcs (PITPNs), a flexible model of timed systems where parameters are allowed in firing bounds.","We prove that our semantics is bisimilar to the \"standard\" semantics of PITPNs.","This allows us to use the rewriting logic tool Maude, combined with SMT solving, to provide sound and complete formal analyses for PITPNs.","We develop and implement a new general folding approach for symbolic reachability, so that Maude-with-SMT reachability analysis terminates whenever the parametric state-class graph of the PITPN is finite.","Our work opens up the possibility of using the many formal analysis capabilities of Maude -- including full LTL model checking, analysis with user-defined analysis strategies, and even statistical model checking -- for such nets.","We illustrate this by explaining how almost all formal analysis and parameter synthesis methods supported by the state-of-the-art PITPN tool Romeo can be performed using Maude with SMT.","In addition, we also support analysis and parameter synthesis from parametric initial markings, as well as full LTL model checking and analysis with user-defined execution strategies.","Experiments show that our methods outperform Romeo in many cases."],"url":"http://arxiv.org/abs/2401.01884v1"}
{"created":"2024-01-03 18:53:22","title":"Mining Temporal Attack Patterns from Cyberthreat Intelligence Reports","abstract":"Defending from cyberattacks requires practitioners to operate on high-level adversary behavior. Cyberthreat intelligence (CTI) reports on past cyberattack incidents describe the chain of malicious actions with respect to time. To avoid repeating cyberattack incidents, practitioners must proactively identify and defend against recurring chain of actions - which we refer to as temporal attack patterns. Automatically mining the patterns among actions provides structured and actionable information on the adversary behavior of past cyberattacks. The goal of this paper is to aid security practitioners in prioritizing and proactive defense against cyberattacks by mining temporal attack patterns from cyberthreat intelligence reports. To this end, we propose ChronoCTI, an automated pipeline for mining temporal attack patterns from cyberthreat intelligence (CTI) reports of past cyberattacks. To construct ChronoCTI, we build the ground truth dataset of temporal attack patterns and apply state-of-the-art large language models, natural language processing, and machine learning techniques. We apply ChronoCTI on a set of 713 CTI reports, where we identify 124 temporal attack patterns - which we categorize into nine pattern categories. We identify that the most prevalent pattern category is to trick victim users into executing malicious code to initiate the attack, followed by bypassing the anti-malware system in the victim network. Based on the observed patterns, we advocate organizations to train users about cybersecurity best practices, introduce immutable operating systems with limited functionalities, and enforce multi-user authentications. Moreover, we advocate practitioners to leverage the automated mining capability of ChronoCTI and design countermeasures against the recurring attack patterns.","sentences":["Defending from cyberattacks requires practitioners to operate on high-level adversary behavior.","Cyberthreat intelligence (CTI) reports on past cyberattack incidents describe the chain of malicious actions with respect to time.","To avoid repeating cyberattack incidents, practitioners must proactively identify and defend against recurring chain of actions - which we refer to as temporal attack patterns.","Automatically mining the patterns among actions provides structured and actionable information on the adversary behavior of past cyberattacks.","The goal of this paper is to aid security practitioners in prioritizing and proactive defense against cyberattacks by mining temporal attack patterns from cyberthreat intelligence reports.","To this end, we propose ChronoCTI, an automated pipeline for mining temporal attack patterns from cyberthreat intelligence (CTI) reports of past cyberattacks.","To construct ChronoCTI, we build the ground truth dataset of temporal attack patterns and apply state-of-the-art large language models, natural language processing, and machine learning techniques.","We apply ChronoCTI on a set of 713 CTI reports, where we identify 124 temporal attack patterns - which we categorize into nine pattern categories.","We identify that the most prevalent pattern category is to trick victim users into executing malicious code to initiate the attack, followed by bypassing the anti-malware system in the victim network.","Based on the observed patterns, we advocate organizations to train users about cybersecurity best practices, introduce immutable operating systems with limited functionalities, and enforce multi-user authentications.","Moreover, we advocate practitioners to leverage the automated mining capability of ChronoCTI and design countermeasures against the recurring attack patterns."],"url":"http://arxiv.org/abs/2401.01883v1"}
{"created":"2024-01-03 18:39:13","title":"Theoretical guarantees on the best-of-n alignment policy","abstract":"A simple and effective method for the alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the base policy is equal to $\\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes. Finally, we propose a new estimator for the KL divergence and empirically show that it provides a tight approximation through a few examples.","sentences":["A simple and effective method for the alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked based on a reward function, and the highest ranking one is selected.","A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the base policy is equal to $\\log (n) - (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence.","We also explore the tightness of this upper bound in different regimes.","Finally, we propose a new estimator for the KL divergence and empirically show that it provides a tight approximation through a few examples."],"url":"http://arxiv.org/abs/2401.01879v1"}
{"created":"2024-01-03 18:24:18","title":"On the hardness of learning under symmetries","abstract":"We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries (\"equivariance\") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension. Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard.","sentences":["We study the problem of learning equivariant neural networks via gradient descent.","The incorporation of known symmetries (\"equivariance\") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision.","However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent.","In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent?","We answer this question in the negative.","In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension.","Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard."],"url":"http://arxiv.org/abs/2401.01869v1"}
{"created":"2024-01-03 18:23:30","title":"Step length measurement in the wild using FMCW radar","abstract":"With an aging population, numerous assistive and monitoring technologies are under development to enable older adults to age in place. To facilitate aging in place predicting risk factors such as falls, and hospitalization and providing early interventions are important. Much of the work on ambient monitoring for risk prediction has centered on gait speed analysis, utilizing privacy-preserving sensors like radar. Despite compelling evidence that monitoring step length, in addition to gait speed, is crucial for predicting risk, radar-based methods have not explored step length measurement in the home. Furthermore, laboratory experiments on step length measurement using radars are limited to proof of concept studies with few healthy subjects. To address this gap, a radar-based step length measurement system for the home is proposed based on detection and tracking using radar point cloud, followed by Doppler speed profiling of the torso to obtain step lengths in the home. The proposed method was evaluated in a clinical environment, involving 35 frail older adults, to establish its validity. Additionally, the method was assessed in people's homes, with 21 frail older adults who had participated in the clinical assessment. The proposed radar-based step length measurement method was compared to the gold standard Zeno Walkway Gait Analysis System, revealing a 4.5cm/8.3% error in a clinical setting. Furthermore, it exhibited excellent reliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings. The method also proved accurate in uncontrolled home settings, as indicated by a strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home measurements and in-clinic assessments.","sentences":["With an aging population, numerous assistive and monitoring technologies are under development to enable older adults to age in place.","To facilitate aging in place predicting risk factors such as falls, and hospitalization and providing early interventions are important.","Much of the work on ambient monitoring for risk prediction has centered on gait speed analysis, utilizing privacy-preserving sensors like radar.","Despite compelling evidence that monitoring step length, in addition to gait speed, is crucial for predicting risk, radar-based methods have not explored step length measurement in the home.","Furthermore, laboratory experiments on step length measurement using radars are limited to proof of concept studies with few healthy subjects.","To address this gap, a radar-based step length measurement system for the home is proposed based on detection and tracking using radar point cloud, followed by Doppler speed profiling of the torso to obtain step lengths in the home.","The proposed method was evaluated in a clinical environment, involving 35 frail older adults, to establish its validity.","Additionally, the method was assessed in people's homes, with 21 frail older adults who had participated in the clinical assessment.","The proposed radar-based step length measurement method was compared to the gold standard Zeno Walkway Gait Analysis System, revealing a 4.5cm/8.3% error in a clinical setting.","Furthermore, it exhibited excellent reliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings.","The method also proved accurate in uncontrolled home settings, as indicated by a strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home measurements and in-clinic assessments."],"url":"http://arxiv.org/abs/2401.01868v1"}
{"created":"2024-01-03 18:19:51","title":"Dataset Difficulty and the Role of Inductive Bias","abstract":"Motivated by the goals of dataset pruning and defect identification, a growing body of methods have been developed to score individual examples within a dataset. These methods, which we call \"example difficulty scores\", are typically used to rank or categorize examples, but the consistency of rankings between different training runs, scoring methods, and model architectures is generally unknown. To determine how example rankings vary due to these random and controlled effects, we systematically compare different formulations of scores over a range of runs and model architectures. We find that scores largely share the following traits: they are noisy over individual runs of a model, strongly correlated with a single notion of difficulty, and reveal examples that range from being highly sensitive to insensitive to the inductive biases of certain model architectures. Drawing from statistical genetics, we develop a simple method for fingerprinting model architectures using a few sensitive examples. These findings guide practitioners in maximizing the consistency of their scores (e.g. by choosing appropriate scoring methods, number of runs, and subsets of examples), and establishes comprehensive baselines for evaluating scores in the future.","sentences":["Motivated by the goals of dataset pruning and defect identification, a growing body of methods have been developed to score individual examples within a dataset.","These methods, which we call \"example difficulty scores\", are typically used to rank or categorize examples, but the consistency of rankings between different training runs, scoring methods, and model architectures is generally unknown.","To determine how example rankings vary due to these random and controlled effects, we systematically compare different formulations of scores over a range of runs and model architectures.","We find that scores largely share the following traits: they are noisy over individual runs of a model, strongly correlated with a single notion of difficulty, and reveal examples that range from being highly sensitive to insensitive to the inductive biases of certain model architectures.","Drawing from statistical genetics, we develop a simple method for fingerprinting model architectures using a few sensitive examples.","These findings guide practitioners in maximizing the consistency of their scores (e.g. by choosing appropriate scoring methods, number of runs, and subsets of examples), and establishes comprehensive baselines for evaluating scores in the future."],"url":"http://arxiv.org/abs/2401.01867v1"}
{"created":"2024-01-03 18:16:40","title":"Attackers reveal their arsenal: An investigation of adversarial techniques in CTI reports","abstract":"Context: Cybersecurity vendors often publish cyber threat intelligence (CTI) reports, referring to the written artifacts on technical and forensic analysis of the techniques used by the malware in APT attacks. Objective: The goal of this research is to inform cybersecurity practitioners about how adversaries form cyberattacks through an analysis of adversarial techniques documented in cyberthreat intelligence reports. Dataset: We use 594 adversarial techniques cataloged in MITRE ATT\\&CK. We systematically construct a set of 667 CTI reports that MITRE ATT\\&CK used as citations in the descriptions of the cataloged adversarial techniques. Methodology: We analyze the frequency and trend of adversarial techniques, followed by a qualitative analysis of the implementation of techniques. Next, we perform association rule mining to identify pairs of techniques recurring in APT attacks. We then perform qualitative analysis to identify the underlying relations among the techniques in the recurring pairs. Findings: The set of 667 CTI reports documents 10,370 techniques in total, and we identify 19 prevalent techniques accounting for 37.3\\% of documented techniques. We also identify 425 statistically significant recurring pairs and seven types of relations among the techniques in these pairs. The top three among the seven relationships suggest that techniques used by the malware inter-relate with one another in terms of (a) abusing or affecting the same system assets, (b) executing in sequences, and (c) overlapping in their implementations. Overall, the study quantifies how adversaries leverage techniques through malware in APT attacks based on publicly reported documents. We advocate organizations prioritize their defense against the identified prevalent techniques and actively hunt for potential malicious intrusion based on the identified pairs of techniques.","sentences":["Context: Cybersecurity vendors often publish cyber threat intelligence (CTI) reports, referring to the written artifacts on technical and forensic analysis of the techniques used by the malware in APT attacks.","Objective: The goal of this research is to inform cybersecurity practitioners about how adversaries form cyberattacks through an analysis of adversarial techniques documented in cyberthreat intelligence reports.","Dataset: We use 594 adversarial techniques cataloged in MITRE ATT\\&CK.","We systematically construct a set of 667 CTI reports that MITRE ATT\\&CK used as citations in the descriptions of the cataloged adversarial techniques.","Methodology:","We analyze the frequency and trend of adversarial techniques, followed by a qualitative analysis of the implementation of techniques.","Next, we perform association rule mining to identify pairs of techniques recurring in APT attacks.","We then perform qualitative analysis to identify the underlying relations among the techniques in the recurring pairs.","Findings:","The set of 667 CTI reports documents 10,370 techniques in total, and we identify 19 prevalent techniques accounting for 37.3\\% of documented techniques.","We also identify 425 statistically significant recurring pairs and seven types of relations among the techniques in these pairs.","The top three among the seven relationships suggest that techniques used by the malware inter-relate with one another in terms of (a) abusing or affecting the same system assets, (b) executing in sequences, and (c) overlapping in their implementations.","Overall, the study quantifies how adversaries leverage techniques through malware in APT attacks based on publicly reported documents.","We advocate organizations prioritize their defense against the identified prevalent techniques and actively hunt for potential malicious intrusion based on the identified pairs of techniques."],"url":"http://arxiv.org/abs/2401.01865v1"}
{"created":"2024-01-03 18:09:33","title":"A Vision Check-up for Language Models","abstract":"What does learning to model relationships between strings teach large language models (LLMs) about the visual world? We systematically evaluate LLMs' abilities to generate and recognize an assortment of visual concepts of increasing complexity and then demonstrate how a preliminary visual representation learning system can be trained using models of text. As language models lack the ability to consume or output visual information as pixels, we use code to represent images in our study. Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world. Furthermore, experiments on self-supervised visual representation learning, utilizing images generated with text models, highlight the potential to train vision models capable of making semantic assessments of natural images using just LLMs.","sentences":["What does learning to model relationships between strings teach large language models (LLMs) about the visual world?","We systematically evaluate LLMs' abilities to generate and recognize an assortment of visual concepts of increasing complexity and then demonstrate how a preliminary visual representation learning system can be trained using models of text.","As language models lack the ability to consume or output visual information as pixels, we use code to represent images in our study.","Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world.","Furthermore, experiments on self-supervised visual representation learning, utilizing images generated with text models, highlight the potential to train vision models capable of making semantic assessments of natural images using just LLMs."],"url":"http://arxiv.org/abs/2401.01862v1"}
{"created":"2024-01-03 18:06:28","title":"Synthetic dataset of ID and Travel Document","abstract":"This paper presents a new synthetic dataset of ID and travel documents, called SIDTD. The SIDTD dataset is created to help training and evaluating forged ID documents detection systems. Such a dataset has become a necessity as ID documents contain personal information and a public dataset of real documents can not be released. Moreover, forged documents are scarce, compared to legit ones, and the way they are generated varies from one fraudster to another resulting in a class of high intra-variability. In this paper we trained state-of-the-art models on this dataset and we compare them to the performance achieved in larger, but private, datasets. The creation of this dataset will help to document image analysis community to progress in the task of ID document verification.","sentences":["This paper presents a new synthetic dataset of ID and travel documents, called SIDTD.","The SIDTD dataset is created to help training and evaluating forged ID documents detection systems.","Such a dataset has become a necessity as ID documents contain personal information and a public dataset of real documents can not be released.","Moreover, forged documents are scarce, compared to legit ones, and the way they are generated varies from one fraudster to another resulting in a class of high intra-variability.","In this paper we trained state-of-the-art models on this dataset and we compare them to the performance achieved in larger, but private, datasets.","The creation of this dataset will help to document image analysis community to progress in the task of ID document verification."],"url":"http://arxiv.org/abs/2401.01858v1"}
{"created":"2024-01-03 18:02:13","title":"Optimal cross-learning for contextual bandits with unknown context distributions","abstract":"We consider the problem of designing contextual bandit algorithms in the ``cross-learning'' setting of Balseiro et al., where the learner observes the loss for the action they play in all possible contexts, not just the context of the current round. We specifically consider the setting where losses are chosen adversarially and contexts are sampled i.i.d. from an unknown distribution. In this setting, we resolve an open problem of Balseiro et al. by providing an efficient algorithm with a nearly tight (up to logarithmic factors) regret bound of $\\widetilde{O}(\\sqrt{TK})$, independent of the number of contexts. As a consequence, we obtain the first nearly tight regret bounds for the problems of learning to bid in first-price auctions (under unknown value distributions) and sleeping bandits with a stochastic action set.   At the core of our algorithm is a novel technique for coordinating the execution of a learning algorithm over multiple epochs in such a way to remove correlations between estimation of the unknown distribution and the actions played by the algorithm. This technique may be of independent interest for other learning problems involving estimation of an unknown context distribution.","sentences":["We consider the problem of designing contextual bandit algorithms in the ``cross-learning'' setting of Balseiro et al., where the learner observes the loss for the action they play in all possible contexts, not just the context of the current round.","We specifically consider the setting where losses are chosen adversarially and contexts are sampled i.i.d.","from an unknown distribution.","In this setting, we resolve an open problem of Balseiro et al. by providing an efficient algorithm with a nearly tight (up to logarithmic factors) regret bound of $\\widetilde{O}(\\sqrt{TK})$, independent of the number of contexts.","As a consequence, we obtain the first nearly tight regret bounds for the problems of learning to bid in first-price auctions (under unknown value distributions) and sleeping bandits with a stochastic action set.   ","At the core of our algorithm is a novel technique for coordinating the execution of a learning algorithm over multiple epochs in such a way to remove correlations between estimation of the unknown distribution and the actions played by the algorithm.","This technique may be of independent interest for other learning problems involving estimation of an unknown context distribution."],"url":"http://arxiv.org/abs/2401.01857v1"}
{"created":"2024-01-03 17:51:16","title":"Transformer Neural Autoregressive Flows","abstract":"Density estimation, a central problem in machine learning, can be performed using Normalizing Flows (NFs). NFs comprise a sequence of invertible transformations, that turn a complex target distribution into a simple one, by exploiting the change of variables theorem. Neural Autoregressive Flows (NAFs) and Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant members of the NF family. However, they suffer scalability issues and training instability due to the constraints imposed on the network structure. In this paper, we propose a novel solution to these challenges by exploiting transformers to define a new class of neural flows called Transformer Neural Autoregressive Flows (T-NAFs). T-NAFs treat each dimension of a random variable as a separate input token, using attention masking to enforce an autoregressive constraint. We take an amortization-inspired approach where the transformer outputs the parameters of an invertible transformation. The experimental results demonstrate that T-NAFs consistently match or outperform NAFs and B-NAFs across multiple datasets from the UCI benchmark. Remarkably, T-NAFs achieve these results using an order of magnitude fewer parameters than previous approaches, without composing multiple flows.","sentences":["Density estimation, a central problem in machine learning, can be performed using Normalizing Flows (NFs).","NFs comprise a sequence of invertible transformations, that turn a complex target distribution into a simple one, by exploiting the change of variables theorem.","Neural Autoregressive Flows (NAFs) and Block Neural Autoregressive Flows (B-NAFs) are arguably the most perfomant members of the NF family.","However, they suffer scalability issues and training instability due to the constraints imposed on the network structure.","In this paper, we propose a novel solution to these challenges by exploiting transformers to define a new class of neural flows called Transformer Neural Autoregressive Flows (T-NAFs).","T-NAFs treat each dimension of a random variable as a separate input token, using attention masking to enforce an autoregressive constraint.","We take an amortization-inspired approach where the transformer outputs the parameters of an invertible transformation.","The experimental results demonstrate that T-NAFs consistently match or outperform NAFs and B-NAFs across multiple datasets from the UCI benchmark.","Remarkably, T-NAFs achieve these results using an order of magnitude fewer parameters than previous approaches, without composing multiple flows."],"url":"http://arxiv.org/abs/2401.01855v1"}
{"created":"2024-01-03 17:48:10","title":"Multilingual Instruction Tuning With Just a Pinch of Multilinguality","abstract":"As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial. One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language. In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages. We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning. Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning. In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those languages. Finally, we find that increasing the number of languages in the instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual generalization. Our results suggest that building massively multilingual instruction-tuned models can be done with only a very small set of multilingual instruction-responses.","sentences":["As instruction-tuned large language models (LLMs) gain global adoption, their ability to follow instructions in multiple languages becomes increasingly crucial.","One promising approach is cross-lingual transfer, where a model acquires specific functionality on some language by finetuning on another language.","In this work, we investigate how multilinguality during instruction tuning of a multilingual LLM affects instruction-following across languages.","We first show that many languages transfer some instruction-following capabilities to other languages from even monolingual tuning.","Furthermore, we find that only 40 multilingual examples in an English tuning set substantially improve multilingual instruction-following, both in seen and unseen languages during tuning.","In general, we observe that models tuned on multilingual mixtures exhibit comparable or superior performance in several languages compared to monolingually tuned models, despite training on 10x fewer examples in those languages.","Finally, we find that increasing the number of languages in the instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual generalization.","Our results suggest that building massively multilingual instruction-tuned models can be done with only a very small set of multilingual instruction-responses."],"url":"http://arxiv.org/abs/2401.01854v1"}
{"created":"2024-01-03 17:44:17","title":"The Power of Training: How Different Neural Network Setups Influence the Energy Demand","abstract":"This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption. While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission. Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer. Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results. Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning.","sentences":["This work examines the effects of variations in machine learning training regimes and learning paradigms on the corresponding energy consumption.","While increasing data availability and innovation in high-performance hardware fuels the training of sophisticated models, it also supports the fading perception of energy consumption and carbon emission.","Therefore, the goal of this work is to create awareness about the energy impact of general training parameters and processes, from learning rate over batch size to knowledge transfer.","Multiple setups with different hyperparameter initializations are evaluated on two different hardware configurations to obtain meaningful results.","Experiments on pretraining and multitask training are conducted on top of the baseline results to determine their potential towards sustainable machine learning."],"url":"http://arxiv.org/abs/2401.01851v1"}
{"created":"2024-01-03 17:36:27","title":"DGDNN: Decoupled Graph Diffusion Neural Network for Stock Movement Prediction","abstract":"Forecasting future stock trends remains challenging for academia and industry due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics influencing stock prices. In recent years, graph neural networks have achieved remarkable performance in this problem by formulating multiple stocks as graph-structured data. However, most of these approaches rely on artificially defined factors to construct static stock graphs, which fail to capture the intrinsic interdependencies between stocks that rapidly evolve. In addition, these methods often ignore the hierarchical features of the stocks and lose distinctive information within. In this work, we propose a novel graph learning approach implemented without expert knowledge to address these issues. First, our approach automatically constructs dynamic stock graphs by entropy-driven edge generation from a signal processing perspective. Then, we further learn task-optimal dependencies between stocks via a generalized graph diffusion process on constructed stock graphs. Last, a decoupled representation learning scheme is adopted to capture distinctive hierarchical intra-stock features. Experimental results demonstrate substantial improvements over state-of-the-art baselines on real-world datasets. Moreover, the ablation study and sensitivity study further illustrate the effectiveness of the proposed method in modeling the time-evolving inter-stock and intra-stock dynamics.","sentences":["Forecasting future stock trends remains challenging for academia and industry due to stochastic inter-stock dynamics and hierarchical intra-stock dynamics influencing stock prices.","In recent years, graph neural networks have achieved remarkable performance in this problem by formulating multiple stocks as graph-structured data.","However, most of these approaches rely on artificially defined factors to construct static stock graphs, which fail to capture the intrinsic interdependencies between stocks that rapidly evolve.","In addition, these methods often ignore the hierarchical features of the stocks and lose distinctive information within.","In this work, we propose a novel graph learning approach implemented without expert knowledge to address these issues.","First, our approach automatically constructs dynamic stock graphs by entropy-driven edge generation from a signal processing perspective.","Then, we further learn task-optimal dependencies between stocks via a generalized graph diffusion process on constructed stock graphs.","Last, a decoupled representation learning scheme is adopted to capture distinctive hierarchical intra-stock features.","Experimental results demonstrate substantial improvements over state-of-the-art baselines on real-world datasets.","Moreover, the ablation study and sensitivity study further illustrate the effectiveness of the proposed method in modeling the time-evolving inter-stock and intra-stock dynamics."],"url":"http://arxiv.org/abs/2401.01846v1"}
{"created":"2024-01-03 17:22:48","title":"Investigating Semi-Supervised Learning Algorithms in Text Datasets","abstract":"Using large training datasets enhances the generalization capabilities of neural networks. Semi-supervised learning (SSL) is useful when there are few labeled data and a lot of unlabeled data. SSL methods that use data augmentation are most successful for image datasets. In contrast, texts do not have consistent augmentation methods as images. Consequently, methods that use augmentation are not as effective in text data as they are in image data. In this study, we compared SSL algorithms that do not require augmentation; these are self-training, co-training, tri-training, and tri-training with disagreement. In the experiments, we used 4 different text datasets for different tasks. We examined the algorithms from a variety of perspectives by asking experiment questions and suggested several improvements. Among the algorithms, tri-training with disagreement showed the closest performance to the Oracle; however, performance gap shows that new semi-supervised algorithms or improvements in existing methods are needed.","sentences":["Using large training datasets enhances the generalization capabilities of neural networks.","Semi-supervised learning (SSL) is useful when there are few labeled data and a lot of unlabeled data.","SSL methods that use data augmentation are most successful for image datasets.","In contrast, texts do not have consistent augmentation methods as images.","Consequently, methods that use augmentation are not as effective in text data as they are in image data.","In this study, we compared SSL algorithms that do not require augmentation; these are self-training, co-training, tri-training, and tri-training with disagreement.","In the experiments, we used 4 different text datasets for different tasks.","We examined the algorithms from a variety of perspectives by asking experiment questions and suggested several improvements.","Among the algorithms, tri-training with disagreement showed the closest performance to the Oracle; however, performance gap shows that new semi-supervised algorithms or improvements in existing methods are needed."],"url":"http://arxiv.org/abs/2401.01843v1"}
{"created":"2024-01-03 17:20:27","title":"Wasserstein Nonnegative Tensor Factorization with Manifold Regularization","abstract":"Nonnegative tensor factorization (NTF) has become an important tool for feature extraction and part-based representation with preserved intrinsic structure information from nonnegative high-order data. However, the original NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss function which treats each feature equally leading to the neglect of the side-information of features. To utilize correlation information of features and manifold information of samples, we introduce Wasserstein manifold nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein distance between the distribution of input tensorial data and the distribution of reconstruction. Although some researches about Wasserstein distance have been proposed in nonnegative matrix factorization (NMF), they ignore the spatial structure information of higher-order data. We use Wasserstein distance (a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and add a graph regularizer to a latent factor. Experimental results demonstrate the effectiveness of the proposed method compared with other NMF and NTF methods.","sentences":["Nonnegative tensor factorization (NTF) has become an important tool for feature extraction and part-based representation with preserved intrinsic structure information from nonnegative high-order data.","However, the original NTF methods utilize Euclidean or Kullback-Leibler divergence as the loss function which treats each feature equally leading to the neglect of the side-information of features.","To utilize correlation information of features and manifold information of samples, we introduce Wasserstein manifold nonnegative tensor factorization (WMNTF), which minimizes the Wasserstein distance between the distribution of input tensorial data and the distribution of reconstruction.","Although some researches about Wasserstein distance have been proposed in nonnegative matrix factorization (NMF), they ignore the spatial structure information of higher-order data.","We use Wasserstein distance (a.k.a Earth Mover's distance or Optimal Transport distance) as a metric and add a graph regularizer to a latent factor.","Experimental results demonstrate the effectiveness of the proposed method compared with other NMF and NTF methods."],"url":"http://arxiv.org/abs/2401.01842v1"}
{"created":"2024-01-03 17:19:54","title":"Act as You Learn: Adaptive Decision-Making in Non-Stationary Markov Decision Processes","abstract":"A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time. Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP). However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment. We argue that both these assumptions are invalid in practice -- updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about. We present a heuristic search algorithm called \\textit{Adaptive Monte Carlo Tree Search (ADA-MCTS)} that addresses these challenges. We show that the agent can learn the updated dynamics of the environment over time and then act as it learns, i.e., if the agent is in a region of the state space about which it has updated knowledge, it can avoid being pessimistic. To quantify ``updated knowledge,'' we disintegrate the aleatoric and epistemic uncertainty in the agent's updated belief and show how the agent can use these estimates for decision-making. We compare the proposed approach with the multiple state-of-the-art approaches in decision-making across multiple well-established open-source problems and empirically show that our approach is faster and highly adaptive without sacrificing safety.","sentences":["A fundamental (and largely open) challenge in sequential decision-making is dealing with non-stationary environments, where exogenous environmental conditions change over time.","Such problems are traditionally modeled as non-stationary Markov decision processes (NSMDP).","However, existing approaches for decision-making in NSMDPs have two major shortcomings: first, they assume that the updated environmental dynamics at the current time are known (although future dynamics can change); and second, planning is largely pessimistic, i.e., the agent acts ``safely'' to account for the non-stationary evolution of the environment.","We argue that both these assumptions are invalid in practice -- updated environmental conditions are rarely known, and as the agent interacts with the environment, it can learn about the updated dynamics and avoid being pessimistic, at least in states whose dynamics it is confident about.","We present a heuristic search algorithm called \\textit{Adaptive Monte Carlo Tree Search (ADA-MCTS)} that addresses these challenges.","We show that the agent can learn the updated dynamics of the environment over time and then act as it learns, i.e., if the agent is in a region of the state space about which it has updated knowledge, it can avoid being pessimistic.","To quantify ``updated knowledge,'' we disintegrate the aleatoric and epistemic uncertainty in the agent's updated belief and show how the agent can use these estimates for decision-making.","We compare the proposed approach with the multiple state-of-the-art approaches in decision-making across multiple well-established open-source problems and empirically show that our approach is faster and highly adaptive without sacrificing safety."],"url":"http://arxiv.org/abs/2401.01841v1"}
{"created":"2024-01-03 17:11:27","title":"Frequency Domain Modality-invariant Feature Learning for Visible-infrared Person Re-Identification","abstract":"Visible-infrared person re-identification (VI-ReID) is challenging due to the significant cross-modality discrepancies between visible and infrared images. While existing methods have focused on designing complex network architectures or using metric learning constraints to learn modality-invariant features, they often overlook which specific component of the image causes the modality discrepancy problem. In this paper, we first reveal that the difference in the amplitude component of visible and infrared images is the primary factor that causes the modality discrepancy and further propose a novel Frequency Domain modality-invariant feature learning framework (FDMNet) to reduce modality discrepancy from the frequency domain perspective. Our framework introduces two novel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and the Phrase-Preserving Normalization (PPNorm) module, to enhance the modality-invariant amplitude component and suppress the modality-specific component at both the image- and feature-levels. Extensive experimental results on two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior performance of our FDMNet against state-of-the-art methods.","sentences":["Visible-infrared person re-identification (VI-ReID) is challenging due to the significant cross-modality discrepancies between visible and infrared images.","While existing methods have focused on designing complex network architectures or using metric learning constraints to learn modality-invariant features, they often overlook which specific component of the image causes the modality discrepancy problem.","In this paper, we first reveal that the difference in the amplitude component of visible and infrared images is the primary factor that causes the modality discrepancy and further propose a novel Frequency Domain modality-invariant feature learning framework (FDMNet) to reduce modality discrepancy from the frequency domain perspective.","Our framework introduces two novel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and the Phrase-Preserving Normalization (PPNorm) module, to enhance the modality-invariant amplitude component and suppress the modality-specific component at both the image- and feature-levels.","Extensive experimental results on two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior performance of our FDMNet against state-of-the-art methods."],"url":"http://arxiv.org/abs/2401.01839v1"}
{"created":"2024-01-03 17:05:17","title":"NODEC: Neural ODE For Optimal Control of Unknown Dynamical Systems","abstract":"Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework. For systems with unknown dynamics, an additional step of dynamics modeling is required. However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function. Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency. To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model. Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknown dynamical system towards target states. Our experiments demonstrate the effectiveness and data efficiency of NODEC for learning optimal control of unknown dynamical systems.","sentences":["Controlling complex dynamical systems is generally associated with minimizing certain control objectives with known dynamics under the variational calculus framework.","For systems with unknown dynamics, an additional step of dynamics modeling is required.","However, any inaccuracy in dynamics modeling will lead to sub-optimality in the resulting control function.","Another set of approaches for controlling unknown dynamical systems - reinforcement learning, folds the dynamics modeling into controller training via value function approximation or policy gradient through extensively interacting with the environment, but it suffers from low data efficiency.","To address these, we introduce NODEC, a novel framework for controlling unknown dynamical systems, which combines dynamics modelling and controller training using a coupled neural ODE model.","Through an intriguing interplay between the two coupled neural networks, NODEC learns system dynamics as well as optimal controls that guides the unknown dynamical system towards target states.","Our experiments demonstrate the effectiveness and data efficiency of NODEC for learning optimal control of unknown dynamical systems."],"url":"http://arxiv.org/abs/2401.01836v1"}
{"created":"2024-01-03 17:01:44","title":"Concurrent Brainstorming & Hypothesis Satisfying: An Iterative Framework for Enhanced Retrieval-Augmented Generation (R2CBR3H-SR)","abstract":"Addressing the complexity of comprehensive information retrieval, this study introduces an innovative, iterative retrieval-augmented generation system. Our approach uniquely integrates a vector-space driven re-ranking mechanism with concurrent brainstorming to expedite the retrieval of highly relevant documents, thereby streamlining the generation of potential queries. This sets the stage for our novel hybrid process, which synergistically combines hypothesis formulation with satisfying decision-making strategy to determine content adequacy, leveraging a chain of thought-based prompting technique. This unified hypothesize-satisfied phase intelligently distills information to ascertain whether user queries have been satisfactorily addressed. Upon reaching this criterion, the system refines its output into a concise representation, maximizing conceptual density with minimal verbosity. The iterative nature of the workflow enhances process efficiency and accuracy. Crucially, the concurrency within the brainstorming phase significantly accelerates recursive operations, facilitating rapid convergence to solution satisfaction. Compared to conventional methods, our system demonstrates a marked improvement in computational time and cost-effectiveness. This research advances the state-of-the-art in intelligent retrieval systems, setting a new benchmark for resource-efficient information extraction and abstraction in knowledge-intensive applications.","sentences":["Addressing the complexity of comprehensive information retrieval, this study introduces an innovative, iterative retrieval-augmented generation system.","Our approach uniquely integrates a vector-space driven re-ranking mechanism with concurrent brainstorming to expedite the retrieval of highly relevant documents, thereby streamlining the generation of potential queries.","This sets the stage for our novel hybrid process, which synergistically combines hypothesis formulation with satisfying decision-making strategy to determine content adequacy, leveraging a chain of thought-based prompting technique.","This unified hypothesize-satisfied phase intelligently distills information to ascertain whether user queries have been satisfactorily addressed.","Upon reaching this criterion, the system refines its output into a concise representation, maximizing conceptual density with minimal verbosity.","The iterative nature of the workflow enhances process efficiency and accuracy.","Crucially, the concurrency within the brainstorming phase significantly accelerates recursive operations, facilitating rapid convergence to solution satisfaction.","Compared to conventional methods, our system demonstrates a marked improvement in computational time and cost-effectiveness.","This research advances the state-of-the-art in intelligent retrieval systems, setting a new benchmark for resource-efficient information extraction and abstraction in knowledge-intensive applications."],"url":"http://arxiv.org/abs/2401.01835v1"}
{"created":"2024-01-03 16:49:30","title":"Teaching with a companion: the case of gravity","abstract":"Virtual Reality (VR) has repeatedly proven its effectiveness in student learning. However, despite its benefits, the student equipped with a personal headset remains isolated from the real world while immersed in a virtual space and the classic student-teacher model of learning is difficult to transpose in such a situation. This study aims to bring the teacher back into the learning process when students use a VR headset. We describe the benefits of using a companion for educational purposes, taking as a test case the concept of gravity. We present an experimental setup designed to compare three different teaching contexts: with a physically present real teacher, using a live video of the teacher, and with a VR avatar of the teacher. We designed and evaluated three scenarios to teach the concept of gravity: an introduction to the concept of free fall, a parabolic trajectory workshop and a final exercise combining both approaches. Due to sanitary conditions, only pre-tests are reported. The results showed that the effectiveness of using the VR simulations for learning and the self-confidence level of the students increased as well. The interviews show that the students ranked the teaching modes in this order: VR companion mode, video communication and real teacher.","sentences":["Virtual Reality (VR) has repeatedly proven its effectiveness in student learning.","However, despite its benefits, the student equipped with a personal headset remains isolated from the real world while immersed in a virtual space and the classic student-teacher model of learning is difficult to transpose in such a situation.","This study aims to bring the teacher back into the learning process when students use a VR headset.","We describe the benefits of using a companion for educational purposes, taking as a test case the concept of gravity.","We present an experimental setup designed to compare three different teaching contexts: with a physically present real teacher, using a live video of the teacher, and with a VR avatar of the teacher.","We designed and evaluated three scenarios to teach the concept of gravity: an introduction to the concept of free fall, a parabolic trajectory workshop and a final exercise combining both approaches.","Due to sanitary conditions, only pre-tests are reported.","The results showed that the effectiveness of using the VR simulations for learning and the self-confidence level of the students increased as well.","The interviews show that the students ranked the teaching modes in this order: VR companion mode, video communication and real teacher."],"url":"http://arxiv.org/abs/2401.01832v1"}
{"created":"2024-01-03 16:48:39","title":"Immersive Serious Games for Learning Physics Concepts: The Case of Density","abstract":"Training students in basic concepts of physics, such as the ones related to mass, volume, or density, is much more complicated than just stating the underlying definitions and laws. One of the reasons for this is that most students have deeply rooted delusions and misconceptions about the behavior of objects, sometimes close to magical thinking. Many innovative and promising technologies, in particular Virtual Reality (VR), can be used to enhance student learning. We compared the effectiveness of a serious immersive game in teaching the concept of density in various conditions: a 2D version in an embedded web browser and a 3D immersive game in VR. We also developed a specific questionnaire to assess students' knowledge improvement. Primary results have shown an increase in learning efficiency using VR. Also, most students were able to see the shortcomings of their initial theories and revise them, which means that they improved their understanding of this topic.","sentences":["Training students in basic concepts of physics, such as the ones related to mass, volume, or density, is much more complicated than just stating the underlying definitions and laws.","One of the reasons for this is that most students have deeply rooted delusions and misconceptions about the behavior of objects, sometimes close to magical thinking.","Many innovative and promising technologies, in particular Virtual Reality (VR), can be used to enhance student learning.","We compared the effectiveness of a serious immersive game in teaching the concept of density in various conditions: a 2D version in an embedded web browser and a 3D immersive game in VR.","We also developed a specific questionnaire to assess students' knowledge improvement.","Primary results have shown an increase in learning efficiency using VR.","Also, most students were able to see the shortcomings of their initial theories and revise them, which means that they improved their understanding of this topic."],"url":"http://arxiv.org/abs/2401.01831v1"}
{"created":"2024-01-03 16:47:13","title":"Iterative Mask Filling: An Effective Text Augmentation Method Using Masked Language Modeling","abstract":"Data augmentation is an effective technique for improving the performance of machine learning models. However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision. In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model. Our method involves iteratively masking words in a sentence and replacing them with language model predictions. We have tested our proposed method on various NLP tasks and found it to be effective in many cases. Our results are presented along with a comparison to existing augmentation methods. Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets.","sentences":["Data augmentation is an effective technique for improving the performance of machine learning models.","However, it has not been explored as extensively in natural language processing (NLP) as it has in computer vision.","In this paper, we propose a novel text augmentation method that leverages the Fill-Mask feature of the transformer-based BERT model.","Our method involves iteratively masking words in a sentence and replacing them with language model predictions.","We have tested our proposed method on various NLP tasks and found it to be effective in many cases.","Our results are presented along with a comparison to existing augmentation methods.","Experimental results show that our proposed method significantly improves performance, especially on topic classification datasets."],"url":"http://arxiv.org/abs/2401.01830v1"}
{"created":"2024-01-03 16:43:47","title":"Moonshot: Towards Controllable Video Generation and Editing with Multimodal Conditions","abstract":"Most existing video diffusion models (VDMs) are limited to mere text conditions. Thereby, they are usually lacking in control over visual appearance and geometry structure of the generated videos. This work presents Moonshot, a new video generation model that conditions simultaneously on multimodal inputs of image and text. The model builts upon a core module, called multimodal video block (MVB), which consists of conventional spatialtemporal layers for representing video features, and a decoupled cross-attention layer to address image and text inputs for appearance conditioning. In addition, we carefully design the model architecture such that it can optionally integrate with pre-trained image ControlNet modules for geometry visual conditions, without needing of extra training overhead as opposed to prior methods. Experiments show that with versatile multimodal conditioning mechanisms, Moonshot demonstrates significant improvement on visual quality and temporal consistency compared to existing models. In addition, the model can be easily repurposed for a variety of generative applications, such as personalized video generation, image animation and video editing, unveiling its potential to serve as a fundamental architecture for controllable video generation. Models will be made public on https://github.com/salesforce/LAVIS.","sentences":["Most existing video diffusion models (VDMs) are limited to mere text conditions.","Thereby, they are usually lacking in control over visual appearance and geometry structure of the generated videos.","This work presents Moonshot, a new video generation model that conditions simultaneously on multimodal inputs of image and text.","The model builts upon a core module, called multimodal video block (MVB), which consists of conventional spatialtemporal layers for representing video features, and a decoupled cross-attention layer to address image and text inputs for appearance conditioning.","In addition, we carefully design the model architecture such that it can optionally integrate with pre-trained image ControlNet modules for geometry visual conditions, without needing of extra training overhead as opposed to prior methods.","Experiments show that with versatile multimodal conditioning mechanisms, Moonshot demonstrates significant improvement on visual quality and temporal consistency compared to existing models.","In addition, the model can be easily repurposed for a variety of generative applications, such as personalized video generation, image animation and video editing, unveiling its potential to serve as a fundamental architecture for controllable video generation.","Models will be made public on https://github.com/salesforce/LAVIS."],"url":"http://arxiv.org/abs/2401.01827v1"}
{"created":"2024-01-03 16:43:09","title":"Data-Driven Power Modeling and Monitoring via Hardware Performance Counters Tracking","abstract":"In the current high-performance and embedded computing era, full-stack energy-centric design is paramount. Use cases require increasingly high performance at an affordable power budget, often under real-time constraints. Extreme heterogeneity and parallelism address these issues but greatly complicate online power consumption assessment, which is essential for dynamic hardware and software stack adaptations. We introduce a novel architecture-agnostic power modeling methodology with state-of-the-art accuracy, low overhead, and high responsiveness. Our methodology identifies the best Performance Monitoring Counters (PMCs) to model the power consumption of each hardware sub-system at each Dynamic Voltage and Frequency Scaling (DVFS) state. The individual linear models are combined into a complete model that effectively describes the power consumption of the whole system, achieving high accuracy and low overhead. Our evaluation reports an average estimation error of 7.5 % for power consumption and 1.3 % for energy. Furthermore, we propose Runmeter, an open-source, PMC-based monitoring framework integrated into the Linux kernel. Runmeter manages PMC samples collection and manipulation, efficiently evaluating our power models at runtime. With a time overhead of only 0.7 % in the worst case, Runmeter provides responsive and accurate power measurements directly in the kernel, which can be employed for actuation policies such as Dynamic Power Management (DPM) and power-aware task scheduling.","sentences":["In the current high-performance and embedded computing era, full-stack energy-centric design is paramount.","Use cases require increasingly high performance at an affordable power budget, often under real-time constraints.","Extreme heterogeneity and parallelism address these issues but greatly complicate online power consumption assessment, which is essential for dynamic hardware and software stack adaptations.","We introduce a novel architecture-agnostic power modeling methodology with state-of-the-art accuracy, low overhead, and high responsiveness.","Our methodology identifies the best Performance Monitoring Counters (PMCs) to model the power consumption of each hardware sub-system at each Dynamic Voltage and Frequency Scaling (DVFS) state.","The individual linear models are combined into a complete model that effectively describes the power consumption of the whole system, achieving high accuracy and low overhead.","Our evaluation reports an average estimation error of 7.5 % for power consumption and 1.3 % for energy.","Furthermore, we propose Runmeter, an open-source, PMC-based monitoring framework integrated into the Linux kernel.","Runmeter manages PMC samples collection and manipulation, efficiently evaluating our power models at runtime.","With a time overhead of only 0.7 % in the worst case, Runmeter provides responsive and accurate power measurements directly in the kernel, which can be employed for actuation policies such as Dynamic Power Management (DPM) and power-aware task scheduling."],"url":"http://arxiv.org/abs/2401.01826v1"}
{"created":"2024-01-03 16:42:13","title":"Physio: An LLM-Based Physiotherapy Advisor","abstract":"The capabilities of the most recent language models have increased the interest in integrating them into real-world applications. However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains. Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being. In this paper, we present Physio, a chat-based application for physical rehabilitation. Physio is capable of making an initial diagnosis while citing reliable health sources to support the information provided. Furthermore, drawing upon external knowledge databases, Physio can recommend rehabilitation exercises and over-the-counter medication for symptom relief. By combining these features, Physio can leverage the power of generative models for language processing while also conditioning its response on dependable and verifiable sources. A live demo of Physio is available at https://physio.inesctec.pt.","sentences":["The capabilities of the most recent language models have increased the interest in integrating them into real-world applications.","However, the fact that these models generate plausible, yet incorrect text poses a constraint when considering their use in several domains.","Healthcare is a prime example of a domain where text-generative trustworthiness is a hard requirement to safeguard patient well-being.","In this paper, we present Physio, a chat-based application for physical rehabilitation.","Physio is capable of making an initial diagnosis while citing reliable health sources to support the information provided.","Furthermore, drawing upon external knowledge databases, Physio can recommend rehabilitation exercises and over-the-counter medication for symptom relief.","By combining these features, Physio can leverage the power of generative models for language processing while also conditioning its response on dependable and verifiable sources.","A live demo of Physio is available at https://physio.inesctec.pt."],"url":"http://arxiv.org/abs/2401.01825v1"}
{"created":"2024-01-03 16:38:56","title":"HawkRover: An Autonomous mmWave Vehicular Communication Testbed with Multi-sensor Fusion and Deep Learning","abstract":"Connected and automated vehicles (CAVs) have become a transformative technology that can change our daily life. Currently, millimeter-wave (mmWave) bands are identified as the promising CAV connectivity solution. While it can provide high data rate, their realization faces many challenges such as high attenuation during mmWave signal propagation and mobility management. Existing solution has to initiate pilot signal to measure channel information, then apply signal processing to calculate the best narrow beam towards the receiver end to guarantee sufficient signal power. This process takes significant overhead and time, hence not suitable for vehicles. In this study, we propose an autonomous and low-cost testbed to collect extensive co-located mmWave signal and other sensors data such as LiDAR (Light Detection and Ranging), cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave vehicular communications. Intuitively, these sensors can build a 3D map around the vehicle and signal propagation path can be estimated, eliminating iterative the process via pilot signals. This multimodal data fusion, together with AI, is expected to bring significant advances in ``connected'' research.","sentences":["Connected and automated vehicles (CAVs) have become a transformative technology that can change our daily life.","Currently, millimeter-wave (mmWave) bands are identified as the promising CAV connectivity solution.","While it can provide high data rate, their realization faces many challenges such as high attenuation during mmWave signal propagation and mobility management.","Existing solution has to initiate pilot signal to measure channel information, then apply signal processing to calculate the best narrow beam towards the receiver end to guarantee sufficient signal power.","This process takes significant overhead and time, hence not suitable for vehicles.","In this study, we propose an autonomous and low-cost testbed to collect extensive co-located mmWave signal and other sensors data such as LiDAR (Light Detection and Ranging), cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave vehicular communications.","Intuitively, these sensors can build a 3D map around the vehicle and signal propagation path can be estimated, eliminating iterative the process via pilot signals.","This multimodal data fusion, together with AI, is expected to bring significant advances in ``connected'' research."],"url":"http://arxiv.org/abs/2401.01822v1"}
{"created":"2024-01-03 16:38:56","title":"Detours for Navigating Instructional Videos","abstract":"We introduce the video detours problem for navigating instructional videos. Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration. To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries. Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data. We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques. Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%.","sentences":["We introduce the video detours problem for navigating instructional videos.","Given a source video and a natural language query asking to alter the how-to video's current path of execution in a certain way, the goal is to find a related ''detour video'' that satisfies the requested alteration.","To address this challenge, we propose VidDetours, a novel video-language approach that learns to retrieve the targeted temporal segments from a large repository of how-to's using video-and-text conditioned queries.","Furthermore, we devise a language-based pipeline that exploits how-to video narration text to create weakly supervised training data.","We demonstrate our idea applied to the domain of how-to cooking videos, where a user can detour from their current recipe to find steps with alternate ingredients, tools, and techniques.","Validating on a ground truth annotated dataset of 16K samples, we show our model's significant improvements over best available methods for video retrieval and question answering, with recall rates exceeding the state of the art by 35%."],"url":"http://arxiv.org/abs/2401.01823v1"}
{"created":"2024-01-03 16:21:46","title":"SENS3: Multisensory Database of Finger-Surface Interactions and Corresponding Sensations","abstract":"The growing demand for natural interactions with technology underscores the importance of achieving realistic touch sensations in digital environments. Realizing this goal highly depends on comprehensive databases of finger-surface interactions, which need further development. Here, we present SENS3, an extensive open-access repository of multisensory data acquired from fifty surfaces when two participants explored them with their fingertips through static contact, pressing, tapping, and sliding. SENS3 encompasses high-fidelity visual, audio, and haptic information recorded during these interactions, including videos, sounds, contact forces, torques, positions, accelerations, skin temperature, heat flux, and surface photographs. Additionally, it incorporates thirteen participants' psychophysical sensation ratings while exploring these surfaces freely. We anticipate that SENS3 will be valuable for advancing multisensory texture rendering, user experience development, and touch sensing in robotics.","sentences":["The growing demand for natural interactions with technology underscores the importance of achieving realistic touch sensations in digital environments.","Realizing this goal highly depends on comprehensive databases of finger-surface interactions, which need further development.","Here, we present SENS3, an extensive open-access repository of multisensory data acquired from fifty surfaces when two participants explored them with their fingertips through static contact, pressing, tapping, and sliding.","SENS3 encompasses high-fidelity visual, audio, and haptic information recorded during these interactions, including videos, sounds, contact forces, torques, positions, accelerations, skin temperature, heat flux, and surface photographs.","Additionally, it incorporates thirteen participants' psychophysical sensation ratings while exploring these surfaces freely.","We anticipate that SENS3 will be valuable for advancing multisensory texture rendering, user experience development, and touch sensing in robotics."],"url":"http://arxiv.org/abs/2401.01818v1"}
{"created":"2024-01-03 16:20:11","title":"Many-Objective-Optimized Semi-Automated Robotic Disassembly Sequences","abstract":"This study tasckles the problem of many-objective sequence optimization for semi-automated robotic disassembly operations. To this end, we employ a many-objective genetic algorithm (MaOGA) algorithm inspired by the Non-dominated Sorting Genetic Algorithm (NSGA)-III, along with robotic-disassembly-oriented constraints and objective functions derived from geometrical and robot simulations using 3-dimensional (3D) geometrical information stored in a 3D Computer-Aided Design (CAD) model of the target product. The MaOGA begins by generating a set of initial chromosomes based on a contact and connection graph (CCG), rather than random chromosomes, to avoid falling into a local minimum and yield repeatable convergence. The optimization imposes constraints on feasibility and stability as well as objective functions regarding difficulty, efficiency, prioritization, and allocability to generate a sequence that satisfies many preferred conditions under mandatory requirements for semi-automated robotic disassembly. The NSGA-III-inspired MaOGA also utilizes non-dominated sorting and niching with reference lines to further encourage steady and stable exploration and uniformly lower the overall evaluation values. Our sequence generation experiments for a complex product (36 parts) demonstrated that the proposed method can consistently produce feasible and stable sequences with a 100% success rate, bringing the multiple preferred conditions closer to the optimal solution required for semi-automated robotic disassembly operations.","sentences":["This study tasckles the problem of many-objective sequence optimization for semi-automated robotic disassembly operations.","To this end, we employ a many-objective genetic algorithm (MaOGA) algorithm inspired by the Non-dominated Sorting Genetic Algorithm (NSGA)-III, along with robotic-disassembly-oriented constraints and objective functions derived from geometrical and robot simulations using 3-dimensional (3D) geometrical information stored in a 3D Computer-Aided Design (CAD) model of the target product.","The MaOGA begins by generating a set of initial chromosomes based on a contact and connection graph (CCG), rather than random chromosomes, to avoid falling into a local minimum and yield repeatable convergence.","The optimization imposes constraints on feasibility and stability as well as objective functions regarding difficulty, efficiency, prioritization, and allocability to generate a sequence that satisfies many preferred conditions under mandatory requirements for semi-automated robotic disassembly.","The NSGA-III-inspired MaOGA also utilizes non-dominated sorting and niching with reference lines to further encourage steady and stable exploration and uniformly lower the overall evaluation values.","Our sequence generation experiments for a complex product (36 parts) demonstrated that the proposed method can consistently produce feasible and stable sequences with a 100% success rate, bringing the multiple preferred conditions closer to the optimal solution required for semi-automated robotic disassembly operations."],"url":"http://arxiv.org/abs/2401.01817v1"}
{"created":"2024-01-03 16:15:57","title":"Large Language Models Relearn Removed Concepts","abstract":"Advances in model editing through neuron pruning hold promise for removing undesirable concepts from large language models. However, it remains unclear whether models have the capacity to reacquire pruned concepts after editing. To investigate this, we evaluate concept relearning in models by tracking concept saliency and similarity in pruned neurons during retraining. Our findings reveal that models can quickly regain performance post-pruning by relocating advanced concepts to earlier layers and reallocating pruned concepts to primed neurons with similar semantics. This demonstrates that models exhibit polysemantic capacities and can blend old and new concepts in individual neurons. While neuron pruning provides interpretability into model concepts, our results highlight the challenges of permanent concept removal for improved model \\textit{safety}. Monitoring concept reemergence and developing techniques to mitigate relearning of unsafe concepts will be important directions for more robust model editing. Overall, our work strongly demonstrates the resilience and fluidity of concept representations in LLMs post concept removal.","sentences":["Advances in model editing through neuron pruning hold promise for removing undesirable concepts from large language models.","However, it remains unclear whether models have the capacity to reacquire pruned concepts after editing.","To investigate this, we evaluate concept relearning in models by tracking concept saliency and similarity in pruned neurons during retraining.","Our findings reveal that models can quickly regain performance post-pruning by relocating advanced concepts to earlier layers and reallocating pruned concepts to primed neurons with similar semantics.","This demonstrates that models exhibit polysemantic capacities and can blend old and new concepts in individual neurons.","While neuron pruning provides interpretability into model concepts, our results highlight the challenges of permanent concept removal for improved model \\textit{safety}.","Monitoring concept reemergence and developing techniques to mitigate relearning of unsafe concepts will be important directions for more robust model editing.","Overall, our work strongly demonstrates the resilience and fluidity of concept representations in LLMs post concept removal."],"url":"http://arxiv.org/abs/2401.01814v1"}
{"created":"2024-01-03 16:15:22","title":"Signal Processing in the Retina: Interpretable Graph Classifier to Predict Ganglion Cell Responses","abstract":"It is a popular hypothesis in neuroscience that ganglion cells in the retina are activated by selectively detecting visual features in an observed scene. While ganglion cell firings can be predicted via data-trained deep neural nets, the networks remain indecipherable, thus providing little understanding of the cells' underlying operations. To extract knowledge from the cell firings, in this paper we learn an interpretable graph-based classifier from data to predict the firings of ganglion cells in response to visual stimuli. Specifically, we learn a positive semi-definite (PSD) metric matrix $\\mathbf{M} \\succeq 0$ that defines Mahalanobis distances between graph nodes (visual events) endowed with pre-computed feature vectors; the computed inter-node distances lead to edge weights and a combinatorial graph that is amenable to binary classification. Mathematically, we define the objective of metric matrix $\\mathbf{M}$ optimization using a graph adaptation of large margin nearest neighbor (LMNN), which is rewritten as a semi-definite programming (SDP) problem. We solve it efficiently via a fast approximation called Gershgorin disc perfect alignment (GDPA) linearization. The learned metric matrix $\\mathbf{M}$ provides interpretability: important features are identified along $\\mathbf{M}$'s diagonal, and their mutual relationships are inferred from off-diagonal terms. Our fast metric learning framework can be applied to other biological systems with pre-chosen features that require interpretation.","sentences":["It is a popular hypothesis in neuroscience that ganglion cells in the retina are activated by selectively detecting visual features in an observed scene.","While ganglion cell firings can be predicted via data-trained deep neural nets, the networks remain indecipherable, thus providing little understanding of the cells' underlying operations.","To extract knowledge from the cell firings, in this paper we learn an interpretable graph-based classifier from data to predict the firings of ganglion cells in response to visual stimuli.","Specifically, we learn a positive semi-definite (PSD) metric matrix $\\mathbf{M} \\succeq 0$ that defines Mahalanobis distances between graph nodes (visual events) endowed with pre-computed feature vectors; the computed inter-node distances lead to edge weights and a combinatorial graph that is amenable to binary classification.","Mathematically, we define the objective of metric matrix $\\mathbf{M}$ optimization using a graph adaptation of large margin nearest neighbor (LMNN), which is rewritten as a semi-definite programming (SDP) problem.","We solve it efficiently via a fast approximation called Gershgorin disc perfect alignment (GDPA) linearization.","The learned metric matrix $\\mathbf{M}$ provides interpretability: important features are identified along $\\mathbf{M}$'s diagonal, and their mutual relationships are inferred from off-diagonal terms.","Our fast metric learning framework can be applied to other biological systems with pre-chosen features that require interpretation."],"url":"http://arxiv.org/abs/2401.01813v1"}
{"created":"2024-01-03 16:10:07","title":"aMUSEd: An Open MUSE Reproduction","abstract":"We present aMUSEd, an open-source, lightweight masked image model (MIM) for text-to-image generation based on MUSE. With 10 percent of MUSE's parameters, aMUSEd is focused on fast image generation. We believe MIM is under-explored compared to latent diffusion, the prevailing approach for text-to-image generation. Compared to latent diffusion, MIM requires fewer inference steps and is more interpretable. Additionally, MIM can be fine-tuned to learn additional styles with only a single image. We hope to encourage further exploration of MIM by demonstrating its effectiveness on large-scale text-to-image generation and releasing reproducible training code. We also release checkpoints for two models which directly produce images at 256x256 and 512x512 resolutions.","sentences":["We present aMUSEd, an open-source, lightweight masked image model (MIM) for text-to-image generation based on MUSE.","With 10 percent of MUSE's parameters, aMUSEd is focused on fast image generation.","We believe MIM is under-explored compared to latent diffusion, the prevailing approach for text-to-image generation.","Compared to latent diffusion, MIM requires fewer inference steps and is more interpretable.","Additionally, MIM can be fine-tuned to learn additional styles with only a single image.","We hope to encourage further exploration of MIM by demonstrating its effectiveness on large-scale text-to-image generation and releasing reproducible training code.","We also release checkpoints for two models which directly produce images at 256x256 and 512x512 resolutions."],"url":"http://arxiv.org/abs/2401.01808v1"}
{"created":"2024-01-03 15:59:35","title":"A quatum inspired neural network for geometric modeling","abstract":"By conceiving physical systems as 3D many-body point clouds, geometric graph neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased promising performance. In particular, their effective message-passing mechanics make them adept at modeling molecules and crystalline materials. However, current geometric GNNs only offer a mean-field approximation of the many-body system, encapsulated within two-body message passing, thus falling short in capturing intricate relationships within these geometric graphs. To address this limitation, tensor networks, widely employed by computational physics to handle manybody systems using high-order tensors, have been introduced. Nevertheless, integrating these tensorized networks into the message-passing framework of GNNs faces scalability and symmetry conservation (e.g., permutation and rotation) challenges. In response, we introduce an innovative equivariant Matrix Product State (MPS)-based message-passing strategy, through achieving an efficient implementation of the tensor contraction operation. Our method effectively models complex many-body relationships, suppressing mean-field approximations, and captures symmetries within geometric graphs. Importantly, it seamlessly replaces the standard message-passing and layer-aggregation modules intrinsic to geometric GNNs. We empirically validate the superior accuracy of our approach on benchmark tasks, including predicting classical Newton systems and quantum tensor Hamiltonian matrices. To our knowledge, our approach represents the inaugural utilization of parameterized geometric tensor networks.","sentences":["By conceiving physical systems as 3D many-body point clouds, geometric graph neural networks (GNNs), such as SE(3)/E(3) equivalent GNNs, have showcased promising performance.","In particular, their effective message-passing mechanics make them adept at modeling molecules and crystalline materials.","However, current geometric GNNs only offer a mean-field approximation of the many-body system, encapsulated within two-body message passing, thus falling short in capturing intricate relationships within these geometric graphs.","To address this limitation, tensor networks, widely employed by computational physics to handle manybody systems using high-order tensors, have been introduced.","Nevertheless, integrating these tensorized networks into the message-passing framework of GNNs faces scalability and symmetry conservation (e.g., permutation and rotation) challenges.","In response, we introduce an innovative equivariant Matrix Product State (MPS)-based message-passing strategy, through achieving an efficient implementation of the tensor contraction operation.","Our method effectively models complex many-body relationships, suppressing mean-field approximations, and captures symmetries within geometric graphs.","Importantly, it seamlessly replaces the standard message-passing and layer-aggregation modules intrinsic to geometric GNNs.","We empirically validate the superior accuracy of our approach on benchmark tasks, including predicting classical Newton systems and quantum tensor Hamiltonian matrices.","To our knowledge, our approach represents the inaugural utilization of parameterized geometric tensor networks."],"url":"http://arxiv.org/abs/2401.01801v1"}
{"created":"2024-01-03 15:56:37","title":"Gradient-Based Optimization of Lattice Quantizers","abstract":"Lattices with minimal normalized second moments are designed using a new numerical optimization algorithm. Starting from a random lower-triangular generator matrix and applying stochastic gradient descent, all elements are updated towards the negative gradient, which makes it the most efficient algorithm proposed so far for this purpose. A graphical illustration of the theta series, called theta image, is introduced and shown to be a powerful tool for converting numerical lattice representations into their underlying exact forms. As a proof of concept, optimized lattices are designed in dimensions up to 16. In all dimensions, the algorithm converges to either the previously best known lattice or a better one. The dual of the 15-dimensional laminated lattice is conjectured to be optimal in its dimension.","sentences":["Lattices with minimal normalized second moments are designed using a new numerical optimization algorithm.","Starting from a random lower-triangular generator matrix and applying stochastic gradient descent, all elements are updated towards the negative gradient, which makes it the most efficient algorithm proposed so far for this purpose.","A graphical illustration of the theta series, called theta image, is introduced and shown to be a powerful tool for converting numerical lattice representations into their underlying exact forms.","As a proof of concept, optimized lattices are designed in dimensions up to 16.","In all dimensions, the algorithm converges to either the previously best known lattice or a better one.","The dual of the 15-dimensional laminated lattice is conjectured to be optimal in its dimension."],"url":"http://arxiv.org/abs/2401.01799v1"}
{"created":"2024-01-03 15:50:43","title":"Understanding engagement with platform safety technology for reducing exposure to online harms","abstract":"User facing 'platform safety technology' encompasses an array of tools offered by platforms to help people protect themselves from harm, for example allowing people to report content and unfollow or block other users. These tools are an increasingly important part of online safety: in the UK, legislation has made it a requirement for large platforms to offer them. However, little is known about user engagement with such tools. We present findings from a nationally representative survey of UK adults covering their awareness of and experiences with seven common safety technologies. We show that experience of online harms is widespread, with 67% of people having seen what they perceived as harmful content online; 26% of people have also had at least one piece of content removed by content moderation. Use of safety technologies is also high, with more than 80\\% of people having used at least one. Awareness of specific tools is varied, with people more likely to be aware of 'post-hoc' safety tools, such as reporting, than preventative measures. However, satisfaction with safety technologies is generally low. People who have previously seen online harms are more likely to use safety tools, implying a 'learning the hard way' route to engagement. Those higher in digital literacy are also more likely to use some of these tools, raising concerns about the accessibility of these technologies to all users. Additionally, women are more likely to engage in particular types of online 'safety work'. We discuss the implications of our results for those seeking a safer online environment.","sentences":["User facing 'platform safety technology' encompasses an array of tools offered by platforms to help people protect themselves from harm, for example allowing people to report content and unfollow or block other users.","These tools are an increasingly important part of online safety: in the UK, legislation has made it a requirement for large platforms to offer them.","However, little is known about user engagement with such tools.","We present findings from a nationally representative survey of UK adults covering their awareness of and experiences with seven common safety technologies.","We show that experience of online harms is widespread, with 67% of people having seen what they perceived as harmful content online; 26% of people have also had at least one piece of content removed by content moderation.","Use of safety technologies is also high, with more than 80\\% of people having used at least one.","Awareness of specific tools is varied, with people more likely to be aware of 'post-hoc' safety tools, such as reporting, than preventative measures.","However, satisfaction with safety technologies is generally low.","People who have previously seen online harms are more likely to use safety tools, implying a 'learning the hard way' route to engagement.","Those higher in digital literacy are also more likely to use some of these tools, raising concerns about the accessibility of these technologies to all users.","Additionally, women are more likely to engage in particular types of online 'safety work'.","We discuss the implications of our results for those seeking a safer online environment."],"url":"http://arxiv.org/abs/2401.01796v1"}
{"created":"2024-01-03 15:45:23","title":"Moonshot: Optimizing Chain-Based Rotating Leader BFT via Optimistic Proposals","abstract":"Existing chain-based rotating leader BFT SMR protocols for the partially synchronous network model that commit blocks with $O(1)$ minimum latency have block periods of at least $2\\delta$ (where $\\delta$ is the message transmission latency). While a protocol with a block period of $\\delta$ exists under the synchronous model, its minimum commit latency is linear in the size of the system.   To close this gap, we present the first chain-based BFT SMR protocols with best-case delays of $\\delta$ between the proposals of distinct honest leaders, and minimum commit latencies of $3\\delta$. We present three protocols for the partially synchronous network model under different notions of optimistic responsiveness, two of which implement pipelining and one of which does not. All of our protocols achieve reorg resilience and two have short view lengths; properties that many existing chain-based BFT SMR protocols lack. We experimentally evaluate our protocols and show that they achieve significant increases in throughput and reductions in latency compared to the state-of-the-art, Jolteon. Our results also demonstrate that techniques commonly employed to reduce communication complexity$\\unicode{x2014}$such as vote-pipelining and the use of designated vote-aggregators$\\unicode{x2014}$actually reduce practical performance in many settings.","sentences":["Existing chain-based rotating leader BFT SMR protocols for the partially synchronous network model that commit blocks with $O(1)$ minimum latency have block periods of at least $2\\delta$ (where $\\delta$ is the message transmission latency).","While a protocol with a block period of $\\delta$ exists under the synchronous model, its minimum commit latency is linear in the size of the system.   ","To close this gap, we present the first chain-based BFT SMR protocols with best-case delays of $\\delta$ between the proposals of distinct honest leaders, and minimum commit latencies of $3\\delta$. We present three protocols for the partially synchronous network model under different notions of optimistic responsiveness, two of which implement pipelining and one of which does not.","All of our protocols achieve reorg resilience and two have short view lengths; properties that many existing chain-based BFT SMR protocols lack.","We experimentally evaluate our protocols and show that they achieve significant increases in throughput and reductions in latency compared to the state-of-the-art, Jolteon.","Our results also demonstrate that techniques commonly employed to reduce communication complexity$\\unicode{x2014}$such as vote-pipelining and the use of designated vote-aggregators$\\unicode{x2014}$actually reduce practical performance in many settings."],"url":"http://arxiv.org/abs/2401.01791v1"}
{"created":"2024-01-03 15:36:33","title":"Applications of machine learning and IoT for Outdoor Air Pollution Monitoring and Prediction: A Systematic Literature Review","abstract":"According to the World Health Organization (WHO), air pollution kills seven million people every year. Outdoor air pollution is a major environmental health problem affecting low, middle, and high-income countries. In the past few years, the research community has explored IoT-enabled machine learning applications for outdoor air pollution prediction. The general objective of this paper is to systematically review applications of machine learning and Internet of Things (IoT) for outdoor air pollution prediction and the combination of monitoring sensors and input features used. Two research questions were formulated for this review. 1086 publications were collected in the initial PRISMA stage. After the screening and eligibility phases, 37 papers were selected for inclusion. A cost-based analysis was conducted on the findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled prediction. Three methods of prediction were identified: time series, feature-based and spatio-temporal. This review's findings identify major limitations in applications found in the literature, namely lack of coverage, lack of diversity of data and lack of inclusion of context-specific features. This review proposes directions for future research and underlines practical implications in healthcare, urban planning, global synergy and smart cities.","sentences":["According to the World Health Organization (WHO), air pollution kills seven million people every year.","Outdoor air pollution is a major environmental health problem affecting low, middle, and high-income countries.","In the past few years, the research community has explored IoT-enabled machine learning applications for outdoor air pollution prediction.","The general objective of this paper is to systematically review applications of machine learning and Internet of Things (IoT) for outdoor air pollution prediction and the combination of monitoring sensors and input features used.","Two research questions were formulated for this review.","1086 publications were collected in the initial PRISMA stage.","After the screening and eligibility phases, 37 papers were selected for inclusion.","A cost-based analysis was conducted on the findings to highlight high-cost monitoring, low-cost IoT and hybrid enabled prediction.","Three methods of prediction were identified: time series, feature-based and spatio-temporal.","This review's findings identify major limitations in applications found in the literature, namely lack of coverage, lack of diversity of data and lack of inclusion of context-specific features.","This review proposes directions for future research and underlines practical implications in healthcare, urban planning, global synergy and smart cities."],"url":"http://arxiv.org/abs/2401.01788v1"}
{"created":"2024-01-03 15:34:26","title":"Impact of RIS on Outage Probability and Ergodic Rate in Wireless Powered Communication","abstract":"Wireless powered communication (WPC) combines information and energy transmission for energy-constrained nodes. Reconfigurable intelligent surfaces (RISs) are capable of controlling radio signals in a dynamic and goal-oriented manner. This paper investigates the combination of RIS and WPC to enhance the performance of an energy-constrained user. Using an RIS, a base station, and a wireless user transmit energy and information signals, respectively. We derive closed-form expressions for outage probability and secrecy rate to analyze the performance of the proposed framework. Based on the theoretical analysis and simulation results, valuable insights are revealed and parameter selection is demonstrated.","sentences":["Wireless powered communication (WPC) combines information and energy transmission for energy-constrained nodes.","Reconfigurable intelligent surfaces (RISs) are capable of controlling radio signals in a dynamic and goal-oriented manner.","This paper investigates the combination of RIS and WPC to enhance the performance of an energy-constrained user.","Using an RIS, a base station, and a wireless user transmit energy and information signals, respectively.","We derive closed-form expressions for outage probability and secrecy rate to analyze the performance of the proposed framework.","Based on the theoretical analysis and simulation results, valuable insights are revealed and parameter selection is demonstrated."],"url":"http://arxiv.org/abs/2401.01787v1"}
{"created":"2024-01-03 15:27:10","title":"An experimental sorting method for improving metagenomic data encoding","abstract":"Minimizing data storage poses a significant challenge in large-scale metagenomic projects. In this paper, we present a new method for improving the encoding of FASTQ files generated by metagenomic sequencing. This method incorporates metagenomic classification followed by a recursive filter for clustering reads by DNA sequence similarity to improve the overall reference-free compression. In the results, we show an overall improvement in the compression of several datasets. As hypothesized, we show a progressive compression gain for higher coverage depth and number of identified species. Additionally, we provide an implementation that is freely available at https://github.com/cobilab/mizar and can be customized to work with other FASTQ compression tools.","sentences":["Minimizing data storage poses a significant challenge in large-scale metagenomic projects.","In this paper, we present a new method for improving the encoding of FASTQ files generated by metagenomic sequencing.","This method incorporates metagenomic classification followed by a recursive filter for clustering reads by DNA sequence similarity to improve the overall reference-free compression.","In the results, we show an overall improvement in the compression of several datasets.","As hypothesized, we show a progressive compression gain for higher coverage depth and number of identified species.","Additionally, we provide an implementation that is freely available at https://github.com/cobilab/mizar and can be customized to work with other FASTQ compression tools."],"url":"http://arxiv.org/abs/2401.01786v1"}
{"created":"2024-01-03 15:15:00","title":"Profiling the carbon footprint of performance bugs","abstract":"Much debate nowadays is devoted to the impacts of modern information and communication technology on global carbon emissions. Green information and communication technology is a paradigm creating a sustainable and environmentally friendly computing field that tries to minimize the adverse effects on the environment. Green information and communication technology are under constant development nowadays. Thus, in this paper, we undertake the problem of performance bugs that, until recently, have never been studied so profoundly. We assume that inappropriate software implementations can have a crucial influence on global carbon emissions. Here, we classify those performance bugs and develop inappropriate implementations of four programs written in C++. To mitigate these simulated performance bugs, measuring software and hardware methods that can estimate the increased carbon footprint properly were proposed.","sentences":["Much debate nowadays is devoted to the impacts of modern information and communication technology on global carbon emissions.","Green information and communication technology is a paradigm creating a sustainable and environmentally friendly computing field that tries to minimize the adverse effects on the environment.","Green information and communication technology are under constant development nowadays.","Thus, in this paper, we undertake the problem of performance bugs that, until recently, have never been studied so profoundly.","We assume that inappropriate software implementations can have a crucial influence on global carbon emissions.","Here, we classify those performance bugs and develop inappropriate implementations of four programs written in C++.","To mitigate these simulated performance bugs, measuring software and hardware methods that can estimate the increased carbon footprint properly were proposed."],"url":"http://arxiv.org/abs/2401.01782v1"}
{"created":"2024-01-03 15:14:11","title":"Evaluating Trustworthiness of Online News Publishers via Article Classification","abstract":"The proliferation of low-quality online information in today's era has underscored the need for robust and automatic mechanisms to evaluate the trustworthiness of online news publishers. In this paper, we analyse the trustworthiness of online news media outlets by leveraging a dataset of 4033 news stories from 40 different sources. We aim to infer the trustworthiness level of the source based on the classification of individual articles' content. The trust labels are obtained from NewsGuard, a journalistic organization that evaluates news sources using well-established editorial and publishing criteria. The results indicate that the classification model is highly effective in classifying the trustworthiness levels of the news articles. This research has practical applications in alerting readers to potentially untrustworthy news sources, assisting journalistic organizations in evaluating new or unfamiliar media outlets and supporting the selection of articles for their trustworthiness assessment.","sentences":["The proliferation of low-quality online information in today's era has underscored the need for robust and automatic mechanisms to evaluate the trustworthiness of online news publishers.","In this paper, we analyse the trustworthiness of online news media outlets by leveraging a dataset of 4033 news stories from 40 different sources.","We aim to infer the trustworthiness level of the source based on the classification of individual articles' content.","The trust labels are obtained from NewsGuard, a journalistic organization that evaluates news sources using well-established editorial and publishing criteria.","The results indicate that the classification model is highly effective in classifying the trustworthiness levels of the news articles.","This research has practical applications in alerting readers to potentially untrustworthy news sources, assisting journalistic organizations in evaluating new or unfamiliar media outlets and supporting the selection of articles for their trustworthiness assessment."],"url":"http://arxiv.org/abs/2401.01781v1"}
{"created":"2024-01-03 15:12:42","title":"Navigating Uncertainty: Optimizing API Dependency for Hallucination Reduction in Closed-Book Question Answering","abstract":"While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination. Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers. Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information. However, searching in a large collection of documents introduces additional computational/time costs. An optimal behavior would be to query external resources only when the LLM is not confident about answers. In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool. We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task. In addition, we propose to leverage parameter-efficient fine-tuning techniques to train our model on a small amount of data. Our model directly provides answers for $78.2\\%$ of the known queries and opts to search for $77.2\\%$ of the unknown ones. This results in the API being utilized only $62\\%$ of the time.","sentences":["While Large Language Models (LLM) are able to accumulate and restore knowledge, they are still prone to hallucination.","Especially when faced with factual questions, LLM cannot only rely on knowledge stored in parameters to guarantee truthful and correct answers.","Augmenting these models with the ability to search on external information sources, such as the web, is a promising approach to ground knowledge to retrieve information.","However, searching in a large collection of documents introduces additional computational/time costs.","An optimal behavior would be to query external resources only when the LLM is not confident about answers.","In this paper, we propose a new LLM able to self-estimate if it is able to answer directly or needs to request an external tool.","We investigate a supervised approach by introducing a hallucination masking mechanism in which labels are generated using a close book question-answering task.","In addition, we propose to leverage parameter-efficient fine-tuning techniques to train our model on a small amount of data.","Our model directly provides answers for $78.2\\%$ of the known queries and opts to search for $77.2\\%$ of the unknown ones.","This results in the API being utilized only $62\\%$ of the time."],"url":"http://arxiv.org/abs/2401.01780v1"}
{"created":"2024-01-03 15:11:17","title":"Lossy Compression of Individual Sequences Revisited: Fundamental Limits of Finite-State Encoders","abstract":"We extend Ziv and Lempel's model of finite-state encoders to the realm of lossy compression of individual sequences. In particular, the model of the encoder includes a finite-state reconstruction codebook followed by an information lossless finite-state encoder that compresses the reconstruction codeword with no additional distortion. We first derive two different lower bounds to the compression ratio that depend on the number of states of the lossless encoder. Both bounds are asymptotically achievable by conceptually simple coding schemes. We then show that when the number of states of the lossless encoder is large enough in terms of the reconstruction block-length, the performance can be improved, sometimes significantly so. In particular, the improved performance is achievable using a random-coding ensemble that is universal, not only in terms of the source sequence, but also in terms of the distortion measure.","sentences":["We extend Ziv and Lempel's model of finite-state encoders to the realm of lossy compression of individual sequences.","In particular, the model of the encoder includes a finite-state reconstruction codebook followed by an information lossless finite-state encoder that compresses the reconstruction codeword with no additional distortion.","We first derive two different lower bounds to the compression ratio that depend on the number of states of the lossless encoder.","Both bounds are asymptotically achievable by conceptually simple coding schemes.","We then show that when the number of states of the lossless encoder is large enough in terms of the reconstruction block-length, the performance can be improved, sometimes significantly so.","In particular, the improved performance is achievable using a random-coding ensemble that is universal, not only in terms of the source sequence, but also in terms of the distortion measure."],"url":"http://arxiv.org/abs/2401.01779v1"}
{"created":"2024-01-03 14:52:18","title":"A Novel Paradigm for Neural Computation: X-Net with Learnable Neurons and Adaptable Structure","abstract":"Artificial neural networks (ANNs) have permeated various disciplinary domains, ranging from bioinformatics to financial analytics, where their application has become an indispensable facet of contemporary scientific research endeavors. However, the inherent limitations of traditional neural networks arise due to their relatively fixed network structures and activation functions. 1, The type of activation function is single and relatively fixed, which leads to poor \"unit representation ability\" of the network, and it is often used to solve simple problems with very complex networks; 2, the network structure is not adaptive, it is easy to cause network structure redundant or insufficient. To address the aforementioned issues, this study proposes a novel neural network called X-Net. By utilizing our designed Alternating Backpropagation mechanism, X-Net dynamically selects appropriate activation functions based on derivative information during training to enhance the network's representation capability for specific tasks. Simultaneously, it accurately adjusts the network structure at the neuron level to accommodate tasks of varying complexities and reduce computational costs. Through a series of experiments, we demonstrate the dual advantages of X-Net in terms of reducing model size and improving representation power. Specifically, in terms of the number of parameters, X-Net is only 3$\\%$ of baselines on average, and only 1.4$\\%$ under some tasks. In terms of representation ability, X-Net can achieve an average $R^2$=0.985 on the fitting task by only optimizing the activation function without introducing any parameters. Finally, we also tested the ability of X-Net to help scientific discovery on data from multiple disciplines such as society, energy, environment, and aerospace, and achieved concise and good results.","sentences":["Artificial neural networks (ANNs) have permeated various disciplinary domains, ranging from bioinformatics to financial analytics, where their application has become an indispensable facet of contemporary scientific research endeavors.","However, the inherent limitations of traditional neural networks arise due to their relatively fixed network structures and activation functions.","1, The type of activation function is single and relatively fixed, which leads to poor \"unit representation ability\" of the network, and it is often used to solve simple problems with very complex networks; 2, the network structure is not adaptive, it is easy to cause network structure redundant or insufficient.","To address the aforementioned issues, this study proposes a novel neural network called X-Net.","By utilizing our designed Alternating Backpropagation mechanism, X-Net dynamically selects appropriate activation functions based on derivative information during training to enhance the network's representation capability for specific tasks.","Simultaneously, it accurately adjusts the network structure at the neuron level to accommodate tasks of varying complexities and reduce computational costs.","Through a series of experiments, we demonstrate the dual advantages of X-Net in terms of reducing model size and improving representation power.","Specifically, in terms of the number of parameters, X-Net is only 3$\\%$ of baselines on average, and only 1.4$\\%$ under some tasks.","In terms of representation ability, X-Net can achieve an average $R^2$=0.985 on the fitting task by only optimizing the activation function without introducing any parameters.","Finally, we also tested the ability of X-Net to help scientific discovery on data from multiple disciplines such as society, energy, environment, and aerospace, and achieved concise and good results."],"url":"http://arxiv.org/abs/2401.01772v1"}
{"created":"2024-01-03 14:32:47","title":"Multichannel blind speech source separation with a disjoint constraint source model","abstract":"Multichannel convolutive blind speech source separation refers to the problem of separating different speech sources from the observed multichannel mixtures without much a priori information about the mixing system. Multichannel nonnegative matrix factorization (MNMF) has been proven to be one of the most powerful separation frameworks and the representative algorithms such as MNMF and the independent low-rank matrix analysis (ILRMA) have demonstrated great performance. However, the sparseness properties of speech source signals are not fully taken into account in such a framework. It is well known that speech signals are sparse in nature, which is considered in this work to improve the separation performance. Specifically, we utilize the Bingham and Laplace distributions to formulate a disjoint constraint regularizer, which is subsequently incorporated into both MNMF and ILRMA. We then derive majorization-minimization rules for updating parameters related to the source model, resulting in the development of two enhanced algorithms: s-MNMF and s-ILRMA. Comprehensive simulations are conducted, and the results unequivocally demonstrate the efficacy of our proposed methodologies.","sentences":["Multichannel convolutive blind speech source separation refers to the problem of separating different speech sources from the observed multichannel mixtures without much a priori information about the mixing system.","Multichannel nonnegative matrix factorization (MNMF) has been proven to be one of the most powerful separation frameworks and the representative algorithms such as MNMF and the independent low-rank matrix analysis (ILRMA) have demonstrated great performance.","However, the sparseness properties of speech source signals are not fully taken into account in such a framework.","It is well known that speech signals are sparse in nature, which is considered in this work to improve the separation performance.","Specifically, we utilize the Bingham and Laplace distributions to formulate a disjoint constraint regularizer, which is subsequently incorporated into both MNMF and ILRMA.","We then derive majorization-minimization rules for updating parameters related to the source model, resulting in the development of two enhanced algorithms: s-MNMF and s-ILRMA.","Comprehensive simulations are conducted, and the results unequivocally demonstrate the efficacy of our proposed methodologies."],"url":"http://arxiv.org/abs/2401.01763v1"}
{"created":"2024-01-03 14:32:38","title":"Independent low-rank matrix analysis based on the Sinkhorn divergence source model for blind source separation","abstract":"The so-called independent low-rank matrix analysis (ILRMA) has demonstrated a great potential for dealing with the problem of determined blind source separation (BSS) for audio and speech signals. This method assumes that the spectra from different frequency bands are independent and the spectral coefficients in any frequency band are Gaussian distributed. The Itakura-Saito divergence is then employed to estimate the source model related parameters. In reality, however, the spectral coefficients from different frequency bands may be dependent, which is not considered in the existing ILRMA algorithm. This paper presents an improved version of ILRMA, which considers the dependency between the spectral coefficients from different frequency bands. The Sinkhorn divergence is then exploited to optimize the source model parameters. As a result of using the cross-band information, the BSS performance is improved. But the number of parameters to be estimated also increases significantly, and so is the computational complexity. To reduce the algorithm complexity, we apply the Kronecker product to decompose the modeling matrix into the product of a number of matrices of much smaller dimensionality. An efficient algorithm is then developed to implement the Sinkhorn divergence based BSS algorithm and the complexity is reduced by an order of magnitude.","sentences":["The so-called independent low-rank matrix analysis (ILRMA) has demonstrated a great potential for dealing with the problem of determined blind source separation (BSS) for audio and speech signals.","This method assumes that the spectra from different frequency bands are independent and the spectral coefficients in any frequency band are Gaussian distributed.","The Itakura-Saito divergence is then employed to estimate the source model related parameters.","In reality, however, the spectral coefficients from different frequency bands may be dependent, which is not considered in the existing ILRMA algorithm.","This paper presents an improved version of ILRMA, which considers the dependency between the spectral coefficients from different frequency bands.","The Sinkhorn divergence is then exploited to optimize the source model parameters.","As a result of using the cross-band information, the BSS performance is improved.","But the number of parameters to be estimated also increases significantly, and so is the computational complexity.","To reduce the algorithm complexity, we apply the Kronecker product to decompose the modeling matrix into the product of a number of matrices of much smaller dimensionality.","An efficient algorithm is then developed to implement the Sinkhorn divergence based BSS algorithm and the complexity is reduced by an order of magnitude."],"url":"http://arxiv.org/abs/2401.01762v1"}
{"created":"2024-01-03 14:28:55","title":"Cross-target Stance Detection by Exploiting Target Analytical Perspectives","abstract":"Cross-target stance detection (CTSD) is an important task, which infers the attitude of the destination target by utilizing annotated data derived from the source target. One important approach in CTSD is to extract domain-invariant features to bridge the knowledge gap between multiple targets. However, the analysis of informal and short text structure, and implicit expressions, complicate the extraction of domain-invariant knowledge. In this paper, we propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the analysis perspective as a bridge to transfer knowledge. First, we develop a two-stage instruct-based chain-of-thought method (TsCoT) to elicit target analysis perspectives and provide natural language explanations (NLEs) from multiple viewpoints by formulating instructions based on large language model (LLM). Second, we propose a multi-perspective prompt-tuning framework (MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments results demonstrate the superiority of MPPT against the state-of-the-art baseline methods.","sentences":["Cross-target stance detection (CTSD) is an important task, which infers the attitude of the destination target by utilizing annotated data derived from the source target.","One important approach in CTSD is to extract domain-invariant features to bridge the knowledge gap between multiple targets.","However, the analysis of informal and short text structure, and implicit expressions, complicate the extraction of domain-invariant knowledge.","In this paper, we propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the analysis perspective as a bridge to transfer knowledge.","First, we develop a two-stage instruct-based chain-of-thought method (TsCoT) to elicit target analysis perspectives and provide natural language explanations (NLEs) from multiple viewpoints by formulating instructions based on large language model (LLM).","Second, we propose a multi-perspective prompt-tuning framework (MultiPLN) to fuse the NLEs into the stance predictor.","Extensive experiments results demonstrate the superiority of MPPT against the state-of-the-art baseline methods."],"url":"http://arxiv.org/abs/2401.01761v1"}
{"created":"2024-01-03 14:24:02","title":"VGA: Vision and Graph Fused Attention Network for Rumor Detection","abstract":"With the development of social media, rumors have been spread broadly on social media platforms, causing great harm to society. Beside textual information, many rumors also use manipulated images or conceal textual information within images to deceive people and avoid being detected, making multimodal rumor detection be a critical problem. The majority of multimodal rumor detection methods mainly concentrate on extracting features of source claims and their corresponding images, while ignoring the comments of rumors and their propagation structures. These comments and structures imply the wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these methods usually only extract visual features in a basic manner, seldom consider tampering or textual information in images. Therefore, in this study, we propose a novel Vision and Graph Fused Attention Network (VGA) for rumor detection to utilize propagation structures among posts so as to obtain the crowd opinions and further explore visual tampering features, as well as the textual information hidden in images. We conduct extensive experiments on three datasets, demonstrating that VGA can effectively detect multimodal rumors and outperform state-of-the-art methods significantly.","sentences":["With the development of social media, rumors have been spread broadly on social media platforms, causing great harm to society.","Beside textual information, many rumors also use manipulated images or conceal textual information within images to deceive people and avoid being detected, making multimodal rumor detection be a critical problem.","The majority of multimodal rumor detection methods mainly concentrate on extracting features of source claims and their corresponding images, while ignoring the comments of rumors and their propagation structures.","These comments and structures imply the wisdom of crowds and are proved to be crucial to debunk rumors.","Moreover, these methods usually only extract visual features in a basic manner, seldom consider tampering or textual information in images.","Therefore, in this study, we propose a novel Vision and Graph Fused Attention Network (VGA) for rumor detection to utilize propagation structures among posts so as to obtain the crowd opinions and further explore visual tampering features, as well as the textual information hidden in images.","We conduct extensive experiments on three datasets, demonstrating that VGA can effectively detect multimodal rumors and outperform state-of-the-art methods significantly."],"url":"http://arxiv.org/abs/2401.01759v1"}
{"created":"2024-01-03 14:19:04","title":"Fuzzy Logic Controller Design for Mobile Robot Outdoor Navigation","abstract":"Many researchers around the world are researching to get control solutions that enhance robots' ability to navigate in dynamic environments autonomously. However, until these days robots have limited capability and many navigation tasks on Earth and other planets have been difficult so far. This paperwork presents the development of a control system for a differential drive-wheeled mobile robot that autonomously controls its position, heading, and speed based on destination information given and surrounding data gathered through mounted proximity and GPS sensors. The intelligence of this control system is implemented by using a fuzzy logic algorithm which is a very powerful tool to handle un-modeled systems like the dynamically changing environment dealt with in this research. The fuzzy controller is used to address the problems associated with navigation in an obstacle-strewn environment. Such issues include position estimation, path planning, and obstacle avoidance. In this study modeling, design, and simulation of the system have been done. The simulation result shows that the developed mobile robot travels successfully from any location to the destination location without colliding with obstacles.","sentences":["Many researchers around the world are researching to get control solutions that enhance robots' ability to navigate in dynamic environments autonomously.","However, until these days robots have limited capability and many navigation tasks on Earth and other planets have been difficult so far.","This paperwork presents the development of a control system for a differential drive-wheeled mobile robot that autonomously controls its position, heading, and speed based on destination information given and surrounding data gathered through mounted proximity and GPS sensors.","The intelligence of this control system is implemented by using a fuzzy logic algorithm which is a very powerful tool to handle un-modeled systems like the dynamically changing environment dealt with in this research.","The fuzzy controller is used to address the problems associated with navigation in an obstacle-strewn environment.","Such issues include position estimation, path planning, and obstacle avoidance.","In this study modeling, design, and simulation of the system have been done.","The simulation result shows that the developed mobile robot travels successfully from any location to the destination location without colliding with obstacles."],"url":"http://arxiv.org/abs/2401.01756v1"}
{"created":"2024-01-03 14:17:35","title":"Incremental FastPitch: Chunk-based High Quality Text to Speech","abstract":"Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process compared with conventional auto-regressive models. Although parallel models have benefits in many aspects, they become naturally unfit for incremental synthesis due to their fully parallel architecture such as transformer. In this work, we propose Incremental FastPitch, a novel FastPitch variant capable of incrementally producing high-quality Mel chunks by improving the architecture with chunk-based FFT blocks, training with receptive-field constrained chunk attention masks, and inference with fixed size past model states. Experimental results show that our proposal can produce speech quality comparable to the parallel FastPitch, with a significant lower latency that allows even lower response time for real-time speech applications.","sentences":["Parallel text-to-speech models have been widely applied for real-time speech synthesis, and they offer more controllability and a much faster synthesis process compared with conventional auto-regressive models.","Although parallel models have benefits in many aspects, they become naturally unfit for incremental synthesis due to their fully parallel architecture such as transformer.","In this work, we propose Incremental FastPitch, a novel FastPitch variant capable of incrementally producing high-quality Mel chunks by improving the architecture with chunk-based FFT blocks, training with receptive-field constrained chunk attention masks, and inference with fixed size past model states.","Experimental results show that our proposal can produce speech quality comparable to the parallel FastPitch, with a significant lower latency that allows even lower response time for real-time speech applications."],"url":"http://arxiv.org/abs/2401.01755v1"}
{"created":"2024-01-03 14:15:25","title":"Using AI/ML to Find and Remediate Enterprise Secrets in Code & Document Sharing Platforms","abstract":"We introduce a new challenge to the software development community: 1) leveraging AI to accurately detect and flag up secrets in code and on popular document sharing platforms that frequently used by developers, such as Confluence and 2) automatically remediating the detections (e.g. by suggesting password vault functionality). This is a challenging, and mostly unaddressed task. Existing methods leverage heuristics and regular expressions, that can be very noisy, and therefore increase toil on developers. The next step - modifying code itself - to automatically remediate a detection, is a complex task. We introduce two baseline AI models that have good detection performance and propose an automatic mechanism for remediating secrets found in code, opening up the study of this task to the wider community.","sentences":["We introduce a new challenge to the software development community: 1) leveraging AI to accurately detect and flag up secrets in code and on popular document sharing platforms that frequently used by developers, such as Confluence and 2) automatically remediating the detections (e.g. by suggesting password vault functionality).","This is a challenging, and mostly unaddressed task.","Existing methods leverage heuristics and regular expressions, that can be very noisy, and therefore increase toil on developers.","The next step - modifying code itself - to automatically remediate a detection, is a complex task.","We introduce two baseline AI models that have good detection performance and propose an automatic mechanism for remediating secrets found in code, opening up the study of this task to the wider community."],"url":"http://arxiv.org/abs/2401.01754v1"}
{"created":"2024-01-03 14:13:24","title":"A Generative AI Assistant to Accelerate Cloud Migration","abstract":"We present a tool that leverages generative AI to accelerate the migration of on-premises applications to the cloud. The Cloud Migration LLM accepts input from the user specifying the parameters of their migration, and outputs a migration strategy with an architecture diagram. A user study suggests that the migration LLM can assist inexperienced users in finding the right cloud migration profile, while avoiding complexities of a manual approach.","sentences":["We present a tool that leverages generative AI to accelerate the migration of on-premises applications to the cloud.","The Cloud Migration LLM accepts input from the user specifying the parameters of their migration, and outputs a migration strategy with an architecture diagram.","A user study suggests that the migration LLM can assist inexperienced users in finding the right cloud migration profile, while avoiding complexities of a manual approach."],"url":"http://arxiv.org/abs/2401.01753v1"}
{"created":"2024-01-03 14:08:39","title":"FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision Transformers","abstract":"In recent years, the Vision Transformer (ViT) model has gradually become mainstream in various computer vision tasks, and the robustness of the model has received increasing attention. However, existing large models tend to prioritize performance during training, potentially neglecting the robustness, which may lead to serious security concerns. In this paper, we establish a new challenge: exploring how to use a small number of additional parameters for adversarial finetuning to quickly and effectively enhance the adversarial robustness of a standardly trained model. To address this challenge, we develop the novel LNLoRA module, incorporating a learnable layer normalization before the conventional LoRA module, which helps mitigate magnitude differences in parameters between the adversarial and standard training paradigms.   Furthermore, we propose the FullLoRA-AT framework by integrating the learnable LNLoRA modules into all key components of ViT-based models while keeping the pretrained model frozen, which can significantly improve the model robustness via adversarial finetuning in a parameter-efficient manner.   Extensive experiments on CIFAR-10, CIFAR-100, and Imagenette demonstrate the superiority of our proposed FullLoRA-AT framework. It achieves comparable robustness with full finetuning while only requiring about 5% of the learnable parameters. This also effectively addresses concerns regarding extra model storage space and enormous training time caused by adversarial finetuning.","sentences":["In recent years, the Vision Transformer (ViT) model has gradually become mainstream in various computer vision tasks, and the robustness of the model has received increasing attention.","However, existing large models tend to prioritize performance during training, potentially neglecting the robustness, which may lead to serious security concerns.","In this paper, we establish a new challenge: exploring how to use a small number of additional parameters for adversarial finetuning to quickly and effectively enhance the adversarial robustness of a standardly trained model.","To address this challenge, we develop the novel LNLoRA module, incorporating a learnable layer normalization before the conventional LoRA module, which helps mitigate magnitude differences in parameters between the adversarial and standard training paradigms.   ","Furthermore, we propose the FullLoRA-AT framework by integrating the learnable LNLoRA modules into all key components of ViT-based models while keeping the pretrained model frozen, which can significantly improve the model robustness via adversarial finetuning in a parameter-efficient manner.   ","Extensive experiments on CIFAR-10, CIFAR-100, and Imagenette demonstrate the superiority of our proposed FullLoRA-AT framework.","It achieves comparable robustness with full finetuning while only requiring about 5% of the learnable parameters.","This also effectively addresses concerns regarding extra model storage space and enormous training time caused by adversarial finetuning."],"url":"http://arxiv.org/abs/2401.01752v1"}
{"created":"2024-01-03 14:06:06","title":"Text mining arXiv: a look through quantitative finance papers","abstract":"This paper explores articles hosted on the arXiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research. Employing text mining techniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arXiv from 1997 to 2022. We extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain. Additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches.","sentences":["This paper explores articles hosted on the arXiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research.","Employing text mining techniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arXiv from 1997 to 2022.","We extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain.","Additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches."],"url":"http://arxiv.org/abs/2401.01751v1"}
{"created":"2024-01-03 13:58:35","title":"Towards Robust Semantic Segmentation against Patch-based Attack via Attention Refinement","abstract":"The attention mechanism has been proven effective on various visual tasks in recent years. In the semantic segmentation task, the attention mechanism is applied in various methods, including the case of both Convolution Neural Networks (CNN) and Vision Transformer (ViT) as backbones. However, we observe that the attention mechanism is vulnerable to patch-based adversarial attacks. Through the analysis of the effective receptive field, we attribute it to the fact that the wide receptive field brought by global attention may lead to the spread of the adversarial patch. To address this issue, in this paper, we propose a Robust Attention Mechanism (RAM) to improve the robustness of the semantic segmentation model, which can notably relieve the vulnerability against patch-based attacks. Compared to the vallina attention mechanism, RAM introduces two novel modules called Max Attention Suppression and Random Attention Dropout, both of which aim to refine the attention matrix and limit the influence of a single adversarial patch on the semantic segmentation results of other positions. Extensive experiments demonstrate the effectiveness of our RAM to improve the robustness of semantic segmentation models against various patch-based attack methods under different attack settings.","sentences":["The attention mechanism has been proven effective on various visual tasks in recent years.","In the semantic segmentation task, the attention mechanism is applied in various methods, including the case of both Convolution Neural Networks (CNN) and Vision Transformer (ViT) as backbones.","However, we observe that the attention mechanism is vulnerable to patch-based adversarial attacks.","Through the analysis of the effective receptive field, we attribute it to the fact that the wide receptive field brought by global attention may lead to the spread of the adversarial patch.","To address this issue, in this paper, we propose a Robust Attention Mechanism (RAM) to improve the robustness of the semantic segmentation model, which can notably relieve the vulnerability against patch-based attacks.","Compared to the vallina attention mechanism, RAM introduces two novel modules called Max Attention Suppression and Random Attention Dropout, both of which aim to refine the attention matrix and limit the influence of a single adversarial patch on the semantic segmentation results of other positions.","Extensive experiments demonstrate the effectiveness of our RAM to improve the robustness of semantic segmentation models against various patch-based attack methods under different attack settings."],"url":"http://arxiv.org/abs/2401.01750v1"}
{"created":"2024-01-03 13:57:09","title":"Few-shot Image Generation via Information Transfer from the Built Geodesic Surface","abstract":"Images generated by most of generative models trained with limited data often exhibit deficiencies in either fidelity, diversity, or both. One effective solution to address the limitation is few-shot generative model adaption. However, the type of approaches typically rely on a large-scale pre-trained model, serving as a source domain, to facilitate information transfer to the target domain. In this paper, we propose a method called Information Transfer from the Built Geodesic Surface (ITBGS), which contains two module: Feature Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization (I\\&R). With the FAGS module, a pseudo-source domain is created by projecting image features from the training dataset into the Pre-Shape Space, subsequently generating new features on the Geodesic surface. Thus, no pre-trained models is needed for the adaption process during the training of generative models with FAGS. I\\&R module are introduced for supervising the interpolated images and regularizing their relative distances, respectively, to further enhance the quality of generated images. Through qualitative and quantitative experiments, we demonstrate that the proposed method consistently achieves optimal or comparable results across a diverse range of semantically distinct datasets, even in extremely few-shot scenarios.","sentences":["Images generated by most of generative models trained with limited data often exhibit deficiencies in either fidelity, diversity, or both.","One effective solution to address the limitation is few-shot generative model adaption.","However, the type of approaches typically rely on a large-scale pre-trained model, serving as a source domain, to facilitate information transfer to the target domain.","In this paper, we propose a method called Information Transfer from the Built Geodesic Surface (ITBGS), which contains two module: Feature Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization (I\\&R).","With the FAGS module, a pseudo-source domain is created by projecting image features from the training dataset into the Pre-Shape Space, subsequently generating new features on the Geodesic surface.","Thus, no pre-trained models is needed for the adaption process during the training of generative models with FAGS.","I\\&R module are introduced for supervising the interpolated images and regularizing their relative distances, respectively, to further enhance the quality of generated images.","Through qualitative and quantitative experiments, we demonstrate that the proposed method consistently achieves optimal or comparable results across a diverse range of semantically distinct datasets, even in extremely few-shot scenarios."],"url":"http://arxiv.org/abs/2401.01749v1"}
{"created":"2024-01-03 13:37:01","title":"Minimizing the Weighted Number of Tardy Jobs is W[1]-hard","abstract":"We consider the $1||\\sum w_J U_j$ problem, the problem of minimizing the weighted number of tardy jobs on a single machine. This problem is one of the most basic and fundamental problems in scheduling theory, with several different applications both in theory and practice. We prove that $1||\\sum w_J U_j$ is W[1]-hard with respect to the number $p_{\\#}$ of different processing times in the input, as well as with respect to the number $w_{\\#}$ of different weights in the input. This, along with previous work, provides a complete picture for $1||\\sum w_J U_j$ from the perspective of parameterized complexity, as well as almost tight complexity bounds for the problem under the Exponential Time Hypothesis (ETH).","sentences":["We consider the $1||\\sum w_J U_j$ problem, the problem of minimizing the weighted number of tardy jobs on a single machine.","This problem is one of the most basic and fundamental problems in scheduling theory, with several different applications both in theory and practice.","We prove that $1||\\sum w_J U_j$ is W[1]-hard with respect to the number $p_{\\#}$ of different processing times in the input, as well as with respect to the number $w_{\\#}$ of different weights in the input.","This, along with previous work, provides a complete picture for $1||\\sum w_J U_j$ from the perspective of parameterized complexity, as well as almost tight complexity bounds for the problem under the Exponential Time Hypothesis (ETH)."],"url":"http://arxiv.org/abs/2401.01740v1"}
{"created":"2024-01-03 13:36:51","title":"A Soft Continuum Robot with Self-Controllable Variable Curvature","abstract":"This paper introduces a new type of soft continuum robot, called SCoReS, which is capable of self-controlling continuously its curvature at the segment level; in contrast to previous designs which either require external forces or machine elements, or whose variable curvature capabilities are discrete -- depending on the number of locking mechanisms and segments. The ability to have a variable curvature, whose control is continuous and independent from external factors, makes a soft continuum robot more adaptive in constrained environments, similar to what is observed in nature in the elephant's trunk or ostrich's neck for instance which exhibit multiple curvatures. To this end, our soft continuum robot enables reconfigurable variable curvatures utilizing a variable stiffness growing spine based on micro-particle granular jamming for the first time. We detail the design of the proposed robot, presenting its modeling through beam theory and FEA simulation -- which is validated through experiments. The robot's versatile bending profiles are then explored in experiments and an application to grasp fruits at different configurations is demonstrated.","sentences":["This paper introduces a new type of soft continuum robot, called SCoReS, which is capable of self-controlling continuously its curvature at the segment level; in contrast to previous designs which either require external forces or machine elements, or whose variable curvature capabilities are discrete -- depending on the number of locking mechanisms and segments.","The ability to have a variable curvature, whose control is continuous and independent from external factors, makes a soft continuum robot more adaptive in constrained environments, similar to what is observed in nature in the elephant's trunk or ostrich's neck for instance which exhibit multiple curvatures.","To this end, our soft continuum robot enables reconfigurable variable curvatures utilizing a variable stiffness growing spine based on micro-particle granular jamming for the first time.","We detail the design of the proposed robot, presenting its modeling through beam theory and FEA simulation -- which is validated through experiments.","The robot's versatile bending profiles are then explored in experiments and an application to grasp fruits at different configurations is demonstrated."],"url":"http://arxiv.org/abs/2401.01739v1"}
{"created":"2024-01-03 13:33:28","title":"Integrated Sensing and Communication with Massive MIMO: A Unified Tensor Approach for Channel and Target Parameter Estimation","abstract":"Benefitting from the vast spatial degrees of freedom, the amalgamation of integrated sensing and communication (ISAC) and massive multiple-input multiple-output (MIMO) is expected to simultaneously improve spectral and energy efficiencies as well as the sensing capability. However, a large number of antennas deployed in massive MIMO-ISAC raises critical challenges in acquiring both accurate channel state information and target parameter information. To overcome these two challenges with a unified framework, we first analyze their underlying system models and then propose a novel tensor-based approach that addresses both the channel estimation and target sensing problems. Specifically, by parameterizing the high-dimensional communication channel exploiting a small number of physical parameters, we associate the channel state information with the sensing parameters of targets in terms of angular, delay, and Doppler dimensions. Then, we propose a shared training pattern adopting the same time-frequency resources such that both the channel estimation and target parameter estimation can be formulated as a canonical polyadic decomposition problem with a similar mathematical expression. On this basis, we first investigate the uniqueness condition of the tensor factorization and the maximum number of resolvable targets by utilizing the specific Vandermonde","sentences":["Benefitting from the vast spatial degrees of freedom, the amalgamation of integrated sensing and communication (ISAC) and massive multiple-input multiple-output (MIMO) is expected to simultaneously improve spectral and energy efficiencies as well as the sensing capability.","However, a large number of antennas deployed in massive MIMO-ISAC raises critical challenges in acquiring both accurate channel state information and target parameter information.","To overcome these two challenges with a unified framework, we first analyze their underlying system models and then propose a novel tensor-based approach that addresses both the channel estimation and target sensing problems.","Specifically, by parameterizing the high-dimensional communication channel exploiting a small number of physical parameters, we associate the channel state information with the sensing parameters of targets in terms of angular, delay, and Doppler dimensions.","Then, we propose a shared training pattern adopting the same time-frequency resources such that both the channel estimation and target parameter estimation can be formulated as a canonical polyadic decomposition problem with a similar mathematical expression.","On this basis, we first investigate the uniqueness condition of the tensor factorization and the maximum number of resolvable targets by utilizing the specific Vandermonde"],"url":"http://arxiv.org/abs/2401.01738v1"}
{"created":"2024-01-03 13:19:14","title":"Few-shot Adaptation of Multi-modal Foundation Models: A Survey","abstract":"Multi-modal (vision-language) models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of visual foundation models. These models with robust and aligned semantic representations learned from billions of internet image-text pairs and can be applied to various downstream tasks in a zero-shot manner. However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired. Consequently, many researchers have begun to explore few-shot adaptation methods for these models, gradually deriving three main technical approaches: 1) prompt-based methods, 2) adapter-based methods, and 3) external knowledge-based methods. Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress. Therefore, in this survey, we introduce and analyze the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods. In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models. The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size. Based on this, we propose three possible solutions from the following aspects: 1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive knowledge utilization.","sentences":["Multi-modal (vision-language) models, such as CLIP, are replacing traditional supervised pre-training models (e.g., ImageNet-based pre-training) as the new generation of visual foundation models.","These models with robust and aligned semantic representations learned from billions of internet image-text pairs and can be applied to various downstream tasks in a zero-shot manner.","However, in some fine-grained domains like medical imaging and remote sensing, the performance of multi-modal foundation models often leaves much to be desired.","Consequently, many researchers have begun to explore few-shot adaptation methods for these models, gradually deriving three main technical approaches: 1) prompt-based methods, 2) adapter-based methods, and 3) external knowledge-based methods.","Nevertheless, this rapidly developing field has produced numerous results without a comprehensive survey to systematically organize the research progress.","Therefore, in this survey, we introduce and analyze the research advancements in few-shot adaptation methods for multi-modal models, summarizing commonly used datasets and experimental setups, and comparing the results of different methods.","In addition, due to the lack of reliable theoretical support for existing methods, we derive the few-shot adaptation generalization error bound for multi-modal models.","The theorem reveals that the generalization error of multi-modal foundation models is constrained by three factors: domain gap, model capacity, and sample size.","Based on this, we propose three possible solutions from the following aspects: 1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive knowledge utilization."],"url":"http://arxiv.org/abs/2401.01736v1"}
{"created":"2024-01-03 13:18:24","title":"Economics Arena for Large Language Models","abstract":"Large language models (LLMs) have been extensively used as the backbones for general-purpose agents, and some economics literature suggest that LLMs are capable of playing various types of economics games. Following these works, to overcome the limitation of evaluating LLMs using static benchmarks, we propose to explore competitive games as an evaluation for LLMs to incorporate multi-players and dynamicise the environment. By varying the game history revealed to LLMs-based players, we find that most of LLMs are rational in that they play strategies that can increase their payoffs, but not as rational as indicated by Nash Equilibria (NEs). Moreover, when game history are available, certain types of LLMs, such as GPT-4, can converge faster to the NE strategies, which suggests higher rationality level in comparison to other models. In the meantime, certain types of LLMs can win more often when game history are available, and we argue that the winning rate reflects the reasoning ability with respect to the strategies of other players. Throughout all our experiments, we observe that the ability to strictly follow the game rules described by natural languages also vary among the LLMs we tested. In this work, we provide an economics arena for the LLMs research community as a dynamic simulation to test the above-mentioned abilities of LLMs, i.e. rationality, strategic reasoning ability, and instruction-following capability.","sentences":["Large language models (LLMs) have been extensively used as the backbones for general-purpose agents, and some economics literature suggest that LLMs are capable of playing various types of economics games.","Following these works, to overcome the limitation of evaluating LLMs using static benchmarks, we propose to explore competitive games as an evaluation for LLMs to incorporate multi-players and dynamicise the environment.","By varying the game history revealed to LLMs-based players, we find that most of LLMs are rational in that they play strategies that can increase their payoffs, but not as rational as indicated by Nash Equilibria (NEs).","Moreover, when game history are available, certain types of LLMs, such as GPT-4, can converge faster to the NE strategies, which suggests higher rationality level in comparison to other models.","In the meantime, certain types of LLMs can win more often when game history are available, and we argue that the winning rate reflects the reasoning ability with respect to the strategies of other players.","Throughout all our experiments, we observe that the ability to strictly follow the game rules described by natural languages also vary among the LLMs we tested.","In this work, we provide an economics arena for the LLMs research community as a dynamic simulation to test the above-mentioned abilities of LLMs, i.e. rationality, strategic reasoning ability, and instruction-following capability."],"url":"http://arxiv.org/abs/2401.01735v1"}
{"created":"2024-01-03 13:16:38","title":"Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data","abstract":"Assistive robots should be able to wash, fold or iron clothes. However, due to the variety, deformability and self-occlusions of clothes, creating general-purpose robot systems for cloth manipulation is challenging. Synthetic data is a promising direction to improve generalization, though its usability is often limited by the sim-to-real gap. To advance the use of synthetic data for cloth manipulation and to enable tasks such as robotic folding, we present a synthetic data pipeline to train keypoint detectors for almost flattened cloth items. To test its performance, we have also collected a real-world dataset. We train detectors for both T-shirts, towels and shorts and obtain an average precision of 64.3%. Fine-tuning on real-world data improves performance to 74.2%. Additional insight is provided by discussing various failure modes of the keypoint detectors and by comparing different approaches to obtain cloth meshes and materials. We also quantify the remaining sim-to-real gap and argue that further improvements to the fidelity of cloth assets will be required to further reduce this gap. The code, dataset and trained models are available online.","sentences":["Assistive robots should be able to wash, fold or iron clothes.","However, due to the variety, deformability and self-occlusions of clothes, creating general-purpose robot systems for cloth manipulation is challenging.","Synthetic data is a promising direction to improve generalization, though its usability is often limited by the sim-to-real gap.","To advance the use of synthetic data for cloth manipulation and to enable tasks such as robotic folding, we present a synthetic data pipeline to train keypoint detectors for almost flattened cloth items.","To test its performance, we have also collected a real-world dataset.","We train detectors for both T-shirts, towels and shorts and obtain an average precision of 64.3%.","Fine-tuning on real-world data improves performance to 74.2%.","Additional insight is provided by discussing various failure modes of the keypoint detectors and by comparing different approaches to obtain cloth meshes and materials.","We also quantify the remaining sim-to-real gap and argue that further improvements to the fidelity of cloth assets will be required to further reduce this gap.","The code, dataset and trained models are available online."],"url":"http://arxiv.org/abs/2401.01734v1"}
{"created":"2024-01-03 13:12:04","title":"Investigating the Suitability of Concept Drift Detection for Detecting Leakages in Water Distribution Networks","abstract":"Leakages are a major risk in water distribution networks as they cause water loss and increase contamination risks. Leakage detection is a difficult task due to the complex dynamics of water distribution networks. In particular, small leakages are hard to detect. From a machine-learning perspective, leakages can be modeled as concept drift. Thus, a wide variety of drift detection schemes seems to be a suitable choice for detecting leakages. In this work, we explore the potential of model-loss-based and distribution-based drift detection methods to tackle leakage detection. We additionally discuss the issue of temporal dependencies in the data and propose a way to cope with it when applying distribution-based detection. We evaluate different methods systematically for leakages of different sizes and detection times. Additionally, we propose a first drift-detection-based technique for localizing leakages.","sentences":["Leakages are a major risk in water distribution networks as they cause water loss and increase contamination risks.","Leakage detection is a difficult task due to the complex dynamics of water distribution networks.","In particular, small leakages are hard to detect.","From a machine-learning perspective, leakages can be modeled as concept drift.","Thus, a wide variety of drift detection schemes seems to be a suitable choice for detecting leakages.","In this work, we explore the potential of model-loss-based and distribution-based drift detection methods to tackle leakage detection.","We additionally discuss the issue of temporal dependencies in the data and propose a way to cope with it when applying distribution-based detection.","We evaluate different methods systematically for leakages of different sizes and detection times.","Additionally, we propose a first drift-detection-based technique for localizing leakages."],"url":"http://arxiv.org/abs/2401.01733v1"}
{"created":"2024-01-03 13:11:59","title":"Task and Explanation Network","abstract":"Explainability in deep networks has gained increased importance in recent years. We argue herein that an AI must be tasked not just with a task but also with an explanation of why said task was accomplished as such. We present a basic framework -- Task and Explanation Network (TENet) -- which fully integrates task completion and its explanation. We believe that the field of AI as a whole should insist -- quite emphatically -- on explainability.","sentences":["Explainability in deep networks has gained increased importance in recent years.","We argue herein that an AI must be tasked not just with a task but also with an explanation of why said task was accomplished as such.","We present a basic framework -- Task and Explanation Network (TENet) -- which fully integrates task completion and its explanation.","We believe that the field of AI as a whole should insist -- quite emphatically -- on explainability."],"url":"http://arxiv.org/abs/2401.01732v1"}
{"created":"2024-01-03 13:07:14","title":"STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment Fusion","abstract":"The recovery of 3D human mesh from monocular images has significantly been developed in recent years. However, existing models usually ignore spatial and temporal information, which might lead to mesh and image misalignment and temporal discontinuity. For this reason, we propose a novel Spatio-Temporal Alignment Fusion (STAF) model. As a video-based model, it leverages coherence clues from human motion by an attention-based Temporal Coherence Fusion Module (TCFM). As for spatial mesh-alignment evidence, we extract fine-grained local information through predicted mesh projection on the feature maps. Based on the spatial features, we further introduce a multi-stage adjacent Spatial Alignment Fusion Module (SAFM) to enhance the feature representation of the target frame. In addition to the above, we propose an Average Pooling Module (APM) to allow the model to focus on the entire input sequence rather than just the target frame. This method can remarkably improve the smoothness of recovery results from video. Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the superiority of STAF. We achieve a state-of-the-art trade-off between precision and smoothness. Our code and more video results are on the project page https://yw0208.github.io/staf/","sentences":["The recovery of 3D human mesh from monocular images has significantly been developed in recent years.","However, existing models usually ignore spatial and temporal information, which might lead to mesh and image misalignment and temporal discontinuity.","For this reason, we propose a novel Spatio-Temporal Alignment Fusion (STAF) model.","As a video-based model, it leverages coherence clues from human motion by an attention-based Temporal Coherence Fusion Module (TCFM).","As for spatial mesh-alignment evidence, we extract fine-grained local information through predicted mesh projection on the feature maps.","Based on the spatial features, we further introduce a multi-stage adjacent Spatial Alignment Fusion Module (SAFM) to enhance the feature representation of the target frame.","In addition to the above, we propose an Average Pooling Module (APM) to allow the model to focus on the entire input sequence rather than just the target frame.","This method can remarkably improve the smoothness of recovery results from video.","Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the superiority of STAF.","We achieve a state-of-the-art trade-off between precision and smoothness.","Our code and more video results are on the project page https://yw0208.github.io/staf/"],"url":"http://arxiv.org/abs/2401.01730v1"}
{"created":"2024-01-03 13:07:07","title":"Ravnest: Decentralized Asynchronous Training on Heterogeneous Devices","abstract":"Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets. This trend is expected to continue. However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales. This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics. Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model. These clusters engage in $\\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively execute global parameter averaging across all clusters. We have framed our asynchronous SGD loss function as a block structured optimization problem with delayed updates and derived an optimal convergence rate of $O\\left(\\frac{1}{\\sqrt{K}}\\right)$. We further discuss linear speedup with respect to the number of participating clusters and the bound on the staleness parameter.","sentences":["Modern deep learning models, growing larger and more complex, have demonstrated exceptional generalization and accuracy due to training on huge datasets.","This trend is expected to continue.","However, the increasing size of these models poses challenges in training, as traditional centralized methods are limited by memory constraints at such scales.","This paper proposes an asynchronous decentralized training paradigm for large modern deep learning models that harnesses the compute power of regular heterogeneous PCs with limited resources connected across the internet to achieve favourable performance metrics.","Ravnest facilitates decentralized training by efficiently organizing compute nodes into clusters with similar data transfer rates and compute capabilities, without necessitating that each node hosts the entire model.","These clusters engage in $\\textit{Zero-Bubble Asynchronous Model Parallel}$ training, and a $\\textit{Parallel Multi-Ring All-Reduce}$ method is employed to effectively execute global parameter averaging across all clusters.","We have framed our asynchronous SGD loss function as a block structured optimization problem with delayed updates and derived an optimal convergence rate of $O\\left(\\frac{1}{\\sqrt{K}}\\right)$. We further discuss linear speedup with respect to the number of participating clusters and the bound on the staleness parameter."],"url":"http://arxiv.org/abs/2401.01728v1"}
{"created":"2024-01-03 13:03:44","title":"Lightweight Adaptive Feature De-drifting for Compressed Image Classification","abstract":"JPEG is a widely used compression scheme to efficiently reduce the volume of transmitted images. The artifacts appear among blocks due to the information loss, which not only affects the quality of images but also harms the subsequent high-level tasks in terms of feature drifting. High-level vision models trained on high-quality images will suffer performance degradation when dealing with compressed images, especially on mobile devices. Numerous learning-based JPEG artifact removal methods have been proposed to handle visual artifacts. However, it is not an ideal choice to use these JPEG artifact removal methods as a pre-processing for compressed image classification for the following reasons: 1. These methods are designed for human vision rather than high-level vision models; 2. These methods are not efficient enough to serve as pre-processing on resource-constrained devices. To address these issues, this paper proposes a novel lightweight AFD module to boost the performance of pre-trained image classification models when facing compressed images. First, a FDE-Net is devised to generate the spatial-wise FDM in the DCT domain. Next, the estimated FDM is transmitted to the FE-Net to generate the mapping relationship between degraded features and corresponding high-quality features. A simple but effective RepConv block equipped with structural re-parameterization is utilized in FE-Net, which enriches feature representation in the training phase while maintaining efficiency in the deployment phase. After training on limited compressed images, the AFD-Module can serve as a \"plug-and-play\" model for pre-trained classification models to improve their performance on compressed images. Experiments demonstrate that our proposed AFD module can comprehensively improve the accuracy of the pre-trained classification models and significantly outperform the existing methods.","sentences":["JPEG is a widely used compression scheme to efficiently reduce the volume of transmitted images.","The artifacts appear among blocks due to the information loss, which not only affects the quality of images but also harms the subsequent high-level tasks in terms of feature drifting.","High-level vision models trained on high-quality images will suffer performance degradation when dealing with compressed images, especially on mobile devices.","Numerous learning-based JPEG artifact removal methods have been proposed to handle visual artifacts.","However, it is not an ideal choice to use these JPEG artifact removal methods as a pre-processing for compressed image classification for the following reasons:","1. These methods are designed for human vision rather than high-level vision models; 2.","These methods are not efficient enough to serve as pre-processing on resource-constrained devices.","To address these issues, this paper proposes a novel lightweight AFD module to boost the performance of pre-trained image classification models when facing compressed images.","First, a FDE-Net is devised to generate the spatial-wise FDM in the DCT domain.","Next, the estimated FDM is transmitted to the FE-Net to generate the mapping relationship between degraded features and corresponding high-quality features.","A simple but effective RepConv block equipped with structural re-parameterization is utilized in FE-Net, which enriches feature representation in the training phase while maintaining efficiency in the deployment phase.","After training on limited compressed images, the AFD-Module can serve as a \"plug-and-play\" model for pre-trained classification models to improve their performance on compressed images.","Experiments demonstrate that our proposed AFD module can comprehensively improve the accuracy of the pre-trained classification models and significantly outperform the existing methods."],"url":"http://arxiv.org/abs/2401.01724v1"}
{"created":"2024-01-03 12:55:24","title":"Limited Feedback on Measurements: Sharing a Codebook or a Generative Model?","abstract":"Discrete Fourier transform (DFT) codebook-based solutions are well-established for limited feedback schemes in frequency division duplex (FDD) systems. In recent years, data-aided solutions have been shown to achieve higher performance, enabled by the adaptivity of the feedback scheme to the propagation environment of the base station (BS) cell. In particular, a versatile limited feedback scheme utilizing Gaussian mixture models (GMMs) was recently introduced. The scheme supports multi-user communications, exhibits low complexity, supports parallelization, and offers significant flexibility concerning various system parameters. Conceptually, a GMM captures environment knowledge and is subsequently transferred to the mobile terminals (MTs) for online inference of feedback information. Afterward, the BS designs precoders using either directional information or a generative modeling-based approach. A major shortcoming of recent works is that the assessed system performance is only evaluated through synthetic simulation data that is generally unable to fully characterize the features of real-world environments. It raises the question of how the GMM-based feedback scheme performs on real-world measurement data, especially compared to the well-established DFT-based solution. Our experiments reveal that the GMM-based feedback scheme tremendously improves the system performance measured in terms of sum-rate, allowing to deploy systems with fewer pilots or feedback bits.","sentences":["Discrete Fourier transform (DFT) codebook-based solutions are well-established for limited feedback schemes in frequency division duplex (FDD) systems.","In recent years, data-aided solutions have been shown to achieve higher performance, enabled by the adaptivity of the feedback scheme to the propagation environment of the base station (BS) cell.","In particular, a versatile limited feedback scheme utilizing Gaussian mixture models (GMMs) was recently introduced.","The scheme supports multi-user communications, exhibits low complexity, supports parallelization, and offers significant flexibility concerning various system parameters.","Conceptually, a GMM captures environment knowledge and is subsequently transferred to the mobile terminals (MTs) for online inference of feedback information.","Afterward, the BS designs precoders using either directional information or a generative modeling-based approach.","A major shortcoming of recent works is that the assessed system performance is only evaluated through synthetic simulation data that is generally unable to fully characterize the features of real-world environments.","It raises the question of how the GMM-based feedback scheme performs on real-world measurement data, especially compared to the well-established DFT-based solution.","Our experiments reveal that the GMM-based feedback scheme tremendously improves the system performance measured in terms of sum-rate, allowing to deploy systems with fewer pilots or feedback bits."],"url":"http://arxiv.org/abs/2401.01721v1"}
{"created":"2024-01-03 12:54:31","title":"Local Adaptive Clustering Based Image Matching for Automatic Visual Identification","abstract":"Monitoring cameras are extensively utilized in industrial production to monitor equipment running. With advancements in computer vision, device recognition using image features is viable. This paper presents a vision-assisted identification system that implements real-time automatic equipment labeling through image matching in surveillance videos. The system deploys the ORB algorithm to extract image features and the GMS algorithm to remove incorrect matching points. According to the principles of clustering and template locality, a method known as Local Adaptive Clustering (LAC) has been established to enhance label positioning. This method segments matching templates using the cluster center, which improves the efficiency and stability of labels. The experimental results demonstrate that LAC effectively curtails the label drift.","sentences":["Monitoring cameras are extensively utilized in industrial production to monitor equipment running.","With advancements in computer vision, device recognition using image features is viable.","This paper presents a vision-assisted identification system that implements real-time automatic equipment labeling through image matching in surveillance videos.","The system deploys the ORB algorithm to extract image features and the GMS algorithm to remove incorrect matching points.","According to the principles of clustering and template locality, a method known as Local Adaptive Clustering (LAC) has been established to enhance label positioning.","This method segments matching templates using the cluster center, which improves the efficiency and stability of labels.","The experimental results demonstrate that LAC effectively curtails the label drift."],"url":"http://arxiv.org/abs/2401.01720v1"}
{"created":"2024-01-03 12:47:02","title":"Fact-checking based fake news detection: a review","abstract":"This paper reviews and summarizes the research results on fact-based fake news from the perspectives of tasks and problems, algorithm strategies, and datasets. First, the paper systematically explains the task definition and core problems of fact-based fake news detection. Second, the paper summarizes the existing detection methods based on the algorithm principles. Third, the paper analyzes the classic and newly proposed datasets in the field, and summarizes the experimental results on each dataset. Finally, the paper summarizes the advantages and disadvantages of existing methods, proposes several challenges that methods in this field may face, and looks forward to the next stage of research. It is hoped that this paper will provide reference for subsequent work in the field.","sentences":["This paper reviews and summarizes the research results on fact-based fake news from the perspectives of tasks and problems, algorithm strategies, and datasets.","First, the paper systematically explains the task definition and core problems of fact-based fake news detection.","Second, the paper summarizes the existing detection methods based on the algorithm principles.","Third, the paper analyzes the classic and newly proposed datasets in the field, and summarizes the experimental results on each dataset.","Finally, the paper summarizes the advantages and disadvantages of existing methods, proposes several challenges that methods in this field may face, and looks forward to the next stage of research.","It is hoped that this paper will provide reference for subsequent work in the field."],"url":"http://arxiv.org/abs/2401.01717v1"}
{"created":"2024-01-03 12:28:33","title":"Evaluating Large Language Models in Semantic Parsing for Conversational Question Answering over Knowledge Graphs","abstract":"Conversational question answering systems often rely on semantic parsing to enable interactive information retrieval, which involves the generation of structured database queries from a natural language input. For information-seeking conversations about facts stored within a knowledge graph, dialogue utterances are transformed into graph queries in a process that is called knowledge-based conversational question answering. This paper evaluates the performance of large language models that have not been explicitly pre-trained on this task. Through a series of experiments on an extensive benchmark dataset, we compare models of varying sizes with different prompting techniques and identify common issue types in the generated output. Our results demonstrate that large language models are capable of generating graph queries from dialogues, with significant improvements achievable through few-shot prompting and fine-tuning techniques, especially for smaller models that exhibit lower zero-shot performance.","sentences":["Conversational question answering systems often rely on semantic parsing to enable interactive information retrieval, which involves the generation of structured database queries from a natural language input.","For information-seeking conversations about facts stored within a knowledge graph, dialogue utterances are transformed into graph queries in a process that is called knowledge-based conversational question answering.","This paper evaluates the performance of large language models that have not been explicitly pre-trained on this task.","Through a series of experiments on an extensive benchmark dataset, we compare models of varying sizes with different prompting techniques and identify common issue types in the generated output.","Our results demonstrate that large language models are capable of generating graph queries from dialogues, with significant improvements achievable through few-shot prompting and fine-tuning techniques, especially for smaller models that exhibit lower zero-shot performance."],"url":"http://arxiv.org/abs/2401.01711v1"}
{"created":"2024-01-03 12:25:18","title":"EPA: Neural Collapse Inspired Robust Out-of-Distribution Detector","abstract":"Out-of-distribution (OOD) detection plays a crucial role in ensuring the security of neural networks. Existing works have leveraged the fact that In-distribution (ID) samples form a subspace in the feature space, achieving state-of-the-art (SOTA) performance. However, the comprehensive characteristics of the ID subspace still leave under-explored. Recently, the discovery of Neural Collapse ($\\mathcal{NC}$) sheds light on novel properties of the ID subspace. Leveraging insight from $\\mathcal{NC}$, we observe that the Principal Angle between the features and the ID feature subspace forms a superior representation for measuring the likelihood of OOD. Building upon this observation, we propose a novel $\\mathcal{NC}$-inspired OOD scoring function, named Entropy-enhanced Principal Angle (EPA), which integrates both the global characteristic of the ID subspace and its inner property. We experimentally compare EPA with various SOTA approaches, validating its superior performance and robustness across different network architectures and OOD datasets.","sentences":["Out-of-distribution (OOD) detection plays a crucial role in ensuring the security of neural networks.","Existing works have leveraged the fact that In-distribution (ID) samples form a subspace in the feature space, achieving state-of-the-art (SOTA) performance.","However, the comprehensive characteristics of the ID subspace still leave under-explored.","Recently, the discovery of Neural Collapse ($\\mathcal{NC}$) sheds light on novel properties of the ID subspace.","Leveraging insight from $\\mathcal{NC}$, we observe that the Principal Angle between the features and the ID feature subspace forms a superior representation for measuring the likelihood of OOD.","Building upon this observation, we propose a novel $\\mathcal{NC}$-inspired OOD scoring function, named Entropy-enhanced Principal Angle (EPA), which integrates both the global characteristic of the ID subspace and its inner property.","We experimentally compare EPA with various SOTA approaches, validating its superior performance and robustness across different network architectures and OOD datasets."],"url":"http://arxiv.org/abs/2401.01710v1"}
{"created":"2024-01-03 12:09:43","title":"De-Hallucinator: Iterative Grounding for LLM-Based Code Completion","abstract":"Large languages models (LLMs) trained on datasets of publicly available source code have established a new state-of-the-art in code completion. However, these models are mostly unaware of the code that already exists within a specific project, preventing the models from making good use of existing APIs. Instead, LLMs often invent, or \"hallucinate\", non-existent APIs or produce variants of already existing code. Although the API information is available to IDEs, the input size limit of LLMs prevents code completion techniques from including all relevant context into the prompt. This paper presents De-Hallucinator, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt. The approach exploits the observation that LLMs often predict code that resembles the desired completion, but that fails to correctly refer to already existing APIs. De-Hallucinator automatically identifies project-specific API references related to the code prefix and to the model's initial predictions and adds these references into the prompt. Our evaluation applies the approach to the task of predicting API usages in open-source Python projects. We show that De-Hallucinator consistently improves the predicted code across four state-of-the-art LLMs compared to querying the model only with the code before the cursor. In particular, the approach improves the edit distance of the predicted code by 23-51% and the recall of correctly predicted API usages by 24-61% relative to the baseline.","sentences":["Large languages models (LLMs) trained on datasets of publicly available source code have established a new state-of-the-art in code completion.","However, these models are mostly unaware of the code that already exists within a specific project, preventing the models from making good use of existing APIs.","Instead, LLMs often invent, or \"hallucinate\", non-existent APIs or produce variants of already existing code.","Although the API information is available to IDEs, the input size limit of LLMs prevents code completion techniques from including all relevant context into the prompt.","This paper presents De-Hallucinator, an LLM-based code completion technique that grounds the predictions of a model through a novel combination of retrieving suitable API references and iteratively querying the model with increasingly suitable context information in the prompt.","The approach exploits the observation that LLMs often predict code that resembles the desired completion, but that fails to correctly refer to already existing APIs.","De-Hallucinator automatically identifies project-specific API references related to the code prefix and to the model's initial predictions and adds these references into the prompt.","Our evaluation applies the approach to the task of predicting API usages in open-source Python projects.","We show that De-Hallucinator consistently improves the predicted code across four state-of-the-art LLMs compared to querying the model only with the code before the cursor.","In particular, the approach improves the edit distance of the predicted code by 23-51% and the recall of correctly predicted API usages by 24-61% relative to the baseline."],"url":"http://arxiv.org/abs/2401.01701v1"}
{"created":"2024-01-03 12:06:02","title":"WordArt Designer API: User-Driven Artistic Typography Synthesis with Large Language Models on ModelScope","abstract":"This paper introduces the WordArt Designer API, a novel framework for user-driven artistic typography synthesis utilizing Large Language Models (LLMs) on ModelScope. We address the challenge of simplifying artistic typography for non-professionals by offering a dynamic, adaptive, and computationally efficient alternative to traditional rigid templates. Our approach leverages the power of LLMs to understand and interpret user input, facilitating a more intuitive design process. We demonstrate through various case studies how users can articulate their aesthetic preferences and functional requirements, which the system then translates into unique and creative typographic designs. Our evaluations indicate significant improvements in user satisfaction, design flexibility, and creative expression over existing systems. The WordArt Designer API not only democratizes the art of typography but also opens up new possibilities for personalized digital communication and design.","sentences":["This paper introduces the WordArt Designer API, a novel framework for user-driven artistic typography synthesis utilizing Large Language Models (LLMs) on ModelScope.","We address the challenge of simplifying artistic typography for non-professionals by offering a dynamic, adaptive, and computationally efficient alternative to traditional rigid templates.","Our approach leverages the power of LLMs to understand and interpret user input, facilitating a more intuitive design process.","We demonstrate through various case studies how users can articulate their aesthetic preferences and functional requirements, which the system then translates into unique and creative typographic designs.","Our evaluations indicate significant improvements in user satisfaction, design flexibility, and creative expression over existing systems.","The WordArt Designer API not only democratizes the art of typography but also opens up new possibilities for personalized digital communication and design."],"url":"http://arxiv.org/abs/2401.01699v1"}
{"created":"2024-01-03 12:05:38","title":"Patterns of Persistence and Diffusibility across World's Languages","abstract":"Language similarities can be caused by genetic relatedness, areal contact, universality, or chance. Colexification, i.e.~a type of similarity where a single lexical form is used to convey multiple meanings, is underexplored. In our work, we shed light on the linguistic causes of cross-lingual similarity in colexification and phonology, by exploring genealogical stability (persistence) and contact-induced change (diffusibility). We construct large-scale graphs incorporating semantic, genealogical, phonological and geographical data for 1,966 languages. We then show the potential of this resource, by investigating several established hypotheses from previous work in linguistics, while proposing new ones. Our results strongly support a previously established hypothesis in the linguistic literature, while offering contradicting evidence to another. Our large scale resource opens for further research across disciplines, e.g.~in multilingual NLP and comparative linguistics.","sentences":["Language similarities can be caused by genetic relatedness, areal contact, universality, or chance.","Colexification, i.e.~a type of similarity where a single lexical form is used to convey multiple meanings, is underexplored.","In our work, we shed light on the linguistic causes of cross-lingual similarity in colexification and phonology, by exploring genealogical stability (persistence) and contact-induced change (diffusibility).","We construct large-scale graphs incorporating semantic, genealogical, phonological and geographical data for 1,966 languages.","We then show the potential of this resource, by investigating several established hypotheses from previous work in linguistics, while proposing new ones.","Our results strongly support a previously established hypothesis in the linguistic literature, while offering contradicting evidence to another.","Our large scale resource opens for further research across disciplines, e.g.~in multilingual NLP and comparative linguistics."],"url":"http://arxiv.org/abs/2401.01698v1"}
{"created":"2024-01-03 11:54:48","title":"AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with Detail-Preserving Model-based Deep Learning","abstract":"Deep learning has shown great potential in accelerating diffusion tensor imaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise and detail loss in reconstructing the DTI-derived parametric maps especially when sparsely sampled q-space data are used. This paper proposes a novel method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to facilitate fast and accurate DTI with only six measurements. AID-DTI is equipped with a newly designed Singular Value Decomposition (SVD)-based regularizer, which can effectively capture fine details while suppressing noise during network training. Experimental results on Human Connectome Project (HCP) data consistently demonstrate that the proposed method estimates DTI parameter maps with fine-grained details and outperforms three state-of-the-art methods both quantitatively and qualitatively.","sentences":["Deep learning has shown great potential in accelerating diffusion tensor imaging (DTI).","Nevertheless, existing methods tend to suffer from Rician noise and detail loss in reconstructing the DTI-derived parametric maps especially when sparsely sampled q-space data are used.","This paper proposes a novel method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to facilitate fast and accurate DTI with only six measurements.","AID-DTI is equipped with a newly designed Singular Value Decomposition (SVD)-based regularizer, which can effectively capture fine details while suppressing noise during network training.","Experimental results on Human Connectome Project (HCP) data consistently demonstrate that the proposed method estimates DTI parameter maps with fine-grained details and outperforms three state-of-the-art methods both quantitatively and qualitatively."],"url":"http://arxiv.org/abs/2401.01693v1"}
{"created":"2024-01-03 11:54:30","title":"Predicting challenge moments from students' discourse: A comparison of GPT-4 to two traditional natural language processing approaches","abstract":"Effective collaboration requires groups to strategically regulate themselves to overcome challenges. Research has shown that groups may fail to regulate due to differences in members' perceptions of challenges which may benefit from external support. In this study, we investigated the potential of leveraging three distinct natural language processing models: an expert knowledge rule-based model, a supervised machine learning (ML) model and a Large Language model (LLM), in challenge detection and challenge dimension identification (cognitive, metacognitive, emotional and technical/other challenges) from student discourse, was investigated. The results show that the supervised ML and the LLM approaches performed considerably well in both tasks, in contrast to the rule-based approach, whose efficacy heavily relies on the engineered features by experts. The paper provides an extensive discussion of the three approaches' performance for automated detection and support of students' challenge moments in collaborative learning activities. It argues that, although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation. We conclude the paper with a discussion on additional considerations, including model transparency to explore feasible and meaningful analytical feedback for students and educators using LLMs.","sentences":["Effective collaboration requires groups to strategically regulate themselves to overcome challenges.","Research has shown that groups may fail to regulate due to differences in members' perceptions of challenges which may benefit from external support.","In this study, we investigated the potential of leveraging three distinct natural language processing models: an expert knowledge rule-based model, a supervised machine learning (ML) model and a Large Language model (LLM), in challenge detection and challenge dimension identification (cognitive, metacognitive, emotional and technical/other challenges) from student discourse, was investigated.","The results show that the supervised ML and the LLM approaches performed considerably well in both tasks, in contrast to the rule-based approach, whose efficacy heavily relies on the engineered features by experts.","The paper provides an extensive discussion of the three approaches' performance for automated detection and support of students' challenge moments in collaborative learning activities.","It argues that, although LLMs provide many advantages, they are unlikely to be the panacea to issues of the detection and feedback provision of socially shared regulation of learning due to their lack of reliability, as well as issues of validity evaluation, privacy and confabulation.","We conclude the paper with a discussion on additional considerations, including model transparency to explore feasible and meaningful analytical feedback for students and educators using LLMs."],"url":"http://arxiv.org/abs/2401.01692v1"}
{"created":"2024-01-03 11:49:07","title":"Zero-shot Active Learning Using Self Supervised Learning","abstract":"Deep learning algorithms are often said to be data hungry. The performance of such algorithms generally improve as more and more annotated data is fed into the model. While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task. Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget. In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process. We aim to leverage self-supervised learnt features for the task of Active Learning. The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation.","sentences":["Deep learning algorithms are often said to be data hungry.","The performance of such algorithms generally improve as more and more annotated data is fed into the model.","While collecting unlabelled data is easier (as they can be scraped easily from the internet), annotating them is a tedious and expensive task.","Given a fixed budget available for data annotation, Active Learning helps selecting the best subset of data for annotation, such that the deep learning model when trained over that subset will have maximum generalization performance under this budget.","In this work, we aim to propose a new Active Learning approach which is model agnostic as well as one doesn't require an iterative process.","We aim to leverage self-supervised learnt features for the task of Active Learning.","The benefit of self-supervised learning, is that one can get useful feature representation of the input data, without having any annotation."],"url":"http://arxiv.org/abs/2401.01690v1"}
{"created":"2024-01-03 11:44:09","title":"ODTrack: Online Dense Temporal Token Learning for Visual Tracking","abstract":"Online contextual reasoning and association across consecutive video frames are critical to perceive instances in visual tracking. However, most current top-performing trackers persistently lean on sparse temporal relationships between reference and search frames via an offline mode. Consequently, they can only interact independently within each image-pair and establish limited temporal correlations. To alleviate the above problem, we propose a simple, flexible and effective video-level tracking pipeline, named \\textbf{ODTrack}, which densely associates the contextual relationships of video frames in an online token propagation manner. ODTrack receives video frames of arbitrary length to capture the spatio-temporal trajectory relationships of an instance, and compresses the discrimination features (localization information) of a target into a token sequence to achieve frame-to-frame association. This new solution brings the following benefits: 1) the purified token sequences can serve as prompts for the inference in the next video frame, whereby past information is leveraged to guide future inference; 2) the complex online update strategies are effectively avoided by the iterative propagation of token sequences, and thus we can achieve more efficient model representation and computation. ODTrack achieves a new \\textit{SOTA} performance on seven benchmarks, while running at real-time speed. Code and models are available at \\url{https://github.com/GXNU-ZhongLab/ODTrack}.","sentences":["Online contextual reasoning and association across consecutive video frames are critical to perceive instances in visual tracking.","However, most current top-performing trackers persistently lean on sparse temporal relationships between reference and search frames via an offline mode.","Consequently, they can only interact independently within each image-pair and establish limited temporal correlations.","To alleviate the above problem, we propose a simple, flexible and effective video-level tracking pipeline, named \\textbf{ODTrack}, which densely associates the contextual relationships of video frames in an online token propagation manner.","ODTrack receives video frames of arbitrary length to capture the spatio-temporal trajectory relationships of an instance, and compresses the discrimination features (localization information) of a target into a token sequence to achieve frame-to-frame association.","This new solution brings the following benefits: 1) the purified token sequences can serve as prompts for the inference in the next video frame, whereby past information is leveraged to guide future inference; 2) the complex online update strategies are effectively avoided by the iterative propagation of token sequences, and thus we can achieve more efficient model representation and computation.","ODTrack achieves a new \\textit{SOTA} performance on seven benchmarks, while running at real-time speed.","Code and models are available at \\url{https://github.com/GXNU-ZhongLab/ODTrack}."],"url":"http://arxiv.org/abs/2401.01686v1"}
